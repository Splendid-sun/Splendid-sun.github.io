<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"splendid-sun.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="CS224N:Natural Language Processing with Deep Learning">
<meta property="og:type" content="article">
<meta property="og:title" content="Word2vec(CBOW,skip-gram,负采样,分层softmax)">
<meta property="og:url" content="https://splendid-sun.github.io/2023/01/22/CS224N-2/index.html">
<meta property="og:site_name" content="Hello,David!">
<meta property="og:description" content="CS224N:Natural Language Processing with Deep Learning">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/CBOW.png?raw=true">
<meta property="og:image" content="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/skip_gram.png?raw=true">
<meta property="og:image" content="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/hierarchical_softmax.png?raw=true">
<meta property="article:published_time" content="2023-01-22T04:07:42.000Z">
<meta property="article:modified_time" content="2023-01-23T12:11:02.000Z">
<meta property="article:author" content="David">
<meta property="article:tag" content="CS224n">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/CBOW.png?raw=true">

<link rel="canonical" href="https://splendid-sun.github.io/2023/01/22/CS224N-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>Word2vec(CBOW,skip-gram,负采样,分层softmax) | Hello,David!</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hello,David!</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">There is geometry in the humming of the strings, there is music in the spacing of the spheres.</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://splendid-sun.github.io/2023/01/22/CS224N-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="David">
      <meta itemprop="description" content="Blog my daily study on machine learning, mathematics & computer science etc.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hello,David!">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Word2vec(CBOW,skip-gram,负采样,分层softmax)
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2023-01-22 12:07:42" itemprop="dateCreated datePublished" datetime="2023-01-22T12:07:42+08:00">2023-01-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-01-23 20:11:02" itemprop="dateModified" datetime="2023-01-23T20:11:02+08:00">2023-01-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CS224n/" itemprop="url" rel="index"><span itemprop="name">CS224n</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>0</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>1 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/cs224n/">CS224N:Natural Language Processing with Deep Learning</a><br><span id="more"></span></p>
<h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><p>Word2vec是一个软件包，包括:</p>
<p>两个算法：</p>
<ul>
<li>continuous bag-of-words (CBOW) 连续词袋模型</li>
<li>skip-gram 跳字模型</li>
</ul>
<p>两个训练方法：</p>
<ul>
<li>负采样 negative sampling </li>
<li>分层 hierarchical softmax </li>
</ul>
<p>上下文context:<br>一个单词的上下文是m个周围单词的集合。例如，句子“the quick brown fox jumping over the lazy dog”中单词“fox”的m = 2上下文是{“quick”，“brown”，“jumping”，“over”}。<br>这个模型依赖于语言学中一个非常重要的假设，即分布相似性distributional similarity，即相似的单词具有相似的上下文。</p>
<h2 id="语言模型-Language-Models"><a href="#语言模型-Language-Models" class="headerlink" title="语言模型 Language Models"></a>语言模型 Language Models</h2><p>任意给定n个tokens序列的概率:</p>
<script type="math/tex; mode=display">
P\left(w_1, w_2, \cdots, w_n\right)</script><p>看一个例句：<br>“The cat jumped over the puddle.”</p>
<p>Unigram model（一元模型）：</p>
<script type="math/tex; mode=display">
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=1}^n P\left(w_i\right)</script><p>Bigram model（二元模型）:</p>
<script type="math/tex; mode=display">
P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=2}^{n} P\left(w_{i} \mid w_{i-1}\right)</script><h2 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag of Words Model (CBOW)"></a>Continuous Bag of Words Model (CBOW)</h2><p>从中心词周围的上下文预测中心词，对每一个word，我们学习两个向量</p>
<ul>
<li>$v$输入向量，当这个词在context中</li>
<li>$u$输出向量，当这个词是中心词</li>
</ul>
<p>CBOW模型记号：</p>
<ul>
<li>$w_i$ 词汇表V中的第i个单词</li>
<li>$\mathcal{V} \in \mathbb{R}^{n \times|V|}$ Input word matrix，嵌入矩阵</li>
<li>$v_i$ $\mathcal{V}$的第i列，$w_i$的输入向量表式</li>
<li>$\mathcal{U} \in \mathbb{R}^{|V| \times n}$ Output word matrix</li>
<li>$u_i$ $\mathcal{U}$的第i行，$w_i$的输出向量表式</li>
</ul>
<p>我们创建两个矩阵$\mathcal{V} \in \mathbb{R}^{n \times|V|}$  $\mathcal{U} \in \mathbb{R}^{|V| \times n}$。n可以是任意大小，定义了嵌入空间embedding space的大小。</p>
<p>步骤：</p>
<ul>
<li>1.生成大小为m窗口内的context所有word的one-hot vector：$\left(x^{(c-m)}, \ldots, x^{(c-1)}, x^{(c+1)}, \ldots, x^{(c+m)} \in \mathbb{R}^{|V|}\right)$</li>
<li>2.得出context所有word的嵌入词向量:$v<em>{c-m} = \mathcal{V} x^{(c-m)}, v</em>{c-m+1}=\mathcal{V} x^{(c-m+1)}, \ldots, v_{c+m}=\mathcal{V} x^{(c+m)} \in \mathbb{R}^{n}$</li>
<li>3.把上一步得到的嵌入词向量取平均$\hat{v}=\frac{v<em>{c-m}+v</em>{c-m+1}+\ldots+v_{c+m}}{2 m} \in \mathbb{R}^n$，$\hat{v}$代表context的平均。</li>
<li>4.生成得分向量$z=\mathcal{U} \hat{v} \in \mathbb{R}^{|V|}$</li>
<li>5.$\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}$</li>
<li>6.比较我们生成的概率$\hat{y} \in \mathbb{R}^{|V|}$和真实概率$y \in \mathbb{R}^{|V|}$</li>
</ul>
<p>用交叉熵损失函数计算两个分布之间的距离</p>
<p>loss function:</p>
<script type="math/tex; mode=display">
H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_j \log \left(\hat{y}_j\right)=-y_i \log \left(\hat{y}_i\right)</script><p>优化目标函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{minimize} J & =-\log P\left(w_c \mid w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\
& =-\log P\left(u_c \mid \hat{v}\right) \\
& =-\log \frac{\exp \left(u_c^T \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_j^T \hat{v}\right)} \\
& =-u_c^T \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_j^T \hat{v}\right)
\end{aligned}</script><p>我们用随机梯度下降来更新所有相关的词向量$u_c$和$v_j$</p>
<p> <img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/CBOW.png?raw=true" alt="CBOW"></p>
<h2 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h2><p>根据中心词预测上下文</p>
<p>Skip-Gram模型的记号：</p>
<ul>
<li>$w_i$ 词汇表V中的第i个单词</li>
<li>$\mathcal{V} \in \mathbb{R}^{n \times|V|}$ Input word matrix，嵌入矩阵</li>
<li>$v_i$ $\mathcal{V}$的第i列，$w_i$的输入向量表式</li>
<li>$\mathcal{U} \in \mathbb{R}^{|V| \times n}$ Output word matrix</li>
<li>$u_i$ $\mathcal{U}$的第i行，$w_i$的输出向量表式</li>
</ul>
<p>步骤：</p>
<ul>
<li>1.生成中心单词的one-hot向量$x \in \mathbb{R}^{|V|}$</li>
<li>2.得到中心单词的词嵌入: $v_c=\mathcal{V} x \in \mathbb{R}^n$</li>
<li>3.生成得分向量$z=\mathcal{U} v_c \in \mathbb{R}^{|V|}$</li>
<li>4.把得分转换为概率$\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}$，注意到$\hat{y}<em>{c-m}, \ldots, \hat{y}</em>{c-1}, \hat{y}<em>{c+1}, \ldots, \hat{y}</em>{c+m}$为观测到每个上下文单词的概率。</li>
<li><p>5.我们希望我们生成的概率和真实概率$y^{(c-m)}, \ldots, y^{(c-1)}, y^{(c+1)}, \ldots, y^{(c+m)}$（one-hot形式）比较接近。</p>
<p>下面第一步到第二步使用了朴素贝叶斯假设（条件独立）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } J & =-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} \mid v_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^T v_c\right)}{\sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)} \\
& =-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^T v_c+2 m \log \sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)
\end{aligned}</script><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/skip_gram.png?raw=true" alt="skip_gram"></p>
</li>
</ul>
<h2 id="负采样-negative-sampling"><a href="#负采样-negative-sampling" class="headerlink" title="负采样 negative sampling"></a>负采样 negative sampling</h2><p> <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>观察CBOW和skip-gram算法的objective function，其中求和部分计算量太大，$|V|$是百万级的。负采样的思路就是估计这个值，而不是每次做更新或者评估objective function都去做完整的计算。</p>
<p>训练过程每次循环迭代，我们不要计算完整的对$|V|$的求和，我们可以从分布$P_n(w)$中采样几个负样本。为了引入负采样，我们需要改变</p>
<ul>
<li>objective function</li>
<li>梯度</li>
<li>更新规则</li>
</ul>
<p>考虑$(w,c)$是一个词和它的上下文，$P(D=1|w,c)$是$(w,c)$属于语料库的概率，$P(D=0|w,c)$是$(w,c)$不属于语料库的概率。我们用sigmoid函数表示这个概率</p>
<script type="math/tex; mode=display">
P(D=1 \mid w, c, \theta)=\sigma\left(u_c^T v_w\right)=\frac{1}{1+e^{\left(-u_c^T v_w\right)}}</script><p>接下来，我们创建一个新的objective function，最大化这个似然，（$\theta$是模型参数，在这里是$\mathcal{V}$和$\mathcal{U}$）</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta & =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 \mid w, c, \theta) \\
& =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 \mid w, c, \theta)) \\
& =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 \mid w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 \mid w, c, \theta)) \\
& =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_w^T v_c\right)}\right) \\
& =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right)
\end{aligned}</script><p>上式等价于最小化</p>
<script type="math/tex; mode=display">
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}-\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right)</script><p>$\tilde{D}$是”false” or “negative” corpus</p>
<p>利用上述思想，在CBOW模型中，给定上下文$\hat{v}=\frac{v<em>{c-m}+v</em>{c-m+1}+\ldots+v_{c+m}}{2 m}$，观测到中心词$u_v$，目标函数为</p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_c^T \cdot \hat{v}\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{u}_k^T \cdot \hat{v}\right)</script><p>作为对比，CBOW没有负采样，原始常规softmax损失为</p>
<script type="math/tex; mode=display">
-u_c^T \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_j^T \hat{v}\right)</script><p>在skip-gram模型中，给定中心词c，观测到上下文单词c-m+j目标函数为</p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_{c-m+j}^T \cdot v_c\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{u}_k^T \cdot v_c\right)</script><p>作为对比，skip-gram没有负采样，原始常规softmax损失为</p>
<script type="math/tex; mode=display">
-u_{c-m+j}^T v_c+\log \sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)</script><p>补充：在上述讨论中，$\left{\tilde{u}_k \mid k=1 \ldots K\right}$从分布$P_n(w)$中采样。$P_n(w)$为一元模型的3/4次方。在一元模型中，我们假设</p>
<script type="math/tex; mode=display">
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=1}^n P\left(w_i\right)</script><h2 id="hierarchical-分层-softmax"><a href="#hierarchical-分层-softmax" class="headerlink" title="hierarchical(分层) softmax"></a>hierarchical(分层) softmax</h2><p> <img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/hierarchical_softmax.png?raw=true" alt="hierarchical softmax"></p>
<p>这个算法主要优势：计算复杂度从$O(|V|)$降低到$O(\log (|V|))$<br> hierarchical softmax的符号表示<br>$L(w)$：从根节点到达$w$所在的叶节点的路径上的节点数量（不包括叶子结点)<br>$n(w, i)$为路径上第i个节点，$v_{n(w, i)}$为这个节点对应的向量<br>$\operatorname{ch}(n)$:任意选择内部节点n的子节点</p>
<script type="math/tex; mode=display">
P\left(w \mid w_i\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^T v_{w_i}\right)</script><script type="math/tex; mode=display">
[x]=\left\{\begin{array}{l}
1 \text { if } x \text { is true } \\
-1 \text { otherwise }
\end{array}\right.</script><p>与negative sampling的“采样”思想不同，hierarchical softmax用一棵二叉树来表示整个词表，其中，每个叶结点都代表一个单词，每个内部节点上都有一个向量，这些向量构成了整个二叉树的参数。与原始softmax计算输出词的概率分布时要遍历整个词表不同，我们将“某个单词是输出单词的概率”定义为从根到该单词的叶子的随机漫步（random walk）的概率。所以，我们只需要在二叉树中找到从根结点到输出词所在的叶子结点之间的path，然后根据输入词向量和path中包含的各个内部节点上的向量来计算该输出词的概率。</p>
<p>在这个模型中，没有输出词的向量表示，也就是没有$\mathcal{U}$矩阵。要学习的参数包括两方面：一是输入词矩阵（即$\mathcal{V}$），二是整个树的内部结点上的向量。</p>
<p>无论是negative sampling还是hierarchical softmax，它们都是在对原始的softmax计算做简化近似，只是采取的角度和使用的方法不同而以。在实际使用中，对于低频词，hierarchical softmax的效果会略优于negative sampling，而negative sampling则在高频词和低维向量的情况下效果较好。</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[Bengio et al., 2003] Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155.<br>[Collobert et al., 2011] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. P. (2011). Natural language processing (almost) from scratch. CoRR, abs/1103.0398.<br>[Mikolov et al., 2013] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.<br>[Rong, 2014] Rong, X. (2014). word2vec parameter learning explained. CoRR, abs/1411.2738.<br>[Rumelhart et al., 1988] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1988). Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696–699. MIT Press, Cambridge, MA, USA.</p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/CS224n/" rel="tag"># CS224n</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2023/01/17/cs224n-1/" rel="prev" title="CS224n-1">
      <i class="fa fa-chevron-left"></i> CS224n-1
    </a></div>
      <div class="post-nav-item">
    <a href="/2023/01/25/GloVe/" rel="next" title="GloVe">
      GloVe <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Word2vec"><span class="nav-number">1.</span> <span class="nav-text">Word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B-Language-Models"><span class="nav-number">1.1.</span> <span class="nav-text">语言模型 Language Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Continuous-Bag-of-Words-Model-CBOW"><span class="nav-number">1.2.</span> <span class="nav-text">Continuous Bag of Words Model (CBOW)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Skip-Gram-Model"><span class="nav-number">1.3.</span> <span class="nav-text">Skip-Gram Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%B4%9F%E9%87%87%E6%A0%B7-negative-sampling"><span class="nav-number">1.4.</span> <span class="nav-text">负采样 negative sampling</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hierarchical-%E5%88%86%E5%B1%82-softmax"><span class="nav-number">1.5.</span> <span class="nav-text">hierarchical(分层) softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#References"><span class="nav-number">1.5.1.</span> <span class="nav-text">References</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">David</p>
  <div class="site-description" itemprop="description">Blog my daily study on machine learning, mathematics & computer science etc.</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">42</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">David</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">180k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">2:44</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
