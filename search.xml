<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Evaluation of Word Vectors</title>
    <url>/2023/01/30/WordVector-Evalution/</url>
    <content><![CDATA[<p><a href="https://web.stanford.edu/class/cs224n/">CS224N:Natural Language Processing with Deep Learning</a></p>
<span id="more"></span>
<h2 id="Intrinsic（内在）-Evaluation"><a href="#Intrinsic（内在）-Evaluation" class="headerlink" title="Intrinsic（内在） Evaluation"></a>Intrinsic（内在） Evaluation</h2><ul>
<li>对一个特定中间任务进行评估</li>
<li>子任务比较简单，并且计算较快</li>
<li>子系统性能表现能理解子系统的作用</li>
<li>内在评估与最终任务性能表现呈正相关</li>
</ul>
<h2 id="Extrinsic（外在）-Evaluation"><a href="#Extrinsic（外在）-Evaluation" class="headerlink" title="Extrinsic（外在） Evaluation"></a>Extrinsic（外在） Evaluation</h2><ul>
<li>对真实任务的评估</li>
<li>计算性能可能会很慢</li>
<li>如果性能不佳，搞不清楚问题出现在哪一个子系统、还是子系统之间协调不佳的问题</li>
<li>如果更换子系统可以提高性能，则更改可能是好的</li>
</ul>
<h2 id="内部评估例子-Word-Vector-Analogies（类比）"><a href="#内部评估例子-Word-Vector-Analogies（类比）" class="headerlink" title="内部评估例子: Word Vector Analogies（类比）"></a>内部评估例子: Word Vector Analogies（类比）</h2><p>给定词向量a,b,c，我们想找到一个词向量d，使得</p>
<script type="math/tex; mode=display">
a: b:: c: ?</script><p>根据余弦相似度cosine similarity：</p>
<script type="math/tex; mode=display">
d=\underset{i}{\operatorname{argmax}} \frac{\left(x_b-x_a+x_c\right)^T x_i}{\left\|x_b-x_a+x_c\right\|}</script><p>因为我们想要：$x_b-x_a=x_d-x_c$，比如说：queen – king = actress – actor。<br>这意味着我们想找到一个向量 $x_d$，使得$x_b-x_a+x_c=x_d$</p>
<h3 id="semantic（语义的）-word-vector-analogies-intrinsic-evaluation"><a href="#semantic（语义的）-word-vector-analogies-intrinsic-evaluation" class="headerlink" title="semantic（语义的） word vector analogies (intrinsic evaluation)"></a>semantic（语义的） word vector analogies (intrinsic evaluation)</h3><script type="math/tex; mode=display">
\begin{aligned}
&\text { City } 1 \text { : State containing City } 1 \text { : : City } 2 \text { : State containing City } 2\\
&\begin{array}{ll}
\hline \text { Input } & \text { Result Produced } \\
\hline \text { Chicago : Illinois : : Houston } & \text { Texas } \\
\text { Chicago : Illinois : : Philadelphia } & \text { Pennsylvania } \\
\text { Chicago : Illinois : : Phoenix } & \text { Arizona } \\
\text { Chicago : Illinois : : Dallas } & \text { Texas } \\
\text { Chicago : Illinois : : Jacksonville } & \text { Florida } \\
\text { Chicago : Illinois : : Indianapolis } & \text { Indiana } \\
\text { Chicago : Illinois : : Austin } & \text { Texas } \\
\text { Chicago : Illinois : : Detroit } & \text { Michigan } \\
\text { Chicago : Illinois : : Memphis } & \text { Tennessee } \\
\text { Chicago : Illinois : : Boston } & \text { Massachusetts } \\
\hline
\end{array}
\end{aligned}</script><p>这里存在一个问题：在美国很多城市名字一样，比如美国有十个城市都叫Phoenix。Arizona就不是唯一正确答案</p>
<p>我们可以考虑另一种首都对应国家的词类比形式</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\text { Capital City } 1 \text { : Country } 1 \text { : : Capital City } 2 \text { : Country } 2\\
&\begin{array}{ll}
\hline \text { Input } & \text { Result Produced } \\
\hline \text { Abuja : Nigeria : : Accra } & \text { Ghana } \\
\text { Abuja : Nigeria : : Algiers } & \text { Algeria } \\
\text { Abuja : Nigeria : : Amman } & \text { Jordan } \\
\text { Abuja : Nigeria : : Ankara } & \text { Turkey } \\
\text { Abuja : Nigeria : : Antananarivo } & \text { Madagascar } \\
\text { Abuja : Nigeria : : Apia } & \text { Samoa } \\
\text { Abuja : Nigeria : : Ashgabat } & \text { Turkmenistan } \\
\text { Abuja : Nigeria : : Asmara } & \text { Eritrea } \\
\text { Abuja : Nigeria : : Astana } & \text { Kazakhstan } \\
\hline
\end{array}
\end{aligned}</script><p>这种首都对应国家的词类比形式也可能存在问题，1997年以前，哈萨克斯坦的首都是阿拉木图。如果我们的语料库过时了。</p>
<h3 id="syntactic（语法的）-word-vector-analogies-intrinsic-evaluation"><a href="#syntactic（语法的）-word-vector-analogies-intrinsic-evaluation" class="headerlink" title="syntactic（语法的） word vector analogies (intrinsic evaluation)"></a>syntactic（语法的） word vector analogies (intrinsic evaluation)</h3><p>下面的例子使用内在评价测试词向量捕捉形容词最高级的能力:</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\hline \text { Input } & \text { Result Produced } \\
\hline \text { bad : worst : : big } & \text { biggest } \\
\text { bad : worst : : bright } & \text { brightest } \\
\text { bad : worst : : cold } & \text { coldest } \\
\text { bad : worst : : cool } & \text { coolest } \\
\text { bad : worst : : dark } & \text { darkest } \\
\text { bad : worst : : easy } & \text { easiest } \\
\text { bad : worst : : fast } & \text { fastest } \\
\text { bad : worst : good } & \text { best } \\
\text { bad : worst : : great } & \text { greatest }
\end{array}</script><p>类似地，下面所示的内在评估测试词向量捕捉动词过去时的能力:</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
\hline \text { Input } & \text { Result Produced } \\
\hline \text { bad : worst : : big } & \text { biggest } \\
\text { bad : worst : : bright } & \text { brightest } \\
\text { bad : worst : : cold } & \text { coldest } \\
\text { bad : worst : : cool } & \text { coolest } \\
\text { bad : worst : : dark } & \text { darkest } \\
\text { bad : worst : : easy } & \text { easiest } \\
\text { bad : worst : : fast } & \text { fastest } \\
\text { bad : worst : : good } & \text { best } \\
\text { bad : worst : : great } & \text { greatest } \\
\hline
\end{array}</script><h2 id="词嵌入技术和内在评估的超参数："><a href="#词嵌入技术和内在评估的超参数：" class="headerlink" title="词嵌入技术和内在评估的超参数："></a>词嵌入技术和内在评估的超参数：</h2><ul>
<li>词向量的维数</li>
<li>语料库大小</li>
<li>语料来源/类型</li>
<li>上下文窗口大小</li>
<li><p>上下文对称性</p>
<p>我们可以看到不同超参数下，不同的方法的性能表现</p>
<script type="math/tex; mode=display">
\begin{array}{lll|lll}
\hline \text { Model } & \text { Dimension } & \text { Size } & \text { Semantics } & \text { Syntax } & \text { Total } \\
\hline \text { ivLBL } & 100 & 1.5 \mathrm{~B} & 55.9 & 50.1 & 53.2 \\
\text { HPCA } & 100 & 1.6 \mathrm{~B} & 4.2 & 16.4 & 10.8 \\
\text { GloVE } & 100 & 1.6 \mathrm{~B} & 67.5 & 54.3 & 60.3 \\
\hline \text { SG } & 300 & 1 \mathrm{~B} & 61 & 61 & 61 \\
\text { CBOW } & 300 & 1.6 \mathrm{~B} & 16.1 & 52.6 & 36.1 \\
\text { vLBL } & 300 & 1.5 \mathrm{~B} & 54.2 & 64.8 & 60.0 \\
\text { ivLBL } & 300 & 1.5 \mathrm{~B} & 65.2 & 63.0 & 64.0 \\
\text { GloVe } & 300 & 1.6 \mathrm{~B} & 80.8 & 61.5 & 70.3 \\
\hline \text { SVD } & 300 & 6 \mathrm{~B} & 6.3 & 8.1 & 7.3 \\
\text { SVD-S } & 300 & 6 \mathrm{~B} & 36.7 & 46.6 & 42.1 \\
\text { SVD-L } & 300 & \text { 6B } & 56.6 & 63.0 & 60.1 \\
\text { CBOW } & 300 & \text { 6B } & 63.6 & 67.4 & 65.7 \\
\text { SG } & 300 & 6 \mathrm{~B} & 73.0 & 66.0 & 69.1 \\
\text { GloVe } & 300 & 6 \mathrm{~B} & 77.4 & 67.0 & 71.7 \\
\hline \text { CBOW } & 1000 & \text { 6B } & 57.3 & 68.9 & 63.7 \\
\text { SG } & 1000 & \text { 6B } & 66.1 & 65.1 & 65.6 \\
\text { SVD-L } & 300 & 42 B & 38.4 & 58.2 & 49.2 \\
\text { GloVe } & 300 & 42 \mathrm{~B} & 81.9 & 69.3 & 75.0 \\
\hline
\end{array}</script></li>
<li>词嵌入性能在很大程度上依赖于用于词嵌入的模型</li>
<li>语料库规模越大，性能越好</li>
<li>对于极低维的词向量，性能较低</li>
</ul>
<p>超参数选择：<br> 词向量维度选为300<br> 上下文窗口大小选为8比较好</p>
<h2 id="内在评估例子-相关性评价（Correlation-Evaluation）"><a href="#内在评估例子-相关性评价（Correlation-Evaluation）" class="headerlink" title="内在评估例子:相关性评价（Correlation Evaluation）"></a>内在评估例子:相关性评价（Correlation Evaluation）</h2><p>让人给词向量之间相似度评分（比如：0-10分），然后跟对应的词向量的余弦相似度比较</p>
]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title>GloVe</title>
    <url>/2023/01/25/GloVe/</url>
    <content><![CDATA[<p><a href="https://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a></p>
<span id="more"></span>
<p>GloVe (Global Vectors)名称由来：因为这个模型捕获了语料库全局统计信息</p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>做自然语言处理的时候很多时候会用的Word Embedding，目前常用的方法是word2vec算法训练词向量。不过训练词向量的方法有很多，今天介绍GloVe算法。</p>
<p>模型输入：语料库 corpus</p>
<p>模型输出：每个词的表示向量</p>
<h2 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h2><p>要讲GloVe模型的思想方法，我们先介绍两个其他方法：</p>
<p>一个是基于奇异值分解（SVD）的LSA算法，该方法对word-word矩阵进行奇异值分解。此处使用的是全局统计特征。</p>
<p>另一个方法是word2vec算法，该算法可以分为skip-gram 和 continuous bag-of-words（CBOW）两类,但都是基于局部滑动窗口计算的。即，该方法利用了局部的上下文特征（local context）</p>
<p>LSA和word2vec作为两大类方法的代表，一个是利用了全局特征的矩阵分解方法，一个是利用局部上下文的方法。</p>
<p>GloVe模型就是将这两中特征合并到一起的，即使用了语料库的全局统计（overall statistics）特征，也使用了局部的上下文特征（即滑动窗口）。为了做到这一点GloVe模型引入了Co-occurrence Probabilities Matrix。</p>
<p>notation:<br>$X$: co-occurrence matrix<br>$X<em>{i j}$: 语料库中出现在word i上下文中word j的次数<br>$X_i=\sum_k X</em>{i k}$，出现在word i上下文中所有的word的次数<br>$P<em>{i j}=P(j \mid i)=\frac{X</em>{i j}}{X_i}$，word j出现在word i上下文的概率<br>由以上概念引申出共现概率矩阵（Co-occurrence Probabilities matrix）</p>
<script type="math/tex; mode=display">
\begin{array}{l|cccc}
\text { Probability and Ratio } & k=\text { solid } & k=\text { gas } & k=\text { water } & k=\text { fashion } \\
\hline P(k \mid \text { ice }) & 1.9 \times 10^{-4} & 6.6 \times 10^{-5} & 3.0 \times 10^{-3} & 1.7 \times 10^{-5} \\
P(k \mid \text { steam }) & 2.2 \times 10^{-5} & 7.8 \times 10^{-4} & 2.2 \times 10^{-3} & 1.8 \times 10^{-5} \\
P(k \mid \text { ice }) / P(k \mid \text { steam }) & 8.9 & 8.5 \times 10^{-2} & 1.36 & 0.96
\end{array}</script><p>solid 与 ice 相关，与steam 不相关<br>gas 与 ice 不相关，与steam 相关<br>water 与 ice，steam 都相关<br>fashion 与 ice，steam 都不相关</p>
<p>由Co-occurrence Probabilities matrix可以看出Ratio $=\frac{P<em>{i k}}{P</em>{j k}}$的取值是有规律的。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th><strong>ratio i,j,k的值</strong></th>
<th><strong>单词j,k相关</strong></th>
<th><strong>单词j,k不相关</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>单词i,k相关</strong></td>
<td>趋近1</td>
<td>很大</td>
</tr>
<tr>
<td><strong>单词i,k不相关</strong></td>
<td>很小</td>
<td>趋近1</td>
</tr>
</tbody>
</table>
</div>
<p>也就是说Ratio值能够反映word之间的相关性，<strong>而GloVe模型就是利用了这个Ratio值</strong>。<br>再明确一下，<strong>GloVe模型的目标就是获取每一个word的向量表示 $\mathrm{v}$</strong>。不妨假设现在已经得到了word $i, j, k$ 的词 向量 $w<em>i, w_j, w_k$ 。**GloVe认为，这三个向量通过某种函数的作用后所呈现出来的规律和Ratio $=\frac{P</em>{i k}}{P_{j k}}$ 具有一致性，即相等，也就可以认为词向量中包含了共现概率矩阵中的信息。**<br>假设这个末知的函数是 $F$, 则:</p>
<script type="math/tex; mode=display">
F\left(w_i, w_j, w_k\right)=\frac{P_{i k}}{P_{j k}}</script><p>右侧的 $\frac{P<em>{ik}}{P</em>{jk}}$ 可以通过统计求的；<br>左侧的 $w_i, w_j, w_k$ 是我们模型要求的量；<br>同时函数 $F$ 是末知的。<br>如果能够将函数F的形式确定下来，就可以通过优化算法求解词向量了。那么GloVe模型的作者是怎么将$F$确定下来的呢?</p>
<p>1.$\frac{P<em>{ik}}{P</em>{j k}}$ 考察了 $i, j, k$ 三个word两两之间的相似关系，不妨单独考察 $i, j$ 两个词和他们词向量 $w_i, w_j$ ，线性空间中旳相似关系自然想到的是两个向量的差 $\left(v_i-v_j\right)$ 。所以F函数的形式可以是</p>
<script type="math/tex; mode=display">
F\left(w_i-w_j, w_k\right)=\frac{P_{i k}}{P_{j k}}</script><p>2.$\frac{P<em>{i k}}{P</em>{j k}}$ 是一个标量，而F是作用在两个向量上的，向量和标量之间的关系自然想到了使用内积。所以F函数的形式可以进一步确定为</p>
<script type="math/tex; mode=display">
F\left(\left(w_i-w_j\right)^T w_k\right)=F\left(w_i^T w_k-w_j^T w_k\right)=\frac{P_{i k}}{P_{j k}}</script><p>3.到此为止模型公式的形式是 $F\left(w<em>i^T w_k-w_j^T w_k\right)=\frac{P</em>{i k}}{P_{j k}}$ 。左边是差，右边是商，模型通过将F取作 $\exp$ 来将差和商关联起来</p>
<script type="math/tex; mode=display">
\exp \left(w_i^T w_k-w_j^T w_k\right)=\frac{\exp \left(w_i^T w_k\right)}{\exp \left(w_j^T w_k\right)}=\frac{P_{i k}}{P_{j k}}</script><p>4.现在只需要让分子分母分别相等，上式就能够成立，所以</p>
<script type="math/tex; mode=display">
\exp \left(w_i^T w_k\right)=P_{i k}, \exp \left(w_j^T w_k\right)=P_{j k}</script><p>5.所以只需要在整个文本库中考察$\exp \left(w<em>i^T w_k\right)=P</em>{i k}=\frac{X_{i k}}{X_i}$，即</p>
<script type="math/tex; mode=display">
w_i^T w_k=\log \left(\frac{X_{i k}}{X_i}\right)=\log X_{i k}-\log X_i</script><p>6.作为向量，交换 $i$ 和 $k$ 的顺序 $w<em>i^T w_k$ 和 $w_k^T w_i$ 是相等的，即公式左边对于 $i$ 和 $k$ 的顺序是不敏感的，但是公式右边交换 $i$ 和 $k$ 的顺序 $\log X</em>{i k}-\log X<em>i \neq \log X</em>{k i}-\log X_k$ 。为了解决这个对称性问题 ，模型引入了两个偏置项 $b_i, b_k$,从而将模型变成了</p>
<script type="math/tex; mode=display">
\log X_{i k}=w_i^T w_k+b_i+b_k</script><p>7.上面旳公式只是理想情况下，在实际实验中左右两边只能要求接近。从而就有了代价函数cost function：</p>
<script type="math/tex; mode=display">
J=\sum_{i k}\left(w_i^T w_k+b_i+b_k-\log X_{i k}\right)^2</script><p>8.根据经验，如果两个词共同出现的次数越多，那么这两个词在代价函数中的影响就应该越大，所以可以根据两个词共同出现的次数设计一个权重项来对代价函数中的每一项进行加权：</p>
<script type="math/tex; mode=display">
J=\sum_{i k} f\left(X_{i k}\right)\left(w_i^T w_k+b_i+b_k-\log X_{i k}\right)^2</script><p>模型认为权重函数 $f$ 应该符合以下三个特点：</p>
<ol>
<li>$f(0)=0$ （如果两个词没有共同出现过，权重就是0）； </li>
<li>$f(x)$ 必项是非减函数 (两个词共同出现的次数多，反而权重变小了，违反了设置权重项的初衷）；</li>
<li>$f(x)$ 对于较大的 $x$ 不能取太大的值(就像是汉语中”的”这个字，在很多文章中都会出现很多次，但是其在文中的重要程度非常小)。综合这三条特点的 $f(x)$ 定义为：</li>
</ol>
<script type="math/tex; mode=display">
f(x)=\left\{\begin{array}{c}
\left(\frac{x}{x_{\max }}\right)^\alpha, \text { if } x<x_{\max } \\
1, \text { otherwise }
\end{array}\right.</script><p>根据经验，GloVe作者认为$x_{\max }=100, \alpha=\frac{3}{4}$是一个比较好的选择</p>
<p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/GloVe/weight_function.png?raw=true" alt="weight function"></p>
<h2 id="Least-Squares-Objective"><a href="#Least-Squares-Objective" class="headerlink" title="Least Squares Objective"></a>Least Squares Objective</h2><p>GloVe也从另一个角度推导了这个公式。<br>不同的词向量学习方法最终都是基于语料库的共现统计信息，不同的模型之间应该存在一些共通性<br>GloVe类似skip-gram 模型，我们利用下式计算单词j出现在单词i的上下文的概率</p>
<script type="math/tex; mode=display">
Q_{i j}=\frac{\exp \left(\vec{u}_j^T \vec{v}_i\right)}{\sum_{w=1}^W \exp \left(\vec{u}_w^T \vec{v}_i\right)}</script><p>所以全局交叉熵为</p>
<script type="math/tex; mode=display">
J=-\sum_{i \in \text { corpus }} \sum_{j \in \text { context }(i)} \log Q_{i j}</script><p>利用Co-occurrence Matrix，我们可以将上式写为：</p>
<script type="math/tex; mode=display">
J=-\sum_{i=1}^W \sum_{j=1}^W X_{i j} \log Q_{i j}</script><p>如果定义上述损失函数，那么$Q_{i j}$的softmax分母计算需要很多计算量。为了解决这点，我们使用最小二乘损失函数来代替上述损失函数：</p>
<script type="math/tex; mode=display">
\hat{J}=\sum_{i=1}^W \sum_{j=1}^W X_i\left(\hat{P}_{i j}-\hat{Q}_{i j}\right)^2</script><p>其中</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{P}_{i j} & =X_{i j} \\
\hat{Q}_{i j} & =\exp \left(\vec{u}_j^T \vec{v}_i\right)
\end{aligned}</script><p>由于指数函数可能会产生很大的值，所以对括号内的值取对数更容易减少误差：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{J} & =\sum_{i=1}^W \sum_{j=1}^W X_i\left(\log (\hat{P}_{i j})-\log (\hat{Q}_{i j})\right)^2 \\
& =\sum_{i=1}^W \sum_{j=1}^W X_i\left(\vec{u}_j^T \vec{v}_i-\log X_{i j}\right)^2
\end{aligned}</script><p>注意到$X_i$不一定是最好的系数，更一般的形式为</p>
<script type="math/tex; mode=display">
\hat{J}=\sum_{i=1}^W \sum_{j=1}^W f\left(X_{i j}\right)\left(\vec{u}_j^T \vec{v}_i-\log X_{i j}\right)^2</script>]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title>Word2vec(CBOW,skip-gram,负采样,分层softmax)</title>
    <url>/2023/01/22/CS224N-2/</url>
    <content><![CDATA[<p><a href="https://web.stanford.edu/class/cs224n/">CS224N:Natural Language Processing with Deep Learning</a><br><span id="more"></span></p>
<h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><p>Word2vec是一个软件包，包括:</p>
<p>两个算法：</p>
<ul>
<li>continuous bag-of-words (CBOW) 连续词袋模型</li>
<li>skip-gram 跳字模型</li>
</ul>
<p>两个训练方法：</p>
<ul>
<li>负采样 negative sampling </li>
<li>分层 hierarchical softmax </li>
</ul>
<p>上下文context:<br>一个单词的上下文是m个周围单词的集合。例如，句子“the quick brown fox jumping over the lazy dog”中单词“fox”的m = 2上下文是{“quick”，“brown”，“jumping”，“over”}。<br>这个模型依赖于语言学中一个非常重要的假设，即分布相似性distributional similarity，即相似的单词具有相似的上下文。</p>
<h2 id="语言模型-Language-Models"><a href="#语言模型-Language-Models" class="headerlink" title="语言模型 Language Models"></a>语言模型 Language Models</h2><p>任意给定n个tokens序列的概率:</p>
<script type="math/tex; mode=display">
P\left(w_1, w_2, \cdots, w_n\right)</script><p>看一个例句：<br>“The cat jumped over the puddle.”</p>
<p>Unigram model（一元模型）：</p>
<script type="math/tex; mode=display">
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=1}^n P\left(w_i\right)</script><p>Bigram model（二元模型）:</p>
<script type="math/tex; mode=display">
P\left(w_{1}, w_{2}, \cdots, w_{n}\right)=\prod_{i=2}^{n} P\left(w_{i} \mid w_{i-1}\right)</script><h2 id="Continuous-Bag-of-Words-Model-CBOW"><a href="#Continuous-Bag-of-Words-Model-CBOW" class="headerlink" title="Continuous Bag of Words Model (CBOW)"></a>Continuous Bag of Words Model (CBOW)</h2><p>从中心词周围的上下文预测中心词，对每一个word，我们学习两个向量</p>
<ul>
<li>$v$输入向量，当这个词在context中</li>
<li>$u$输出向量，当这个词是中心词</li>
</ul>
<p>CBOW模型记号：</p>
<ul>
<li>$w_i$ 词汇表V中的第i个单词</li>
<li>$\mathcal{V} \in \mathbb{R}^{n \times|V|}$ Input word matrix，嵌入矩阵</li>
<li>$v_i$ $\mathcal{V}$的第i列，$w_i$的输入向量表式</li>
<li>$\mathcal{U} \in \mathbb{R}^{|V| \times n}$ Output word matrix</li>
<li>$u_i$ $\mathcal{U}$的第i行，$w_i$的输出向量表式</li>
</ul>
<p>我们创建两个矩阵$\mathcal{V} \in \mathbb{R}^{n \times|V|}$  $\mathcal{U} \in \mathbb{R}^{|V| \times n}$。n可以是任意大小，定义了嵌入空间embedding space的大小。</p>
<p>步骤：</p>
<ul>
<li>1.生成大小为m窗口内的context所有word的one-hot vector：$\left(x^{(c-m)}, \ldots, x^{(c-1)}, x^{(c+1)}, \ldots, x^{(c+m)} \in \mathbb{R}^{|V|}\right)$</li>
<li>2.得出context所有word的嵌入词向量:$v<em>{c-m} = \mathcal{V} x^{(c-m)}, v</em>{c-m+1}=\mathcal{V} x^{(c-m+1)}, \ldots, v_{c+m}=\mathcal{V} x^{(c+m)} \in \mathbb{R}^{n}$</li>
<li>3.把上一步得到的嵌入词向量取平均$\hat{v}=\frac{v<em>{c-m}+v</em>{c-m+1}+\ldots+v_{c+m}}{2 m} \in \mathbb{R}^n$，$\hat{v}$代表context的平均。</li>
<li>4.生成得分向量$z=\mathcal{U} \hat{v} \in \mathbb{R}^{|V|}$</li>
<li>5.$\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}$</li>
<li>6.比较我们生成的概率$\hat{y} \in \mathbb{R}^{|V|}$和真实概率$y \in \mathbb{R}^{|V|}$</li>
</ul>
<p>用交叉熵损失函数计算两个分布之间的距离</p>
<p>loss function:</p>
<script type="math/tex; mode=display">
H(\hat{y}, y)=-\sum_{j=1}^{|V|} y_j \log \left(\hat{y}_j\right)=-y_i \log \left(\hat{y}_i\right)</script><p>优化目标函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{minimize} J & =-\log P\left(w_c \mid w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m}\right) \\
& =-\log P\left(u_c \mid \hat{v}\right) \\
& =-\log \frac{\exp \left(u_c^T \hat{v}\right)}{\sum_{j=1}^{|V|} \exp \left(u_j^T \hat{v}\right)} \\
& =-u_c^T \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_j^T \hat{v}\right)
\end{aligned}</script><p>我们用随机梯度下降来更新所有相关的词向量$u_c$和$v_j$</p>
<p> <img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/CBOW.png?raw=true" alt="CBOW"></p>
<h2 id="Skip-Gram-Model"><a href="#Skip-Gram-Model" class="headerlink" title="Skip-Gram Model"></a>Skip-Gram Model</h2><p>根据中心词预测上下文</p>
<p>Skip-Gram模型的记号：</p>
<ul>
<li>$w_i$ 词汇表V中的第i个单词</li>
<li>$\mathcal{V} \in \mathbb{R}^{n \times|V|}$ Input word matrix，嵌入矩阵</li>
<li>$v_i$ $\mathcal{V}$的第i列，$w_i$的输入向量表式</li>
<li>$\mathcal{U} \in \mathbb{R}^{|V| \times n}$ Output word matrix</li>
<li>$u_i$ $\mathcal{U}$的第i行，$w_i$的输出向量表式</li>
</ul>
<p>步骤：</p>
<ul>
<li>1.生成中心单词的one-hot向量$x \in \mathbb{R}^{|V|}$</li>
<li>2.得到中心单词的词嵌入: $v_c=\mathcal{V} x \in \mathbb{R}^n$</li>
<li>3.生成得分向量$z=\mathcal{U} v_c \in \mathbb{R}^{|V|}$</li>
<li>4.把得分转换为概率$\hat{y}=\operatorname{softmax}(z) \in \mathbb{R}^{|V|}$，注意到$\hat{y}<em>{c-m}, \ldots, \hat{y}</em>{c-1}, \hat{y}<em>{c+1}, \ldots, \hat{y}</em>{c+m}$为观测到每个上下文单词的概率。</li>
<li><p>5.我们希望我们生成的概率和真实概率$y^{(c-m)}, \ldots, y^{(c-1)}, y^{(c+1)}, \ldots, y^{(c+m)}$（one-hot形式）比较接近。</p>
<p>下面第一步到第二步使用了朴素贝叶斯假设（条件独立）：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\text { minimize } J & =-\log P\left(w_{c-m}, \ldots, w_{c-1}, w_{c+1}, \ldots, w_{c+m} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(w_{c-m+j} \mid w_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} P\left(u_{c-m+j} \mid v_c\right) \\
& =-\log \prod_{j=0, j \neq m}^{2 m} \frac{\exp \left(u_{c-m+j}^T v_c\right)}{\sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)} \\
& =-\sum_{j=0, j \neq m}^{2 m} u_{c-m+j}^T v_c+2 m \log \sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)
\end{aligned}</script><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/skip_gram.png?raw=true" alt="skip_gram"></p>
</li>
</ul>
<h2 id="负采样-negative-sampling"><a href="#负采样-negative-sampling" class="headerlink" title="负采样 negative sampling"></a>负采样 negative sampling</h2><p> <a href="https://arxiv.org/pdf/1310.4546.pdf">Distributed Representations of Words and Phrases and their Compositionality</a></p>
<p>观察CBOW和skip-gram算法的objective function，其中求和部分计算量太大，$|V|$是百万级的。负采样的思路就是估计这个值，而不是每次做更新或者评估objective function都去做完整的计算。</p>
<p>训练过程每次循环迭代，我们不要计算完整的对$|V|$的求和，我们可以从分布$P_n(w)$中采样几个负样本。为了引入负采样，我们需要改变</p>
<ul>
<li>objective function</li>
<li>梯度</li>
<li>更新规则</li>
</ul>
<p>考虑$(w,c)$是一个词和它的上下文，$P(D=1|w,c)$是$(w,c)$属于语料库的概率，$P(D=0|w,c)$是$(w,c)$不属于语料库的概率。我们用sigmoid函数表示这个概率</p>
<script type="math/tex; mode=display">
P(D=1 \mid w, c, \theta)=\sigma\left(u_c^T v_w\right)=\frac{1}{1+e^{\left(-u_c^T v_w\right)}}</script><p>接下来，我们创建一个新的objective function，最大化这个似然，（$\theta$是模型参数，在这里是$\mathcal{V}$和$\mathcal{U}$）</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta & =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}} P(D=0 \mid w, c, \theta) \\
& =\underset{\theta}{\operatorname{argmax}} \prod_{(w, c) \in D} P(D=1 \mid w, c, \theta) \prod_{(w, c) \in \tilde{D}}(1-P(D=1 \mid w, c, \theta)) \\
& =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log P(D=1 \mid w, c, \theta)+\sum_{(w, c) \in \tilde{D}} \log (1-P(D=1 \mid w, c, \theta)) \\
& =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(1-\frac{1}{1+\exp \left(-u_w^T v_c\right)}\right) \\
& =\underset{\theta}{\operatorname{argmax}} \sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}+\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right)
\end{aligned}</script><p>上式等价于最小化</p>
<script type="math/tex; mode=display">
J=-\sum_{(w, c) \in D} \log \frac{1}{1+\exp \left(-u_w^T v_c\right)}-\sum_{(w, c) \in \tilde{D}} \log \left(\frac{1}{1+\exp \left(u_w^T v_c\right)}\right)</script><p>$\tilde{D}$是”false” or “negative” corpus</p>
<p>利用上述思想，在CBOW模型中，给定上下文$\hat{v}=\frac{v<em>{c-m}+v</em>{c-m+1}+\ldots+v_{c+m}}{2 m}$，观测到中心词$u_v$，目标函数为</p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_c^T \cdot \hat{v}\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{u}_k^T \cdot \hat{v}\right)</script><p>作为对比，CBOW没有负采样，原始常规softmax损失为</p>
<script type="math/tex; mode=display">
-u_c^T \hat{v}+\log \sum_{j=1}^{|V|} \exp \left(u_j^T \hat{v}\right)</script><p>在skip-gram模型中，给定中心词c，观测到上下文单词c-m+j目标函数为</p>
<script type="math/tex; mode=display">
-\log \sigma\left(u_{c-m+j}^T \cdot v_c\right)-\sum_{k=1}^K \log \sigma\left(-\tilde{u}_k^T \cdot v_c\right)</script><p>作为对比，skip-gram没有负采样，原始常规softmax损失为</p>
<script type="math/tex; mode=display">
-u_{c-m+j}^T v_c+\log \sum_{k=1}^{|V|} \exp \left(u_k^T v_c\right)</script><p>补充：在上述讨论中，$\left{\tilde{u}_k \mid k=1 \ldots K\right}$从分布$P_n(w)$中采样。$P_n(w)$为一元模型的3/4次方。在一元模型中，我们假设</p>
<script type="math/tex; mode=display">
P\left(w_1, w_2, \cdots, w_n\right)=\prod_{i=1}^n P\left(w_i\right)</script><h2 id="hierarchical-分层-softmax"><a href="#hierarchical-分层-softmax" class="headerlink" title="hierarchical(分层) softmax"></a>hierarchical(分层) softmax</h2><p> <img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/CS224n_2/hierarchical_softmax.png?raw=true" alt="hierarchical softmax"></p>
<p>这个算法主要优势：计算复杂度从$O(|V|)$降低到$O(\log (|V|))$<br> hierarchical softmax的符号表示<br>$L(w)$：从根节点到达$w$所在的叶节点的路径上的节点数量（不包括叶子结点)<br>$n(w, i)$为路径上第i个节点，$v_{n(w, i)}$为这个节点对应的向量<br>$\operatorname{ch}(n)$:任意选择内部节点n的子节点</p>
<script type="math/tex; mode=display">
P\left(w \mid w_i\right)=\prod_{j=1}^{L(w)-1} \sigma\left([n(w, j+1)=\operatorname{ch}(n(w, j))] \cdot v_{n(w, j)}^T v_{w_i}\right)</script><script type="math/tex; mode=display">
[x]=\left\{\begin{array}{l}
1 \text { if } x \text { is true } \\
-1 \text { otherwise }
\end{array}\right.</script><p>与negative sampling的“采样”思想不同，hierarchical softmax用一棵二叉树来表示整个词表，其中，每个叶结点都代表一个单词，每个内部节点上都有一个向量，这些向量构成了整个二叉树的参数。与原始softmax计算输出词的概率分布时要遍历整个词表不同，我们将“某个单词是输出单词的概率”定义为从根到该单词的叶子的随机漫步（random walk）的概率。所以，我们只需要在二叉树中找到从根结点到输出词所在的叶子结点之间的path，然后根据输入词向量和path中包含的各个内部节点上的向量来计算该输出词的概率。</p>
<p>在这个模型中，没有输出词的向量表示，也就是没有$\mathcal{U}$矩阵。要学习的参数包括两方面：一是输入词矩阵（即$\mathcal{V}$），二是整个树的内部结点上的向量。</p>
<p>无论是negative sampling还是hierarchical softmax，它们都是在对原始的softmax计算做简化近似，只是采取的角度和使用的方法不同而以。在实际使用中，对于低频词，hierarchical softmax的效果会略优于negative sampling，而negative sampling则在高频词和低维向量的情况下效果较好。</p>
<h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><p>[Bengio et al., 2003] Bengio, Y., Ducharme, R., Vincent, P., and Janvin, C. (2003). A neural probabilistic language model. J. Mach. Learn. Res., 3:1137–1155.<br>[Collobert et al., 2011] Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. P. (2011). Natural language processing (almost) from scratch. CoRR, abs/1103.0398.<br>[Mikolov et al., 2013] Mikolov, T., Chen, K., Corrado, G., and Dean, J. (2013). Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.<br>[Rong, 2014] Rong, X. (2014). word2vec parameter learning explained. CoRR, abs/1411.2738.<br>[Rumelhart et al., 1988] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1988). Neurocomputing: Foundations of research. chapter Learning Representations by Back-propagating Errors, pages 696–699. MIT Press, Cambridge, MA, USA.</p>
]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title>CS224n-1</title>
    <url>/2023/01/17/cs224n-1/</url>
    <content><![CDATA[<p><a href="https://web.stanford.edu/class/cs224n/">CS224N:Natural Language Processing with Deep Learning</a></p>
<span id="more"></span>
<h1 id="Human-language-and-word-meaning"><a href="#Human-language-and-word-meaning" class="headerlink" title="Human language and word meaning"></a>Human language and word meaning</h1><p>NLP的目标是设计算法让计算机理解自然语言，NLP的常见任务有：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>难度</th>
<th style="text-align:center">task</th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td>简单</td>
<td style="text-align:center">Spell Checking 拼写检查</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>简单</td>
<td style="text-align:center">Keyword Search 关键字搜索</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>简单</td>
<td style="text-align:center">Finding Synonyms 寻找同义词</td>
<td style="text-align:center"></td>
</tr>
<tr>
<td>中等</td>
<td style="text-align:center">Parsing</td>
<td style="text-align:center">Parsing information from websites, documents, etc.</td>
</tr>
<tr>
<td>难</td>
<td style="text-align:center">Machine Translation 机器翻译</td>
<td style="text-align:center">e.g. Translate Chinese text to English</td>
</tr>
<tr>
<td>难</td>
<td style="text-align:center">Semantic Analysis 语义分析</td>
<td style="text-align:center">What is the meaning of query statement?</td>
</tr>
<tr>
<td>难</td>
<td style="text-align:center">Coreference</td>
<td style="text-align:center">e.g. What does “he” or “it” refer to given a document?</td>
</tr>
<tr>
<td>难</td>
<td style="text-align:center">Question Answering</td>
<td style="text-align:center">e.g. Answering Jeopardy questions</td>
</tr>
</tbody>
</table>
</div>
<p>parse /pɑːz/<br>v. 对（句子）作语法分析；分析<br>n. （计算机）句法分析，句法分析结果</p>
<h2 id="denotational-semantics-指称语义学"><a href="#denotational-semantics-指称语义学" class="headerlink" title="denotational semantics 指称语义学"></a>denotational semantics 指称语义学</h2><p>指称语义学是一种用符号（word或者one-hot vector）表示含义的概念，这种表示是稀疏的，不能捕捉相似性。这是一种”localist” representation.</p>
<p>signifier(symbol) &lt;—&gt; signified(idea or thing)</p>
<h3 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h3><p>WordNet是一个包含同义词集（synonym sets）和上位词(hypernyms)(“is a” relationships)的列表的辞典<br> <img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/cs224n_1/denotational_semantics.png?raw=true" alt="denotational_semantics"></p>
<p>WordNet存在的问题：</p>
<ol>
<li>WordNet是一个很好的资源，但忽略了同义词之间的细微差别<ul>
<li>例如“proficient”被列为“good”的同义词。这只在某些上下文中是正确的</li>
</ul>
</li>
<li>对words的新的含义无法及时更新：<ul>
<li>例如 wicked, badass, nifty, wizard, genius, ninja, bombest</li>
<li>不可能永远保持更新words的新的含义</li>
</ul>
</li>
<li>主观</li>
<li>需要人类劳动和调整</li>
<li>无法计算单词之间的相似度</li>
</ol>
<h3 id="one-hot-vector"><a href="#one-hot-vector" class="headerlink" title="one-hot vector"></a>one-hot vector</h3><p>one-hot vector: 用一个 $\mathbb{R}^{|V| \times 1}$ 向量表示每一个word，1位于这个word在词汇表中的索引位置，其余位置全是0。 $|V|$ 是词汇表的大小. 用这种形式编码的Word vectors 会以如下形式出现：</p>
<script type="math/tex; mode=display">
w^{a a r d v a r k}=\left[\begin{array}{c}
1 \\
0 \\
0 \\
\vdots \\
0
\end{array}\right], w^a=\left[\begin{array}{c}
0 \\
1 \\
0 \\
\vdots \\
0
\end{array}\right], w^{a t}=\left[\begin{array}{c}
0 \\
0 \\
1 \\
\vdots \\
0
\end{array}\right], \cdots ,w^{z e b r a}=\left[\begin{array}{c}
0 \\
0 \\
0 \\
\vdots \\
1
\end{array}\right]</script><p>one-hot vector存在的问题：</p>
<p>所有向量是正交的，没有相似性概念，并且向量维度过大。</p>
<h2 id="Distributional-semantics-分布语义学"><a href="#Distributional-semantics-分布语义学" class="headerlink" title="Distributional semantics 分布语义学"></a>Distributional semantics 分布语义学</h2><p>这是基于word经常出现的上下文context表示一个word含义的概念。这种表示是密集的dense，可以更好地捕捉相似性similarity。</p>
<blockquote>
<p>“You shall know a word by the company it keeps”(J. R Firth 1957: 11)<br>你可以通过一个单词的上下文了解它的含义 语言学家J. R Firth</p>
</blockquote>
<p>分布语义学是现代统计NLP最成功的理念之一</p>
<p>当一个单词word出现在文本中时，它的上下文context是出现在其附近的一组单词(在一个固定大小的窗口中)</p>
<p>word vector, (word) embedding, (neural) word representations这三个概念等价，他们都是distributed representation，区别于”localist” representation</p>
<h2 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h2><p><a href="https://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a></p>
<p>Word2vec (Mikolov et al. 2013) 是一个学习word vectors的框架</p>
<ul>
<li>Word2vec<ul>
<li>我们有大量的文本 (corpus means ‘body’ in Latin. 复数为corpora)</li>
<li>固定词汇表中的每个单词都由一个向量表示</li>
<li>文本中的每个位置t，其中有一个中心词c和上下文(“外部”)单词o</li>
<li>使用c和o的词向量的相似性来计算给定c的o的概率 (反之亦然)</li>
<li>不断调整词向量来最大化这个概率</li>
</ul>
</li>
</ul>
<p>下图为窗口大小为2时的$P\left(w_{t+j} \mid w_t\right)$计算过程，center word分别为into和banking</p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/cs224n_1/1560070410531.png" alt="word2vex"></p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/cs224n_1/1560070494437.png" alt="word2vex"></p>
<h3 id="Word2vec-objective-function-cost-or-loss-function"><a href="#Word2vec-objective-function-cost-or-loss-function" class="headerlink" title="Word2vec: objective function(cost or loss function)"></a>Word2vec: objective function(cost or loss function)</h3><p>对于每个位置$t=1, \ldots, T$，在大小为m的固定窗口内预测上下文单词，给定中心词$w_t$，这个位置的似然为</p>
<script type="math/tex; mode=display">
\text {Likelihood}=L(\theta)=\prod_{\substack{t=1}}^T \prod_{\substack{-m \leq j \leq m \\ j \neq 0}} P\left(w_{t+j} \mid w_t ; \theta\right)</script><p>其中其中，$\theta$为所有需要优化的变量<br>损失函数$J(\theta)$是负的平均对数似然：</p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{T} \log L(\theta)=-\frac{1}{T} \sum_{t=1}^T \sum_{\substack{-m \leq j \leq m \\ j \neq 0}} \log P\left(w_{t+j} \mid w_t ; \theta\right)</script><p>最小化损失函数，等同于最大化预测精度</p>
<p>问题：如何计算$P\left(w_{t+j} \mid w_t ; \theta\right)$？<br>回答：对于每个word都是用两个向量<br>$v_w$，当w是一个centor word<br>$u_w$，当w是一个context word<br>c代表centor word，o代表context word</p>
<script type="math/tex; mode=display">
P(o \mid c)=\frac{\exp \left(u_o^T v_c\right)}{\sum_{w \in V} \exp \left(u_w^T v_c\right)}</script><h3 id="Word2vec-prediction-function"><a href="#Word2vec-prediction-function" class="headerlink" title="Word2vec: prediction function"></a>Word2vec: prediction function</h3><script type="math/tex; mode=display">
P(o \mid c)=\frac{\exp \left(u_o^T v_c\right)}{\sum_{w \in V} \exp \left(u_w^T v_c\right)}</script><p>取幂使任何数都为正<br>点积比较o和c的相似性 ，$u^T v=u·v=\sum_{i=1}^n u_i v_i$点积越大则概率越大<br>分母：对整个词汇表进行标准化，从而给出概率分布</p>
<p>这是一个softmax function：$\mathbb{R}^n \rightarrow(0,1)$的例子</p>
<script type="math/tex; mode=display">
\operatorname{softmax}\left(x_i\right)=\frac{\exp \left(x_i\right)}{\sum_{j=1}^n \exp \left(x_j\right)}=p_i</script><p>softmax将任意值$x_i$映射到概率分布$p_i$</p>
<p>首先我们随机初始化$u_w \in \mathbb{R}^d$和$v_w \in \mathbb{R}^d$，然后使用梯度下降法更新参数</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial v_c} \log P(o \mid c) & =\frac{\partial}{\partial v_c} \log \frac{\exp \left(u_o^T v_c\right)}{\sum_{w \in V} \exp \left(u_w^T v_c\right)} \\
& =\frac{\partial}{\partial v_c}\left(\log \exp \left(u_o^T v_c\right)-\log \sum_{w \in V} \exp \left(u_w^T v_c\right)\right) \\
& =\frac{\partial}{\partial v_c}\left(u_o^T v_c-\log \sum_{w \in V} \exp \left(u_w^T v_c\right)\right) \\
& =u_o-\sum_{w \in V}\frac{\exp \left(u_w^T v_c\right) u_w}{\sum_{w \in V} \exp \left(u_w^T v_c\right)}
\end{aligned}</script><p>我们可以对上述结果重新排列如下，第一项是真正的上下文单词，第二项是预测的上下文单词。使用梯度下降法，模型的预测上下文将逐步接近真正的上下文。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial v_c} \log P(o \mid c) & =u_o-\frac{\sum_{w \in V} \exp \left(u_w^T v_c\right) u_w}{\sum_{w \in V} \exp \left(u_w^T v_c\right)} \\
& =u_o-\sum_{w \in V} \frac{\exp \left(u_w^T v_c\right)}{\sum_{w \in V} \exp \left(u_w^T v_c\right)} u_w \\
& =u_o-\sum_{w \in V} P(w \mid c) u_w
\end{aligned}</script><p>再对$u<em>o$进行偏微分计算，注意这里的$u_o$是$u</em>{w=o}$的简写，故可知</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial u_o} \sum_{w \in V} u_w^T v_c=\frac{\partial}{\partial u_o} u_o^T v_c=v_c</script><script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial}{\partial u_o} \log P(o \mid c) & =\frac{\partial}{\partial u_o} \log \frac{\exp \left(u_o^T v_c\right)}{\sum_{w \in V} \exp \left(u_w^T v_c\right)} \\
& =\frac{\partial}{\partial u_o}\left(\log \exp \left(u_o^T v_c\right)-\log \sum_{w \in V} \exp \left(u_w^T v_c\right)\right) \\
& =\frac{\partial}{\partial u_o}\left(u_o^T v_c-\log \sum_{w \in V} \exp \left(u_w^T v_c\right)\right) \\
& =v_c-\frac{\sum \frac{\partial}{\partial u_o} \exp \left(u_w^T v_c\right)}{\sum_{w \in V} \exp \left(u_w^T v_c\right)} \\
& =v_c-\frac{\exp \left(u_o^T v_c\right) v_c}{\sum_{w \in V} \exp \left(u_w^T v_c\right)} \\
& =v_c-\frac{\exp \left(u_o^T v_c\right)}{\sum_{w \in V} \exp \left(u_w^T v_c\right)} v_c \\
& =v_c-P(o \mid c) v_c \\
& =(1-P(o \mid c)) v_c
\end{aligned}</script><p>可以理解，当$P(o \mid c) \rightarrow 1$ ，即通过中心词c我们可以正确预测上下文词o，此时我们不需要调整$u_o$，反之则相应调整$u_o$。</p>
]]></content>
      <categories>
        <category>CS224n</category>
      </categories>
      <tags>
        <tag>CS224n</tag>
      </tags>
  </entry>
  <entry>
    <title>transfer learning迁移学习</title>
    <url>/2022/11/10/transfer-learning/</url>
    <content><![CDATA[<p>transfer learning迁移学习</p>
<span id="more"></span>
<p>神经网络可以从一个任务中学习到的知识，应用到另一个任务中</p>
<p>例子：<br>（1）image recognition图像识别 -&gt; radiology diagnose放射科诊断<br>（2）speech recignition system 语音识别系统 -&gt; wake word, trigger word detection system唤醒词 触发词 </p>
<p>迁移学习步骤：<br>（1）pre-training:先让神经网络在image recognition任务的数据集上进行训练，把神经网络所有的参数训练好。<br>（2）fine-tuning:再进行迁移学习，把神经网络的最后一层权重参数随机初始化，然后在radiology diagnose数据集上进行训练。<br>如果radiology diagnose数据集数据量少，你可以只随机初始化最后一层或最后两层参数，再在新数据集上进行训练。<br>如果radiology diagnose数据集数据量大，你可以随机初始化所有层参数，再在新数据集上进行训练。</p>
<p>为什么image recognition的知识对radiology diagnose有帮助？<br>很多低层次特征low level features，比如detecting edges边缘检测、detecting curves曲线检测、detecting positive objects阳性对象检测。在image recognition的知识对radiology diagnose有帮助。<br>能让radiology diagnose需要更少的学习数据并学习的更快。</p>
<p>应用迁移学习的前提：<br>（1）两个任务A、B的输入是相同的，比如都是图像或者都是音频<br>（2）迁移前的任务A比迁移后的任务B有更多的数据量<br>（3）迁移前的任务A的低层次特征low level features对迁移后的任务B有帮助</p>
]]></content>
  </entry>
  <entry>
    <title>Batch Normalization</title>
    <url>/2022/10/29/batchnorm/</url>
    <content><![CDATA[<p>Batch Norm</p>
<span id="more"></span>
<h2 id="1-Batch-Normalization是什么"><a href="#1-Batch-Normalization是什么" class="headerlink" title="1.Batch Normalization是什么"></a>1.Batch Normalization是什么</h2><p><strong>Batch Normalization</strong>是为了解决深度学习网络中<strong>Internal Covariate Shift</strong>问题而提出的对一个batch的数据进行Normalization的方法，优点是可以使用更大的学习率、更快的的训练速度、更不依赖于模型的初始化、对模型进行正则化（类似dropout的一部分效果）。</p>
<p>有研究表明对数据进行白化（whiten）操作可以加速模型的收敛，白化是将输入数据分布转换到均值为0、方差为1的正态分布。对于深度网络，每一层的输出都是下一层的输入。所以如果对每一层的输出进行白化操作可以加速收敛，并且每一层的输入的分布稳定。</p>
<h2 id="2-什么是Internal-Covariate-Shift"><a href="#2-什么是Internal-Covariate-Shift" class="headerlink" title="2.什么是Internal Covariate Shift"></a>2.什么是Internal Covariate Shift</h2><p>Internal Covariate Shift：深度神经网络有多层结构，每一层的输出是下一层的输入，所以每一层网络学习的是上一层输出的分布。而每一层参数的更新又会影响该层输出的分布，导致下一层的输入分布变化，使得学习难度加大。并且随着模型深度的增加，学习的难度也会增加。</p>
<h2 id="3-Batch-Normalization流程"><a href="#3-Batch-Normalization流程" class="headerlink" title="3.Batch Normalization流程"></a>3.Batch Normalization流程</h2><p>首先，Batch norm位于X=WU+B激活值获得之后，非线性函数变换之前</p>
<p>如果batch size为m，则在前向传播时每个节点都有m个输出，对每个节点的m个输出进行归一化。实现可以分为两步：</p>
<p>1.Standardization：对m个$x$进行标准化得到zero mean unit variance的 $\hat{x}$<br>2.scale and shift：对$\hat{x}$进行缩放和平移，得到最终的分布$y$，具有新的均值$\beta$和方差$\gamma$</p>
<p><strong>Input</strong>:Values of x over a mini-batch:$\mathcal{B}=\left{x_{1 \ldots m}\right}$;<br>Parameters to be learned: $\gamma, \beta$</p>
<p><strong>Output</strong>:$\left{y<em>i=\mathrm{BN}</em>{\gamma, \beta}\left(x_i\right)\right}$</p>
<p>mini-batch mean:<br>$\mu<em>{\mathcal{B}} \leftarrow \frac{1}{m} \sum</em>{i=1}^m x_i$</p>
<p>mini-batch variance:<br>$\sigma<em>{\mathcal{B}}^2 \leftarrow \frac{1}{m} \sum</em>{i=1}^m\left(x<em>i-\mu</em>{\mathcal{B}}\right)^2$</p>
<p>normalize:<br>$\hat{x}<em>i \leftarrow \frac{x_i-\mu</em>{\mathcal{B}}}{\sqrt{\sigma_{\mathcal{B}}^2+\epsilon}}$</p>
<p>scale and shift:<br>$y<em>i \leftarrow \gamma \hat{x}_i+\beta \equiv \mathrm{BN}</em>{\gamma, \beta}\left(x_i\right)$</p>
<p>Batch Normalizing Transform:</p>
<p>总体可以写为如下公式，所以，无论$x_i$原本的均值和方差是多少，通过Batch Norm后其均值和方差分别变为待学习的$\beta$和$\gamma$。<br>$y_i^{(b)}=B N\left(x_i\right)^{(b)}=\gamma \cdot\left(\frac{x_i^{(b)}-\mu\left(x_i\right)}{\sqrt{\sigma\left(x_i\right)^2+\epsilon}}\right)+\beta$</p>
<h3 id="BN在训练和测试的使用"><a href="#BN在训练和测试的使用" class="headerlink" title="BN在训练和测试的使用"></a>BN在训练和测试的使用</h3><p>这里没细看，感觉暂时不重要</p>
<h2 id="4-Batch-Normalization的作用"><a href="#4-Batch-Normalization的作用" class="headerlink" title="4.Batch Normalization的作用"></a>4.Batch Normalization的作用</h2><p><strong>可以使用更大的学习率</strong>，训练过程更加稳定，极大提高了训练速度。<br><strong>可以将bias置为0</strong>，因为Batch Normalization的Standardization过程会移除直流分量，所以不再需要bias。<br><strong>对权重初始化不再敏感</strong>，通常权重采样自0均值某方差的高斯分布，以往对高斯分布的方差设置十分重要，有了Batch Normalization后，对与同一个输出节点相连的权重进行放缩，其标准差$\sigma$也会放缩同样的倍数，相除抵消。<br><strong>对权重的尺度不再敏感</strong>，理由同上，尺度统一由$\gamma$参数控制，在训练中决定。<br><strong>深层网络可以使用sigmoid和tanh了</strong>，理由同上，BN抑制了梯度消失。<br><strong>Batch Normalization具有某种正则作用，不需要太依赖dropout，减少过拟合。</strong></p>
<p><strong>Reference</strong><br><a href="https://arxiv.org/pdf/1502.03167.pdf">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a></p>
]]></content>
      <categories>
        <category>deeplearning</category>
      </categories>
  </entry>
  <entry>
    <title>Optimization algorithms:SGD/AdaGrad/Adam</title>
    <url>/2022/09/20/optimize/</url>
    <content><![CDATA[<p>吴恩达deeplearning.ai专项课程</p>
<span id="more"></span>
<h2 id="指数加权-滑动-平均-exponentially-weighted（moving）averages"><a href="#指数加权-滑动-平均-exponentially-weighted（moving）averages" class="headerlink" title="指数加权(滑动)平均 exponentially weighted（moving）averages"></a>指数加权(滑动)平均 exponentially weighted（moving）averages</h2><p>$v<em>t=\beta v\</em>{t-1}+(1-\beta) \theta<em>t$<br>$v</em>{0}=0$<br>$\beta = 0.9$</p>
<p>其中$v<em>t$为迭代次数为t的时刻的指数加权平均值，我们一般设置$v_0=0$，$\beta = 0.9$。<br>指数加权平均大概为最近$\frac{1}{1-\beta}$次迭代的平均，因为$(1-(1-\beta))^{\frac{1}{1-\beta}}\approx \frac{1}{e}$。<br>提示：$\lim </em>{x \rightarrow \infty}\left(1-\frac{1}{x}\right)^x=\frac{1}{\mathrm{e}}$</p>
<h3 id="偏差修正bias-correction"><a href="#偏差修正bias-correction" class="headerlink" title="偏差修正bias correction"></a>偏差修正bias correction</h3><p>因为$v_0=0$，所以指数加权平均刚开始偏差比较大。这时候我们可以用偏差修正bias correction:<br>$v_t:=\frac{v_t}{1-\beta^t}$<br>指数加权平均在迭代几步之后一般都没什么偏差了，开始的偏差无伤大雅，所以偏差修正一般很少用。</p>
<h2 id="梯度下降的Challenges"><a href="#梯度下降的Challenges" class="headerlink" title="梯度下降的Challenges"></a>梯度下降的Challenges</h2><p>1.很难选择合适的学习率$\alpha$。学习率太小收敛很慢，学习率太大会阻碍收敛，导致损失函数在极小值附近震荡甚至发散。<br>2.梯度下降不应该对所有的参数使用相同的学习率$\alpha$，如果我们的数据是稀疏的，对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些。对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。<br>3.对于非凸的损失函数的优化，关键挑战在于避免陷入次优的局部极小值(suboptimal local minima)，还有鞍点(saddle points)。鞍点附近损失函数数值接近，形状像一个平台，梯度在所有维度上都接近于零，这使得SGD很难逃脱。</p>
<h2 id="一个框架回顾优化算法"><a href="#一个框架回顾优化算法" class="headerlink" title="一个框架回顾优化算法"></a>一个框架回顾优化算法</h2><p>深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt; AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam 这样的发展历程。这里我们用一个框架来梳理所有的优化算法，做一个对比。</p>
<p>首先定义：待优化参数：$w$，目标函数：$f(w)$，初始学习率$\alpha$。<br>而后，开始进行迭代优化。在每个epoch t：<br>1.计算目标函数关于当前参数的梯度：$g<em>t=\nabla f\left(w_t\right)$<br>2.根据历史梯度计算梯度的一阶矩first moment和二阶矩second moment：$m_t=\phi\left(g_1, g_2, \cdots, g_t\right) ; V_t=\psi\left(g_1, g_2, \cdots, g_t\right)$<br>3.计算当前时刻的下降梯度：$\eta_t=\frac{\alpha}{\sqrt{V_t}+\epsilon} m_t$<br>4.根据下降梯度进行更新：$w</em>{t+1}=w_t-\eta_t$</p>
<h3 id="SGD-with-Momentum"><a href="#SGD-with-Momentum" class="headerlink" title="SGD with Momentum"></a>SGD with Momentum</h3><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/optimize/1.png?raw=true" alt="1"><br>如果损失函数是狭长沟壑状的，梯度下降常常会在沟壑里震荡。momentum方法能帮助SGD加快收敛并抑制振荡。SGDM全称是SGD with momentum，在SGD基础上引入了一阶动量：</p>
<script type="math/tex; mode=display">
m_t=\beta_1 \cdot m_{t-1}+\left(1-\beta_1\right) \cdot g_t</script><p>一阶动量是各个时刻梯度的指数加权平均值，约等于最近$1 /\left(1-\beta_1\right)$个时刻的梯度向量和的平均值。<br>也就是说，t时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。$\beta_1$的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。</p>
<h3 id="Nesterov-accelerated-gradient"><a href="#Nesterov-accelerated-gradient" class="headerlink" title="Nesterov accelerated gradient"></a>Nesterov accelerated gradient</h3><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/optimize/2.png?raw=true" alt="2"><br>NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进，改进点在于步骤1。我们知道在时刻t的下降方向主要是由累积动量决定的，自己目前梯度方向只决定0.1，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG在步骤1，不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向：</p>
<script type="math/tex; mode=display">
g_t=\nabla f\left(w_t- \frac{\alpha}{\sqrt{V_{t-1}}}m_{t-1}\right)</script><script type="math/tex; mode=display">
\begin{aligned}
m_t &=\beta_1 m_{t-1}+(1-\beta_1) \nabla_\theta J\left(\theta-\beta_1 m_{t-1}\right) \\
\theta &=\theta-m_t
\end{aligned}</script><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>此前我们都没有用到二阶动量。二阶动量的出现，才意味着“自适应学习率”优化算法时代的到来。SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些。<br>怎么样去度量历史更新频率呢？那就是二阶动量，对应于某一维度上，迄今为止所有梯度值的平方和：</p>
<script type="math/tex; mode=display">
V_{t,i}=\sum_{\tau=1}^t g_{\tau,i}^2</script><p>t代表t时刻，i代表向量的第i个分量<br>我们再回顾一下步骤3中的下降梯度：</p>
<script type="math/tex; mode=display">
\eta_t=\frac{\alpha}{\sqrt{V_t}} m_t</script><p>可以看出，此时实质上的学习率由$\alpha$变成了$\frac{\alpha}{\sqrt{V_t}}$，$\alpha$一般设置为0.01。 一般为了避免分母为0，会在分母上加一个小的平滑项$\epsilon$(通常是1e−8的量级)。因此$\sqrt{V_t}+\epsilon$是恒大于0的。而且某一维度上参数更新越频繁，这个维度上二阶动量越大，学习率就越小。<br>这一方法在稀疏数据场景下表现非常好。但也存在一些问题：因为$\sqrt{V_t}+\epsilon$是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法有效学习</p>
<h3 id="AdaDelta-RMSProp"><a href="#AdaDelta-RMSProp" class="headerlink" title="AdaDelta / RMSProp"></a>AdaDelta / RMSProp</h3><p>RMSProp全称：Root Mean Square prop<br>由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。<br>修改的思路很简单。前面我们讲到，指数加权平均值大约就是过去$\frac{1}{1-\beta_1}$个时刻的平均值，因此我们用这一方法来计算二阶累积动量：</p>
<script type="math/tex; mode=display">
V_t=\beta_2 * V_{t-1}+\left(1-\beta_2\right) g_t^2</script><p>这就避免了二阶动量持续累积，导致训练过程提前结束的问题了。</p>
<h3 id="Adam-Adaptive-Moment-Estimation-自适应矩估计"><a href="#Adam-Adaptive-Moment-Estimation-自适应矩估计" class="headerlink" title="Adam(Adaptive Moment Estimation):自适应矩估计"></a>Adam(Adaptive Moment Estimation):自适应矩估计</h3><p>谈到这里，Adam和Nadam的出现就很自然而然了——它们是前述方法的集大成者。我们看到，SGD-M在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。<br>SGD的一阶动量：</p>
<script type="math/tex; mode=display">
m_t=\beta_1 m_{t-1}+\left(1-\beta_1\right) \cdot g_t</script><p>加上AdaDelta的二阶动量：</p>
<script type="math/tex; mode=display">
V_t=\beta_2 \cdot V_{t-1}+\left(1-\beta_2\right) g_t^2</script><p>优化算法里最常见的两个超参数$\beta_1, \beta_2$就都在这里了，前者控制一阶动量，后者控制二阶动量。实际使用过程中，参数的经验值是：<br>$\beta_1=0.9, \beta_2=0.999$<br>初始化：$m_0=0, V_0=0$</p>
<p>在初期，$m_t, V_t$都会接近于0，这个估计是有问题的。因此我们常常根据下式进行指数移动平均值的偏差修正：<br>$\tilde{m}_t=m_t /\left(1-\beta_1^t\right)$<br>$\tilde{V}_t=V_t /\left(1-\beta_2^t\right)$</p>
<h3 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h3><p>我们说Adam是集大成者，把Adam加上Nesterov，就是Nadam：</p>
<script type="math/tex; mode=display">
g_t=\nabla f\left(w_t- \frac{\alpha}{\sqrt{V_{t-1}}}m_{t-1}\right)</script><p>这就是Nesterov + Adam = Nadam了。</p>
<h3 id="Adam的缺点："><a href="#Adam的缺点：" class="headerlink" title="Adam的缺点："></a>Adam的缺点：</h3><p>1.<strong>可能不收敛</strong><br>AdaDelta和Adam的二阶动量是固定时间窗口内的累积，随着时间窗口的变化，遇到的数据可能发生巨变，使得$V<em>t$可能会时大时小，不是单调变化。这就可能在训练后期引起学习率的震荡，导致模型无法收敛。可以令：<br>$V_t=\max \left(\beta_2 \cdot V</em>{t-1}+\left(1-\beta<em>2\right) g_t^2, V</em>{t-1}\right)$<br>通过这样的修改，就保证了$\left|V<em>t\right| \geq\left|V</em>{t-1}\right|$，从而使学习率单调递减<br>2.<strong>可能错过全局最优</strong><br>深度神经网络往往包含大量的参数，在这样一个维度极高的空间内，非凸的目标函数往往起起伏伏，拥有无数个高地和洼地。<br>改进Adam的方法：前期用Adam，享受Adam快速收敛的优势；后期切换到SGD，慢慢寻找最优解。</p>
]]></content>
      <categories>
        <category>optimize</category>
      </categories>
      <tags>
        <tag>optimize</tag>
      </tags>
  </entry>
  <entry>
    <title>神经网络参数Xavier初始化</title>
    <url>/2022/09/15/Xavier/</url>
    <content><![CDATA[<p>参考资料：<br>吴恩达深度学习专项课程： deeplearning.ai<br><span id="more"></span></p>
<p>在训练deep neural network时会遇到梯度消失/梯度爆炸的问题（vanishing/exploding gradient)，Xavier初始化能缓解这个问题（不能完全解决）</p>
<p>Xavier初始化参数矩阵</p>
<script type="math/tex; mode=display">
\begin{array}{|c|c|c|c|}
\hline \text { 激活函数 } & \text { sigmoid } & \text { relu } & \tanh \\
\hline \begin{array}{c}
\text { 均匀分布 } \\
\text { (给出取值时的上下限) }
\end{array} & \pm \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}} & \pm \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}} \times \sqrt{2} & \pm \sqrt{\frac{6}{n_{\text {in }}+n_{\text {out }}}} \times 4 \\
\hline \begin{array}{c}
\text { 高斯分布 } \\
\text { (给出取值时的标准差) }
\end{array} & \sqrt{\frac{2}{n_{\text {in }}+n_{\text {out }}}} & \sqrt{\frac{2}{n_{\text {in }}+n_{\text {out }}}} \times \sqrt{2} & \sqrt{\frac{2}{n_{\text {in }}+n_{\text {ont }}}} \times 4 \\
\hline
\end{array}</script><h2 id="以均匀分布为例，证明均匀分布上下限为-sqrt-frac-6-n-i-n-n-text-out"><a href="#以均匀分布为例，证明均匀分布上下限为-sqrt-frac-6-n-i-n-n-text-out" class="headerlink" title="以均匀分布为例，证明均匀分布上下限为$\sqrt{\frac{6}{n{i n}+n{\text {out }}}}$"></a>以均匀分布为例，证明均匀分布上下限为$\sqrt{\frac{6}{n<em>{i n}+n</em>{\text {out }}}}$</h2><h2 id="随机变量乘积的方差-mathrm-D-mathrm-X-mathrm-Y-展开式"><a href="#随机变量乘积的方差-mathrm-D-mathrm-X-mathrm-Y-展开式" class="headerlink" title="随机变量乘积的方差$\mathrm{D}(\mathrm{X}\mathrm{Y})$展开式:"></a>随机变量乘积的方差$\mathrm{D}(\mathrm{X}\mathrm{Y})$展开式:</h2><p>学过概率论，有随机变量的期望和方差公式：<br>$1.\mathrm{E}(\mathrm{X}\mathrm{Y})=\mathrm{E}(\mathrm{X})\mathrm{E}(\mathrm{Y})$<br>$2.\mathrm{D}(\mathrm{X})=\mathrm{E}\left(\mathrm{X}^2\right)-[\mathrm{E}(\mathrm{X})]^2$<br>$3.\mathrm{D}(\mathrm{X}+\mathrm{Y})=\mathrm{D}(\mathrm{X})+\mathrm{D}(\mathrm{Y})$<br>由1.2.3推出<br>$\text { 1.2.3 } \Rightarrow \mathrm{D}(\mathrm{X}\mathrm{Y})=\mathrm{D}(\mathrm{X}) \mathrm{D}(\mathrm{Y})+\mathrm{D}(\mathrm{X})[\mathrm{E}(Y)]^2+\mathrm{D}(\mathrm{Y}) [\mathrm{E}(X)]^2$<br>证明过程如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\mathrm{D}(\mathrm{X}\mathrm{Y})=\mathrm{E}[(\mathrm{X} \mathrm{Y})^2]-[\mathrm{E}(\mathrm{X} \mathrm{Y})]^2 \\
&=\mathrm{E} (\mathrm{X}^2) \mathrm{E} (\mathrm{Y}^2)-[\mathrm{E}(\mathrm{X})]^2[\mathrm{E}(\mathrm{Y})]^2 \\
&=\left(\mathrm{D}(\mathrm{X})+\mathrm{E}^2 (\mathrm{X})\right)\left(\mathrm{D}(\mathrm{Y})+\mathrm{E}^2 (\mathrm{Y})\right)-[\mathrm{E}(\mathrm{X})]^2[\mathrm{E}(\mathrm{Y})]^2\\
&=\mathrm{D}(\mathrm{X}) \mathrm{D}(\mathrm{Y})+\mathrm{D}(\mathrm{X}) \mathrm{E}^2 (\mathrm{Y})+\mathrm{D}(\mathrm{Y}) \mathrm{E}^2 (\mathrm{X})
\end{aligned}</script><p>上面第三行是因为：</p>
<p>$\mathrm{D}(\mathrm{X})=\mathrm{E}\left(\mathrm{X}^2\right)-\mathrm{E}^2 (\mathrm{X})$<br>$\mathrm{D}(\mathrm{Y})=\mathrm{E}\left(\mathrm{Y}^2\right)-\mathrm{E}^2 (\mathrm{Y})$<br>$\mathrm{E}\left(\mathrm{X}^2\right)=\mathrm{D}(\mathrm{X})+\mathrm{E}^2 (\mathrm{X})$<br>$\mathrm{E}\left(\mathrm{Y}^2\right)=\mathrm{D}(\mathrm{Y})+\mathrm{E}^2 (\mathrm{Y})$</p>
<p>对每个神经元的输入$z$：有$z=\sum_{i=1}^n w_i x_i$，其中$n$是上一层神经元的数量。根据我们刚才证明的概率统计里的两个随机变量乘积的方差展开式：<br>$\operatorname{Var}(w_i x_i)=E^2[w_i] \operatorname{Var}(x_i)+E^2[x_i] \operatorname{Var}(w_i)+\operatorname{Var}(w_i) \operatorname{Var}(x_i)$<br>如果$\mathrm{E}(x_i)=\mathrm{E}(w_i)=0$（可以通过批量归一化Batch Normalization来满足），那么就有：</p>
<script type="math/tex; mode=display">
\operatorname{Var}(z)=\sum_{i=1}^n \operatorname{Var}(x_i) \operatorname{Var}(w_i)</script><p>如果随机变量$x_i$和$w_i$再满足独立同分布的话：</p>
<script type="math/tex; mode=display">
\operatorname{Var}(z)=\sum_{i=1}^n \operatorname{Var}(x_i) \operatorname{Var}(w_i)=n \operatorname{Var}(w) \operatorname{Var}(x)</script><p>整个大型前馈神经网络无非就是一个超级大映射，将原始样本稳定的映射成它的类别。也就是将样本空间映射到类别空间。试想，如果样本空间与类别空间的分布差异很大，比如说类别空间特别稠密，样本空间特别稀疏辽阔，那么在类别空间得到的用于反向传播的误差丢给样本空间后简直变得微不足道，也就是会导致模型的训练非常缓慢。同样，如果类别空间特别稀疏，样本空间特别稠密，那么在类别空间算出来的误差丢给样本空间后简直是爆炸般的存在，即导致模型发散震荡，无法收敛。因此，我们要让样本空间与类别空间的分布差异（密度差别）不要太大，也就是要让它们的方差尽可能相等。因此为了得到$\operatorname{Var}(z)=\operatorname{Var}(x)$，只能让$n*\operatorname{Var}(w)=1$，也就是$\operatorname{Var}(w)=1/n$。<br>同样的道理，正向传播时是从前往后计算的，因此 $\operatorname{Var}(w)=1 /\left(n<em>{i n}\right)$ ，反向传播时是从后往前计算的，因此 $\operatorname{Var}(w)=1 /\left(n</em>{\text {out }}\right)$。然而$n<em>{in}$和$n</em>{out}$往往不相等啊，怎么办呢？所以就取他们的均值就好啦~即：<br> 令$\operatorname{Var}(w)=\frac{2}{n<em>{i n}+n</em>{\text {out }}}$<br>因为均匀分布的方差有：</p>
<script type="math/tex; mode=display">
\operatorname{Var}(w)=\frac{(b-a)^2}{12}</script><script type="math/tex; mode=display">
\frac{(b-a)^2}{12}=\frac{2}{n_{in}+n_{out}}</script><p>解得<br>$a=-\frac{\sqrt{6}}{\sqrt{n<em>{\text {in }}+n</em>{\text {out }}}}$<br>$b=\frac{\sqrt{6}}{\sqrt{n<em>{\text {in }}+n</em>{\text {out }}}}$<br>所以</p>
<script type="math/tex; mode=display">
w \sim U\left[-\frac{\sqrt{6}}{\sqrt{n_{\text {in }}+n_{\text {out }}}}, \frac{\sqrt{6}}{\sqrt{n_{\text {in }}+n_{\text {out }}}}\right]</script><p>我们证明了均匀分布上下限为$\sqrt{\frac{6}{n<em>{i n}+n</em>{\text {out }}}}$</p>
]]></content>
      <categories>
        <category>Deeplearning</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>Drop_out 正则化</title>
    <url>/2022/09/14/drop-out/</url>
    <content><![CDATA[<p>参考资料：<br>吴恩达深度学习专项课程： deeplearning.ai</p>
<span id="more"></span>
<p>Dropout(Srivastava et al., 2014) 提供了正则化一大类模型的方法，计算方便且功能强大。Dropout可以被认为是集成大量深层神经网络的实用Bagging方法。Bagging涉及训练多个模型，并在每个测试样本上评估 多个模型。当每个模型都是一个很大的神经网络时，这似乎是不切实际的，因为训 练和评估这样的网络需要花费很多运行时间和内存。通常我们只能集成五至十个神经网络，如Szegedy et al. (2014a)集成了六个神经网络赢得 ILSVRC，超过这个数量 就会迅速变得难以处理。Dropout提供了一种廉价的Bagging集成近似，能够训练和评估<strong>指数级</strong>数量的神经网络。</p>
<h2 id="Drop-out原理"><a href="#Drop-out原理" class="headerlink" title="Drop_out原理"></a>Drop_out原理</h2><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/Drop_out/2.png?raw=true" alt="1"></p>
<p>drop_out的过程很简单，可以明显地减少过拟合现象，可以减少特征检测器(feature detectors)（隐层节点）间的相互作用。神经网络每一层有一个超参数Keep_prob，为这一层隐藏单元保留的几率。对参数较多的$W^{[l]}$，容易过拟合，可以把Keep_prob设置的低些。不容易过拟合的层，可以把Keep_prob设置的高些，甚至为1。</p>
<p>通常输入层不应用drop_out。</p>
<h2 id="inverted-dropout"><a href="#inverted-dropout" class="headerlink" title="inverted dropout"></a>inverted dropout</h2><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/Drop_out/1.png?raw=true" alt="2"></p>
<p>反向随机失活(inverted dropout)方法，通过除以 keep-prob，确保$a^{[l]}$的期望值不变。inverted dropout可以使应用dropout之后，每一层的输出期望值保持稳定。确保在测试阶段不执行 dropout 来调整数值范围，激活函数的预期结果也不会发生变化。</p>
<h2 id="dropout缺点downside-代价函数𝐽不再被明确定义"><a href="#dropout缺点downside-代价函数𝐽不再被明确定义" class="headerlink" title="dropout缺点downside:代价函数𝐽不再被明确定义"></a>dropout缺点downside:代价函数𝐽不再被明确定义</h2><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/Drop_out/3.png?raw=true" alt="3"></p>
<p>为什么drop_out能正则化？因为该单元的输入可能随时被清除，不依赖于任何一个特征，不会太依赖某些局部的特征，使模型泛化性更强。drop_out 将产生收缩权重的平方范数的效果，和之前讲的l2正则化类似。</p>
<p>计算视觉中，图片像素多，输入量非常大。而数据量一般不足，容易过拟合。所以 dropout 在计算机视觉中应用得比较频繁，几乎是计算机视觉研究人员默认选择。</p>
<p>drop_out缺点downside：就是代价函数 不再被明确定义</p>
<p>Reference:</p>
<p>Hinton G E, Srivastava N, Krizhevsky A, et al. Improving neural networks by preventing co-adaptation of feature detectors[J]. arXiv preprint arXiv:1207.0580, 2012.<br>Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.<br>Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.<br>Srivastava N. Improving neural networks with dropout[J]. University of Toronto, 2013, 182.<br>Bouthillier X, Konda K, Vincent P, et al. Dropout as data augmentation[J]. arXiv preprint arXiv:1506.08700, 2015.</p>
]]></content>
      <categories>
        <category>Deeplearning</category>
      </categories>
      <tags>
        <tag>Deeplearning</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树</title>
    <url>/2022/06/01/Decision-tree/</url>
    <content><![CDATA[<p>决策树</p>
<span id="more"></span>
<p>决策树的生成是一个递归过程，有三种情形会导致递归返回：<br>（1）当前结点包含的样本全属于同一类别，无需划分<br>（2）当前属性集为空，或是所有样本的所有属性上取值相同，无法划分<br>（3）当前结点包含的样本集合为空，不能划分</p>
<p>决策树划分目的：随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高</p>
<h2 id="ID3决策树算法："><a href="#ID3决策树算法：" class="headerlink" title="ID3决策树算法："></a>ID3决策树算法：</h2><p>以<strong>信息增益</strong>为准则选择划分属性：</p>
<p><strong>信息熵</strong>（information entropy）：<strong>度量样本集合纯度</strong>的最常用指标，假定当前样本集合D中第k类样本所占比例为$p_{k}(k=1,2, \ldots,|\mathcal{Y}|)$，则D的信息熵定义为：</p>
<script type="math/tex; mode=display">
\operatorname{Ent}(D)=-\sum_{k=1}^{|\mathcal{Y}|} p_{k} \log _{2} p_{k}</script><p>计算信息熵时约定，若$p=0$，则$p \log <em>{2} p=0$<br>$0 &lt; \operatorname{Ent}(D)  &lt; \log </em>{2}|\mathcal{Y}|$，$\operatorname{Ent}(D)$的值越小，则D的纯度越高。<br>假定离散属性a有V个可能的取值$\left{a^{1}, a^{2}, \ldots, a^{V}\right}$，若使用a来对样本集合D进行划分，则会产生V个分支结点，其中第v个分支结点包含了D中所有在属性a上取值为$a^{v}$的样本，记为$D^{v}$。我们可以计算出$D^{v}$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$\left|D^{v}\right| /|D|$，即样本数越多的分支结点的影响越大，于是可计算出<strong>属性a对样本集D进行划分所获得的信息增益</strong>（information gain）</p>
<script type="math/tex; mode=display">
\operatorname{Gain}(D, a)=\operatorname{Ent}(D)-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Ent}\left(D^{v}\right)</script><p>信息增益越大，则意味着使用属性a来进行划分所获得的纯度提升越大。</p>
<h2 id="C4-5决策树算法："><a href="#C4-5决策树算法：" class="headerlink" title="C4.5决策树算法："></a>C4.5决策树算法：</h2><p>信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，著名的C4.5决策树算法使用<strong>增益率（gain ratio）</strong>来选择最优划分属性。增益率定义为：</p>
<script type="math/tex; mode=display">
\text { Gain_ratio }(D, a)=\frac{\operatorname{Gain}(D, a)}{\operatorname{IV}(a)}</script><p>其中</p>
<script type="math/tex; mode=display">
\operatorname{IV}(a)=-\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \log _{2} \frac{\left|D^{v}\right|}{|D|}</script><p>称为属性a的固有值（intrinsic value）。属性a的可能取值数目越多（即V越大），则IV（a）的值通常越大。</p>
<h2 id="CART决策树算法："><a href="#CART决策树算法：" class="headerlink" title="CART决策树算法："></a>CART决策树算法：</h2><p>CART决策树对连续属性采用二分法(bi-partition)<br>数据集D的纯度可用基尼指数（Gini index）来度量：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\operatorname{Gini}(D) &=\sum_{k=1}^{|\mathcal{Y}|} \sum_{k^{\prime} \neq k} p_{k} p_{k^{\prime}} \\
&=1-\sum_{k=1}^{|\mathcal{Y}|} p_{k}^{2}
\end{aligned}</script><p>$\operatorname{Gini}(D)$反映了从数据集D中随机抽取两个样本，其类别标记不一致的概率。因此$\operatorname{Gini}(D)$越小，数据集D的纯度越高。属性a的基尼指数定义为：</p>
<script type="math/tex; mode=display">
\text { Gini_index }(D, a)=\sum_{v=1}^{V} \frac{\left|D^{v}\right|}{|D|} \operatorname{Gini}\left(D^{v}\right)</script><h2 id="剪枝（pruning）"><a href="#剪枝（pruning）" class="headerlink" title="剪枝（pruning）"></a>剪枝（pruning）</h2><p>剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段。</p>
<p>预剪枝（prepruning）：基于“贪心”，带来欠拟合风险。降低过拟合风险，显著减少决策树训练时间和测试时间开销。<br>后剪枝（postpruning）：比预剪枝决策树保留了更多的分支。欠拟合风险小，泛化性能优于预剪枝决策树。但训练时间开销比未剪枝决策树和预剪枝决策树要大得多。</p>
]]></content>
  </entry>
  <entry>
    <title>EM algorithm</title>
    <url>/2022/05/01/EM/</url>
    <content><![CDATA[<p>EM算法</p>
<span id="more"></span>
<h1 id="（1）琴生不等式（Jensen’s-inequality）"><a href="#（1）琴生不等式（Jensen’s-inequality）" class="headerlink" title="（1）琴生不等式（Jensen’s inequality）"></a>（1）琴生不等式（Jensen’s inequality）</h1><p>如果函数$f(X)$是一个凸函数(convex function)，$X$是随机变量。我们有</p>
<script type="math/tex; mode=display">\mathrm{E}[f(X)] \geq f(\mathrm{E} [X])</script><p>相反，如果函数$f(X)$是一个凹函数(concave function)，则有</p>
<script type="math/tex; mode=display">\mathrm{E}[f(X)] \leq f(\mathrm{E} [X])</script><p>如果函数$f(X)$是严格凸(strictly convex)的，使不等式等号成立的充要条件是$P(X=\mathrm{E}[X]) = 1$，随机变量X是一个常数。</p>
<p><strong>凸函数和凹函数的定义</strong>：若一个函数二阶可导，且$f^{\prime \prime}(x) \geq 0(\forall x \in \mathbb{R})$。或者如果函数的输入是一个向量，其Hessian矩阵是半正定的$H \succeq 0$。则函数是个凸函数。注意这是充分不必要条件，凸函数也可能是不可导的。相反，如果$f^{\prime \prime}(x) \leq 0(\forall x \in \mathbb{R})$或者$H \preceq 0$，则函数是凹函数。<br>如果$f^{\prime \prime}(x) &gt; 0(\forall x \in \mathbb{R})$或者$H \succ 0$，则称$f(X)$是严格凸(strictly convex)的。凹函数同理。</p>
<h1 id="（2）the-EM-algorithm"><a href="#（2）the-EM-algorithm" class="headerlink" title="（2）the EM algorithm"></a>（2）the EM algorithm</h1><p>假设我们有一个估计问题，有一个训练集：$\left{x^{(1)}, \ldots, x^{(m)}\right}$，其中包含m个独立的样本。我们的模型是$p(x, z; \theta)$。我们希望拟合模型的参数$\theta$，模型的对数似然是：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ell(\theta) &=\sum_{i=1}^{m} \log p(x ; \theta) \\
&=\sum_{i=1}^{m} \log \sum_{z} p(x, z ; \theta)
\end{aligned}</script><p>其中$z^{(i)}$是隐变量，是未知的，观察不到的。<strong>EM算法能有效地求解含隐变量最大似然估计问题</strong>。因为$z^{(i)}$是隐变量，显式地求解这个似然$\ell(\theta)$很难。EM算法的策略就是不断地为对数似然构建一个下界(lower-bound)(E-step)，然后优化这个下界(M-step)。</p>
<p>对每个样本，随机变量$z^{(i)}$具有某种分布$Q<em>{i}$，满足$(\sum</em>{z} Q<em>{i}(z)=1, Q</em>{i}(z) \geq 0)$。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ell(\theta)= \sum_{i} \log p\left(x^{(i)} ; \theta\right) &=\sum_{i} \log \sum_{z^{(i)}} p\left(x^{(i)}, z^{(i)} ; \theta\right) \\
&=\sum_{i} \log \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)} \\
& \geq \sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}
\end{aligned}</script><p>最后一步利用了琴生不等式，因为$f(x)=\log (x)$ 是一个严格凹函数，其二阶导$f^{\prime \prime}(x)=-1 / x^{2}&lt;0$。<br>通过琴生不等式，我们有：</p>
<script type="math/tex; mode=display">
f\left(\mathrm{E}_{z^{(i)} \sim Q_{i}}\left[\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}\right]\right) \geq \mathrm{E}_{z^{(i)} \sim Q_{i}}\left[f\left(\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}\right)\right]</script><p>下标$z^{(i)} \sim Q<em>{i}$表明这个期望是关于变量$z^{(i)}$的，其分布为$Q</em>{i}$。</p>
<p>现在，对任意分布$Q<em>{i}$，我们给出了似然$\ell(\theta)$的下界。$Q</em>{i}$的分布有无数可能，我们选择哪一个比较好呢？给定当下对参数$\theta$的猜测，我们希望选择一个$Q_{i}$，使这个下界紧贴着我们的似然$\ell(\theta)$。根据琴生不等式等号成立的条件，我们需要</p>
<script type="math/tex; mode=display">
\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}=c</script><p>其中c是常量，说明：</p>
<script type="math/tex; mode=display">
Q_{i}\left(z^{(i)}\right) \propto p\left(x^{(i)}, z^{(i)} ; \theta\right)</script><p>而我们又知道：</p>
<script type="math/tex; mode=display">
\sum_{z} Q_{i}\left(z^{(i)}\right)=1</script><p>所以：</p>
<script type="math/tex; mode=display">
\begin{aligned}
Q_{i}\left(z^{(i)}\right) &=\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{\sum_{z} p\left(x^{(i)}, z ; \theta\right)} \\
&=\frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{p\left(x^{(i)} ; \theta\right)} \\
&=p\left(z^{(i)} \mid x^{(i)} ; \theta\right)
\end{aligned}</script><p>所以，使琴生不等式等号成立，我们只需要令$Q_{i}$等于随机变量$z^{(i)} \mid x^{(i)}$的后验分布。</p>
<h2 id="EM算法的步骤"><a href="#EM算法的步骤" class="headerlink" title="EM算法的步骤"></a>EM算法的步骤</h2><p>Repeat until convergence {<br>(E-step) For each $i$, set</p>
<script type="math/tex; mode=display">
Q_{i}\left(z^{(i)}\right):=p\left(z^{(i)} \mid x^{(i)} ; \theta\right)</script><p>(M-step) Set</p>
<script type="math/tex; mode=display">
\theta:=\arg \max _{\theta} \sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}</script><p>}</p>
<p>随着算法不断迭代，为什么这个算法会收敛呢？为什么对数似然会单调递增呢？假设我们迭代第t步，参数为$\theta^{(t)}$，在E-step我们选择$Q_{i}^{(t)}\left(z^{(i)}\right):=p\left(z^{(i)} \mid x^{(i)} ; \theta^{(t)}\right)$，这个选择确保琴生不等式等号成立，也就是在$\theta^{(t)}$时，似然紧贴着它的下界。我们有：</p>
<script type="math/tex; mode=display">
\ell\left(\theta^{(t)}\right)=\sum_{i} \sum_{z^{(i)}} Q_{i}^{(t)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta^{(t)}\right)}{Q_{i}^{(t)}\left(z^{(i)}\right)}</script><p>我们在M-step找一个$\theta^{(t+1)}$，</p>
<script type="math/tex; mode=display">
\theta^{(t+1)}:=\arg \max _{\theta} \sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}</script><p>我们有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\ell\left(\theta^{(t+1)}\right) & \geq \sum_{i} \sum_{z^{(i)}} Q_{i}^{(t)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta^{(t+1)}\right)}{Q_{i}^{(t)}\left(z^{(i)}\right)} \\
& \geq \sum_{i} \sum_{z^{(i)}} Q_{i}^{(t)}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta^{(t)}\right)}{Q_{i}^{(t)}\left(z^{(i)}\right)} \\
&=\ell\left(\theta^{(t)}\right)
\end{aligned}</script><p>所以</p>
<script type="math/tex; mode=display">
\ell\left(\theta^{(t+1)}\right) \geq \ell\left(\theta^{(t)}\right)</script><p>所以EM算法一定是单调收敛的。</p>
<p>其实可以把EM算法看成是coordinate ascent。E-step看成是固定$\theta$优化$Q$。M-step可以看成是固定$Q$优化$\theta$。</p>
<script type="math/tex; mode=display">
J(Q, \theta)=\sum_{i} \sum_{z^{(i)}} Q_{i}\left(z^{(i)}\right) \log \frac{p\left(x^{(i)}, z^{(i)} ; \theta\right)}{Q_{i}\left(z^{(i)}\right)}</script>]]></content>
      <categories>
        <category>EM</category>
      </categories>
      <tags>
        <tag>EM</tag>
      </tags>
  </entry>
  <entry>
    <title>支持向量机总结</title>
    <url>/2022/01/03/svm_summary/</url>
    <content><![CDATA[<span id="more"></span>
<p>支持向量机的优化目标为在约束下，最大化geometric margin：$\gamma$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\max _{\gamma, w, b} &\quad \gamma \\
\text { s.t. } &\quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \gamma, \quad i=1, \ldots, m \\
&\quad \|w\|=1
\end{aligned}</script><p>其中约束$\|w\|=1$是non-convex的，我们稍作改进</p>
<script type="math/tex; mode=display">
\begin{aligned} \max _{\gamma, w, b} &\quad \frac{\hat{\gamma}}{\|w\|} \\ \text { s.t. } &\quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq \hat{\gamma},\quad i=1, \ldots, m \end{aligned} \\</script><p>（functional margin和geometric margin的关系是：$\gamma = \frac{\hat{\gamma}}{\|w\|}$）<br>虽然我们摆脱了非凸的约束$\|w\|=1$，但是objective function: $\frac{\hat{\gamma}}{\|w\|}$依然是非凸的。我们没有任何现成的软件能求解这种优化问题。</p>
<p>为了解决以上问题，我们把functional margin固定为1: $\hat{\gamma}=1$，经过改进，现在 $\frac{1}{2}\|w\|^{2}$ 是一个凸二次函数(convex quadratic objective)，并且只有线性约束（linear constraints）。能够用commercial quadratic programming (QP software)求解。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\min _{\gamma, w, b} &\quad \frac{1}{2}\|w\|^{2} \\
\text { s.t. } &\quad y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1, \quad i=1, \ldots, m
\end{aligned}</script><hr>
<p>接下来我们讨论拉格朗日对偶，这将引导我们导出optimization problem的对偶形式</p>
<p>（1）这将允许我们使用Kernel，可以在非常高维的空间有效工作。</p>
<p>（2）对偶形式还允许我们推导出一种有效的算法来解决上述优化问题，这种算法通常比通用QP软件做得好得多。</p>
]]></content>
  </entry>
  <entry>
    <title>人工神经网络</title>
    <url>/2021/12/14/neural-network/</url>
    <content><![CDATA[<p>人工神经网络学习笔记，记录前馈传播信号和反向传播误差的矩阵表示</p>
<span id="more"></span>
<p>注意：神经网络输入层仅表示输入信号，输入节点不对输入值应用激活函数，历史就是这样规定的</p>
<p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/neural_network/截屏2021-12-14%20下午4.05.45.png?raw=true" alt="structure"></p>
<h2 id="前馈传播信号"><a href="#前馈传播信号" class="headerlink" title="前馈传播信号"></a>前馈传播信号</h2><p>人工神经网络层与层之间前馈传播方式：</p>
<script type="math/tex; mode=display">
X=W \cdot I</script><p>其中，W是权重矩阵，I是输入矩阵，X是组合调节后的信号，输入到下一层，然后应用激活函数，使神经网络的反应更像是生物神经元，激活函数为logistic function：$y=\frac{1}{1+e^{-x}}$</p>
<script type="math/tex; mode=display">
O=\operatorname{sigmoid}(X)</script><h2 id="反向传播误差"><a href="#反向传播误差" class="headerlink" title="反向传播误差"></a>反向传播误差</h2><p>忽略归一化因子分母，反向传播误差矩阵表示为：以输出层与挨着输出层的隐藏层为例，其他层之间也是一样的道理</p>
<script type="math/tex; mode=display">
\text { error }_{\text {hidden }}=W^{T}_{\text {hidden_output}} \cdot \text { error }_{\text {output }}</script><p>实践证明，这种相对简单的误差信号反馈方式，与我们先前相对复杂的方式一样有效！</p>
<h2 id="Cost-function："><a href="#Cost-function：" class="headerlink" title="Cost function："></a>Cost function：</h2><p>如果神经网络有n个输出节点：</p>
<script type="math/tex; mode=display">
E = \sum_{n}\left(t_{n}-o_{n}\right)^{2}</script><p>对权重$w_{j, k}$的偏导数为</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w_{j, k}}=\frac{\partial}{\partial w_{j, k}}\sum_{n}\left(t_{n}-o_{n}\right)^{2}</script><p>因为神经网络的结构，节点k的输出不依赖于其他不与它相连的链接，这可以让我们删除令人厌烦的求和运算。这是一个很有用的技巧。代价函数根本就不需要对所有输出节点求和。原因是节点的输出只取决于所连接的链接。这个过程在许多教科书中一略而过，这些教科书只是简单地声明了误差函数，却没有解释原因。所以我们有：</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w_{j, k}}=\frac{\partial}{\partial w_{j, k}}\left(t_{k}-o_{k}\right)^{2}</script><h2 id="gradient-descent："><a href="#gradient-descent：" class="headerlink" title="gradient descent："></a>gradient descent：</h2><p>这里我们以三层，每层三个节点的神经网络为例，用i表示输入层，j表示隐藏层，k表示输出层，利用链式法则：<br>$\frac{\partial E}{\partial w<em>{j, k}}=\frac{\partial E}{\partial o</em>{k}} \cdot \frac{\partial o<em>{k}}{\partial w</em>{j, k}}$<br>$\frac{\partial E}{\partial w<em>{j, k}}=-2\left(t</em>{k}-o<em>{k}\right) \cdot \frac{\partial o</em>{k}}{\partial w<em>{j, k}}$<br>$\frac{\partial E}{\partial w</em>{j, k}}=-2\left(t<em>{k}-o</em>{k}\right) \cdot \frac{\partial}{\partial w<em>{j, k}} \operatorname{sigmoid}\left(\Sigma</em>{j} w<em>{j, k} \cdot o</em>{j}\right)$<br>$\frac{\partial E}{\partial w<em>{j, k}}=-2\left(t</em>{k}-o<em>{k}\right) \cdot \operatorname{sigmoid}\left(\Sigma</em>{j} w<em>{j, k} \cdot o</em>{j}\right)\left(1-\operatorname{sigmoid}\left(\Sigma<em>{j} w</em>{j, k} \cdot o<em>{j}\right)\right) \cdot \frac{\partial}{\partial w</em>{j, k}}\left(\Sigma<em>{j} w</em>{j, k} \cdot o<em>{j}\right)$<br>$=-2\left(t</em>{k}-o<em>{k}\right) \cdot \operatorname{sigmoid}\left(\Sigma</em>{j} w<em>{j, k} \cdot o</em>{j}\right)\left(1-\operatorname{sigmoid}\left(\Sigma<em>{j} w</em>{j, k} \cdot o<em>{j}\right)\right) \cdot o</em>{j}$</p>
<p>同理，对于输入层和隐藏层，我们有</p>
<script type="math/tex; mode=display">
\frac{\partial E}{\partial w_{i j}}=-\left(e_{j}\right) \cdot \operatorname{sigmoid}\left(\Sigma_{i} w_{i j} \cdot o_{i}\right)\left(1-\operatorname{sigmoid}\left(\Sigma_{i} w_{i j} \cdot o_{i}\right)\right) \cdot o_{i}</script><p>梯度的矩阵形式：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cccc}
\Delta w_{1,1} & \Delta w_{2,1} & \Delta w_{3,1} & \cdots \\
\Delta w_{1,2} & \Delta w_{2,2} & \Delta w_{3,2} & \cdots \\
\Delta w_{1,3} & \Delta w_{2,3} & \Delta w_{1 j} k & \cdots \\
\cdots & \cdots & \cdots & \cdots
\end{array}\right) =
\left(\begin{array}{cccc}
E_{1} * S_{1}\left(1-S_{1}\right) \\
E_{2} * S_{2}\left(1-S_{3}\right) \\
\cdots \\
E_{k} * S_{k}\left(1-S_{k}\right) \\
\cdots
\end{array}\right)
\cdot
\left(\begin{array}{cccc}
O_{1} & O_{2} & \dots & O_{j} & \dots
\end{array}\right)</script><h2 id="梯度的反向传播"><a href="#梯度的反向传播" class="headerlink" title="梯度的反向传播"></a>梯度的反向传播</h2><h3 id="cost-function对输出层输入值-z-3-的梯度-nabla-z-3-J-1"><a href="#cost-function对输出层输入值-z-3-的梯度-nabla-z-3-J-1" class="headerlink" title="cost function对输出层输入值$z{3}$的梯度$\nabla{z^{(3)}} J_{1}$"></a>cost function对输出层输入值$z<em>{3}$的梯度$\nabla</em>{z^{(3)}} J_{1}$</h3><p>我们以手写数字识别为例，手写字体为$20 \times 20$像素的灰度值图片，神经网络结构为三层，第一层为输入层，为401维$(20 \times 20+1)$，第二层为隐藏层，25+1维，第三层是输出层，10维，每一个输出代表第k类的概率。神经网络前馈传播信号过程如下：</p>
<script type="math/tex; mode=display">
\stackrel{a^{(1)} = x}{\longrightarrow}
\left(\begin{array}{cccc}
1 \\ x_{1} \\  x_{2} \\  \dots \\  x_{400} 
\end{array}\right)
\stackrel{z^{(2)} = \Theta^{(1)}a^{(1)}}{\longrightarrow}
\left(\begin{array}{cccc}
z_{1}^{(2)} \\  z_{2}^{(2)} \\  \dots \\  z_{25}^{(2)} 
\end{array}\right)
\stackrel{a^{(2)} = g \left( z^{(2)} \right)}{\longrightarrow}
\left(\begin{array}{cccc}
1 \\ a^{(2)}_{1} \\ a^{(2)}_{2} \\ \dots \\ a^{(2)}_{25}
\end{array}\right)
\stackrel{z^{(3)} = \Theta^{(2)}a^{(2)}}{\longrightarrow}
\left(\begin{array}{cccc}
z_{1}^{(3)} \\  z_{2}^{(3)} \\  \dots \\  z_{10}^{(3)} 
\end{array}\right)
\stackrel{a^{(3)} = g\left(z^{(3)}\right)=h_{\theta}(x)}{\longrightarrow}
\left(\begin{array}{cccc}
h_{\theta}(x)_{1} \\ h_{\theta}(x)_{2} \\ \dots \\ h_{\theta}(x)_{10}
\end{array}\right)</script><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/neural_network/截屏2021-12-16%20上午11.59.56.png?raw=true" alt="structure"></p>
<p>神经网络的cost function:</p>
<script type="math/tex; mode=display">
\begin{aligned} J(\theta)=& \frac{1}{m} \sum_{i=1}^{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right]+ \\ &\frac{\lambda}{2 m}\left[\sum_{j=1}^{25} \sum_{k=1}^{400}\left(\Theta_{j, k}^{(1)}\right)^{2}+\sum_{j=1}^{10} \sum_{k=1}^{25}\left(\Theta_{j, k}^{(2)}\right)^{2}\right] \end{aligned}</script><p>为了方便，我们把cost function的前半部分用$J_{1}$表示:</p>
<script type="math/tex; mode=display">
J_{1}=\frac{1}{m} \sum_{k=1}^{K}\left[-y_{k}^{(i)} \log \left(\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)-\left(1-y_{k}^{(i)}\right) \log \left(1-\left(h_{\theta}\left(x^{(i)}\right)\right)_{k}\right)\right]</script><p>因为<br>$\frac{\partial ln(h<em>{\theta}(x)</em>{k})}{\partial z^{(3)}<em>{k}}=\frac{1}{h</em>{\theta}(x)<em>{k}}\frac{dh</em>{\theta}(x)<em>{k}}{dz^{(3)}</em>{k}}=\frac{1}{h<em>{\theta}(x)</em>{k}}h<em>{\theta}(x)</em>{k}(1-h<em>{\theta}(x)</em>{k})=1-h<em>{\theta}(x)</em>{k}$<br>$\frac{\partial ln(1-h<em>{\theta}(x)</em>{k})}{\partial z^{(3)}<em>{k}}=\frac{-1}{1-h</em>{\theta}(x)<em>{k}}\frac{dh</em>{\theta}(x)<em>{k}}{dz^{(3)}</em>{k}}=\frac{-1}{1-h<em>{\theta}(x)</em>{k}}h<em>{\theta}(x)</em>{k}(1-h<em>{\theta}(x)</em>{k})=-h<em>{\theta}(x)</em>{k}$<br>$J<em>{1}$虽然有求和符号，但是求梯度分量的过程中我们知道，第k个分量$z^{(3)}</em>{k}$只与$h_{k}$有关，而与其他分量无关。所以求和符号在求梯度过程中可省略掉</p>
<script type="math/tex; mode=display">
\frac{\partial J_{1}}{\partial z^{(3)}_{k}} = -y_{k} (1-h_{\theta}(x)_{k}) - (1-y_{k})(-h_{\theta}(x)_{k}) = h_{\theta}(x)_{k}-y_{k}</script><p>$J_{1}$对$z^{(3)}$求梯度，经过化简最终我们有：</p>
<script type="math/tex; mode=display">
\nabla_{z^{(3)}} J_{1} = \vec{h_{\theta}} - \vec{y}</script><p>我们成功地求出了对输出层z3的梯度！而且对数似然链式求导和logistic函数链式求导后会约掉，数学之美！简洁之美！</p>
<h3 id="cost-function对隐藏层输出值-a-2-的梯度-nabla-a-2-J-1"><a href="#cost-function对隐藏层输出值-a-2-的梯度-nabla-a-2-J-1" class="headerlink" title="cost function对隐藏层输出值$a{2}$的梯度$\nabla{a^{(2)}} J_{1}$"></a>cost function对隐藏层输出值$a<em>{2}$的梯度$\nabla</em>{a^{(2)}} J_{1}$</h3><p>在前馈传播过程中，我们有：</p>
<script type="math/tex; mode=display">
z^{(3)} = \Theta^{(2)}a^{(2)}</script><p>接下来结论很重要，梯度的反向传播的求法：</p>
<p>我们以2维向量前馈传播为例，高维向量一样的道理！</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}y_{1} \\ y_{2} \end{array}\right) = \left(\begin{array}{c}a&b \\ c&d \end{array}\right) \left(\begin{array}{c}x_{1} \\ x_{2} \end{array}\right)=\Theta \left(\begin{array}{c}x_{1} \\ x_{2} \end{array}\right)</script><p>其中，$\vec{y}$是神经网络在下一层输入，$\vec{x}$是前一层输出，$\Theta$是权重矩阵</p>
<script type="math/tex; mode=display">
\Theta = \left(\begin{array}{c}a&b \\ c&d \end{array}\right)</script><p>我们有</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
y_{1} = a x_{1} + bx_{2} \\
y_{2} = c x_{1} + dx_{2}
\end{aligned}
\right.</script><p>cost function对向量$\vec{x}$求梯度，有</p>
<script type="math/tex; mode=display">
\nabla_{x} J_{1} = 
\left(\begin{array}{c}\frac{\partial J_{1}}{\partial x_{1}} \\ \frac{\partial J_{1}}{\partial x_{2}}  \end{array}\right) = \left(\begin{array}{c}\frac{\partial J_{1}}{\partial y_{1}}a + \frac{\partial J_{1}}{\partial y_{2}}c \\ \frac{\partial J_{1}}{\partial y_{1}}b+\frac{\partial J_{1}}{\partial y_{2}}d  \end{array}\right) = \left(\begin{array}{c}a&c\\ b&d  \end{array}\right)\left(\begin{array}{c}\frac{\partial J_{1}}{\partial y_{1}} \\ \frac{\partial J_{1}}{\partial y_{2}}  \end{array}\right) = \Theta^{T}\left(\begin{array}{c}\frac{\partial J_{1}}{\partial y_{1}} \\ \frac{\partial J_{1}}{\partial y_{2}}  \end{array}\right)</script><p>所以有</p>
<script type="math/tex; mode=display">
\nabla_{a_{2}} J_{1} = \Theta^{T} \nabla_{z_{3}} J_{1}</script><h3 id="梯度-nabla-z-3-J-1-与参数-Theta-2-的梯度-nabla-Theta-2-J-1-的关系："><a href="#梯度-nabla-z-3-J-1-与参数-Theta-2-的梯度-nabla-Theta-2-J-1-的关系：" class="headerlink" title="梯度$\nabla{z^{(3)}} J{1}$ 与参数$\Theta{2}$的梯度$\nabla{\Theta{2}} J{1}$的关系："></a>梯度$\nabla<em>{z^{(3)}} J</em>{1}$ 与参数$\Theta<em>{2}$的梯度$\nabla</em>{\Theta<em>{2}} J</em>{1}$的关系：</h3><p>在前馈传播过程中，我们有：</p>
<script type="math/tex; mode=display">
z^{(3)} = \Theta^{(2)}a^{(2)}</script><p>我们还是以2维向量前馈传播为例，高维向量一样的道理！</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}y_{1} \\ y_{2} \end{array}\right) = \left(\begin{array}{c}a&b \\ c&d \end{array}\right) \left(\begin{array}{c}x_{1} \\ x_{2} \end{array}\right)=\Theta \left(\begin{array}{c}x_{1} \\ x_{2} \end{array}\right)</script><p>其中，$\vec{y}$是神经网络在下一层输入，$\vec{x}$是前一层输出，$\Theta$是权重矩阵</p>
<script type="math/tex; mode=display">
\Theta = \left(\begin{array}{c}a&b \\ c&d \end{array}\right)</script><p>我们有</p>
<script type="math/tex; mode=display">
\left\{
\begin{aligned}
y_{1} = a x_{1} + bx_{2} \\
y_{2} = c x_{1} + dx_{2}
\end{aligned}
\right.</script><p>cost function对$\Theta_{2}$求梯度，有</p>
<script type="math/tex; mode=display">
\nabla_{\Theta_{2}} J_{1} = 
\left(\begin{array}{c}\frac{\partial J_{1}}{\partial a} &\frac{\partial J_{1}}{\partial b} \\ \frac{\partial J_{1}}{\partial c}&\frac{\partial J_{1}}{\partial d}   \end{array}\right) = \left(\begin{array}{c}\frac{\partial J_{1}}{\partial y_{1}}x_{1}&\frac{\partial J_{1}}{\partial y_{1}}x_{2} \\ \frac{\partial J_{1}}{\partial y_{2}}x_{1}&\frac{\partial J_{1}}{\partial y_{2}}x_{2}  \end{array}\right) = \left(\begin{array}{c}\frac{\partial J_{1}}{\partial y_{1}} \\ \frac{\partial J_{1}}{\partial y_{2}}  \end{array}\right) \left(\begin{array}{c} x_{1}&x_{2} \end{array}\right) =
\nabla_{y} J_{1} \left(\begin{array}{c} x_{1}&x_{2} \end{array}\right)</script><p>我们成功地求出了对参数$\Theta_{2}$的梯度！</p>
<script type="math/tex; mode=display">
\nabla_{\Theta_{2}} J_{1} = \nabla_{z_{3}} J_{1} \cdot a_{2}^{T}</script><h3 id="cost-function对隐藏层输入值-z-2-的梯度-nabla-z-2-J-1"><a href="#cost-function对隐藏层输入值-z-2-的梯度-nabla-z-2-J-1" class="headerlink" title="cost function对隐藏层输入值$z{2}$的梯度$\nabla{z^{(2)}} J_{1}$"></a>cost function对隐藏层输入值$z<em>{2}$的梯度$\nabla</em>{z^{(2)}} J_{1}$</h3><p>在前馈传播过程中，我们有：</p>
<script type="math/tex; mode=display">
a_{(2)} = sigmoid(z_{(2)})</script><p>对分量求偏导</p>
<script type="math/tex; mode=display">
\frac{\partial J_{1}}{\partial z^{(2)}_{k}}=\frac{\partial J_{1}}{\partial a^{(2)}_{k}} \frac{\partial a^{(2)}_{k}}{\partial z^{(2)}_{k}}
=\frac{\partial J_{1}}{\partial a^{(2)}_{k}} g(z^{(2)}_{k})(1-g(z^{(2)}_{k}))</script><p>把分量整合成向量的形式</p>
<script type="math/tex; mode=display">
\nabla_{z_{2}} J_{1} = \nabla_{a_{2}} J_{1} \ast g(z^{(2)})(1-g(z^{(2)}))</script><h3 id="cost-function对参数-Theta-1-的梯度-nabla-Theta-1-J-1"><a href="#cost-function对参数-Theta-1-的梯度-nabla-Theta-1-J-1" class="headerlink" title="cost function对参数$\Theta{1}$的梯度$\nabla{\Theta{(1)}} J{1}$"></a>cost function对参数$\Theta<em>{1}$的梯度$\nabla</em>{\Theta<em>{(1)}} J</em>{1}$</h3><p>在前馈传播过程中，我们有：</p>
<script type="math/tex; mode=display">
z_{(2)} = \Theta^{(1)} a^{(1)}</script><script type="math/tex; mode=display">
\nabla_{\Theta_{1}} J_{1} = \nabla_{z_{z}} J_{1} \cdot a_{1}^{T}</script>]]></content>
  </entry>
  <entry>
    <title>核函数 Kernel</title>
    <url>/2021/12/08/kernel/</url>
    <content><![CDATA[<p>参考资料:<br>周志华《机器学习》<br>吴恩达 斯坦福大学机器学习 CS229</p>
<span id="more"></span>
<h2 id="问题由来：如何处理线性不可分的数据集？"><a href="#问题由来：如何处理线性不可分的数据集？" class="headerlink" title="问题由来：如何处理线性不可分的数据集？"></a>问题由来：如何处理线性不可分的数据集？</h2><p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/Kernel/1.png?raw=true" alt="non_linear_separable"></p>
<p>如图所示，以“异或”问题为例，这四个样本点在二维平面上是线性不可分的，即找不到二维空间中的一条直线分隔开圈圈和叉叉。我们可以把低维特征空间中的样本点映射到更高维的特征空间中去，使之线性可分。如图所示，我们把原始的二维空间映射到一个合适的三维空间，就找到了一个合适的划分超平面，分隔开圈圈和叉叉，如果原始空间是有限维，即属性数有限，那么一定存在一个高维特征空间使样本可分。</p>
<p>如果在我们的algorithm中，我们的样本点向量全部是以向量内积$\langle x, z\rangle$的形式出现的，我们就可以把算法中所有的向量内积$\langle x, z\rangle$替换为核函数的形式$\langle \phi(x), \phi(z)\rangle$，其中$\phi$表示特征映射feature mapping</p>
<p>给定一个<strong>feature mapping</strong>：$\phi$，我们定义其对应的核函数Kernel为：</p>
<script type="math/tex; mode=display">
K(x, z)=\phi(x)^{T} \phi(z)</script><h2 id="核函数有什么优良特性呢？"><a href="#核函数有什么优良特性呢？" class="headerlink" title="核函数有什么优良特性呢？"></a>核函数有什么优良特性呢？</h2><p>核函数本身容易计算，不需要显式地计算$\phi(x)$ 和 $\phi(z)$并计算它们的内积，计算$\phi(x)$的算法复杂度为$O\left(n^{2}\right)$，而直接计算$K(x, z)$算法复杂度只需$O(n)$！我们只需$O(n)$复杂度去计算核函数，却达到了feature mapping之后在高维特征空间中一样的效果！</p>
<h2 id="有哪些常见的核函数呢？"><a href="#有哪些常见的核函数呢？" class="headerlink" title="有哪些常见的核函数呢？"></a>有哪些常见的核函数呢？</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">名称</th>
<th style="text-align:left">表达式</th>
<th>参数 </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">线性核</td>
<td style="text-align:left">$\kappa\left(\boldsymbol{x}<em>{i}, \boldsymbol{x}</em>{j}\right)=\boldsymbol{x}<em>{i}^{\mathrm{T}} \boldsymbol{x}</em>{j}$</td>
<td></td>
</tr>
<tr>
<td style="text-align:left">多项式核</td>
<td style="text-align:left">$\kappa\left(\boldsymbol{x}<em>{i}, \boldsymbol{x}</em>{j}\right)=\left(\boldsymbol{x}<em>{i}^{\mathrm{T}} \boldsymbol{x}</em>{j}\right)^{d}$</td>
<td>$d \geqslant 1$为多项式的次数，映射到$C_{n+d}^{n}$维，证明见下文</td>
</tr>
<tr>
<td style="text-align:left">高斯核</td>
<td style="text-align:left">$\kappa\left(\boldsymbol{x}<em>{i}, \boldsymbol{x}</em>{j}\right)=\exp \left(-\frac{\left\&#124;\boldsymbol{x}<em>{i}-\boldsymbol{x}</em>{j}\right\&#124;^{2}}{2 \sigma^{2}}\right)$</td>
<td>$\sigma&gt;0$ 为高斯核的带宽(width)，映射到无穷维</td>
</tr>
<tr>
<td style="text-align:left">拉普拉斯核</td>
<td style="text-align:left">$\kappa\left(\boldsymbol{x}<em>{i}, \boldsymbol{x}</em>{j}\right)=\exp \left(-\frac{\left\&#124;\boldsymbol{x}<em>{i}-\boldsymbol{x}</em>{j}\right\&#124;}{\sigma}\right)$</td>
<td>$\sigma&gt;0$</td>
</tr>
<tr>
<td style="text-align:left">Sigmoid核</td>
<td style="text-align:left">$\kappa\left(\boldsymbol{x}<em>{i}, \boldsymbol{x}</em>{j}\right)=\tanh \left(\beta \boldsymbol{x}<em>{i}^{\mathrm{T}} \boldsymbol{x}</em>{j}+\theta\right)$</td>
<td>tanh为双曲正切函数，$\beta&gt;0, \theta&lt;0$</td>
</tr>
</tbody>
</table>
</div>
<h3 id="多项式核映射到-C-n-d-n-维的证明："><a href="#多项式核映射到-C-n-d-n-维的证明：" class="headerlink" title="多项式核映射到$C_{n+d}^{n}$维的证明："></a>多项式核映射到$C_{n+d}^{n}$维的证明：</h3><p>若x=(x1,x2,….xn), $\phi(x)$包含了x分量所有小于等于d次组合，那么$\phi(x)$的维度便 等价于问题$(1+x1+x2+x3+….xn)^{d}$ 展开有多少项，这个问题的计算用隔板法可以推出是$C<em>{n+d}^{n}$。 <br>证明方法： (1^n0)(x1^n1)…..(xi^n1)…(xn^nn) 有多少个等价于： n0+n1+n2….+nn=d 有多少种组合法,其中ni大于等于0。 变形一下： (n0+1)+(n1+1)+(n2+1)+….(nn+1)=d+n+1. 换一下表示方式N0+N1+….Nn=d+n+1, 其中Ni大于等于1, 此时便可以用隔板法了。 d+n+1个球代表有d+n个可选位置，要分成n+1份，则需要放入n个板。所以就是d+n个位置放入n个板，$C</em>{n+d}^{n}$</p>
<h2 id="如何判断一个核函数是否是有效的核函数？"><a href="#如何判断一个核函数是否是有效的核函数？" class="headerlink" title="如何判断一个核函数是否是有效的核函数？"></a>如何判断一个核函数是否是有效的核函数？</h2><p>若已知合适映射$\phi(\cdot)$的具体形式，则可写出核函数$\kappa(\cdot, \cdot)$。但在现实中我们通常不知道$\phi(\cdot)$的具体形式，那么什么样的函数是有效的核函数呢？我们有如下定理：</p>
<h3 id="Theorem（Mercer"><a href="#Theorem（Mercer" class="headerlink" title="Theorem（Mercer):"></a><strong>Theorem（Mercer)</strong>:</h3><p>对于一个给定核函数$K: \mathbb{R}^{n} \times \mathbb{R}^{n} \mapsto \mathbb{R}$，如果K是一个有效的(Mercer)核，对任意m个样本点$\left{x^{(1)}, \ldots, x^{(m)}\right}$，其对应的kernel matrix是半正定的。这互为充要条件。</p>
<p><strong>定义</strong>：Kernel matrix<br>对于m个样本点$\left{x^{(1)}, \ldots, x^{(m)}\right}$，我们有$m \times m$方阵，其$(i, j)$元为$K_{i j}=K\left(x^{(i)}, x^{(j)}\right)$。这个矩阵就叫Kernel Matrix</p>
<script type="math/tex; mode=display">
\mathbf{K}=\left[\begin{array}{ccccc}
\kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{1}\right) & \cdots & \kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{j}\right) & \cdots & \kappa\left(\boldsymbol{x}_{1}, \boldsymbol{x}_{m}\right) \\
\vdots & \ddots & \vdots & \ddots & \vdots \\
\kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{1}\right) & \cdots & \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{j}\right) & \cdots & \kappa\left(\boldsymbol{x}_{i}, \boldsymbol{x}_{m}\right) \\
\vdots & \ddots & \vdots & \ddots & \vdots \\
\kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{1}\right) & \cdots & \kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{j}\right) & \cdots & \kappa\left(\boldsymbol{x}_{m}, \boldsymbol{x}_{m}\right)
\end{array}\right]</script>]]></content>
      <categories>
        <category>kernel</category>
      </categories>
      <tags>
        <tag>kernel</tag>
      </tags>
  </entry>
  <entry>
    <title>主成分分析PCA</title>
    <url>/2021/11/06/PCA/</url>
    <content><![CDATA[<p>本文阐述如何通过 <strong>协方差矩阵</strong> 的 <strong>奇异值分解</strong> 理解 <strong>主成分分析PCA</strong>：</p>
<span id="more"></span>
<h2 id="数据降维问题："><a href="#数据降维问题：" class="headerlink" title="数据降维问题："></a>数据降维问题：</h2><p>我们想对一个feature过多的数据集进行<strong>降维</strong>：假设我们有一个m个样本点的数据集：$\left{x^{(i)} ; i=1, \ldots, m\right}$，其中$x^{(i)} \in \mathbb{R}^{n}$。其中n很大，很多属性对我们的数据分析问题作用不大。我们想通过基变换，把n维空间中的样本点映射到$\mathbb{R}^{n}$的k维子空间中。其中$k \ll n$。这k个基能最大地保留原始数据集的信息，使原数据集的信息损失最少。</p>
<p>我们的原始数据集如下图所示，以$\mathbb{R}^{2}$为例：<br><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/PCA/PCA.jpg?raw=true" alt="PCA"></p>
<p>如何通过协方差矩阵的奇异值分解来理解主成分分析PCA呢？观察上图，在2维平面上我们有m个样本点，这m个样本点我们以自然基$\mathrm{\vec e}<em>{1}:(1,0)$和$\mathrm{\vec e}</em>{2}:(0,1)$下的坐标来表示。我们对这个m个样本点构造一个matrix：</p>
<script type="math/tex; mode=display">X = \left[\begin{array}{ccccc}\vdots & \vdots & \vdots & & \vdots \\ \vec x_{1} & \vec x_{2} & \vec x_{3} & \ldots & \vec x_{m} \\ \vdots & \vdots & \vdots & & \vdots\end{array}\right]</script><p>那么这个数据集的协方差矩阵是$\Sigma=\frac{1}{m} X X^{T}$。在这一步之前我们需要对数据集进行预处理：</p>
<script type="math/tex; mode=display">
x_{j}^{(i)} \leftarrow \frac{x_{j}^{(i)}-\mu_{j}}{\sigma_{j}}</script><p>先normalizing这个数据集，使其各分量均值为0，方差为1。均值为0是为了协方差矩阵不用减去均值，方差为1是为了在同一个量级下比较。<br>协方差矩阵的元素$\Sigma_{ij}$是数据集：$\left{x^{(i)} ; i=1, \ldots, m\right}$的第i和第j分量的协方差，协方差矩阵对角线上的元素为各个分量的方差。</p>
<p>对于图中数据集样本点的分布，我们可以明显看出，在自然基表示下的各个点的坐标，不同分量之间的协方差是不为零的，那么我就想找到新的基，使这m个点在这个新基$\mathrm{\vec u}<em>{1}$和$\mathrm{\vec u}</em>{2}$的表示下，各个分量之间的协方差为零。矩阵$X$的各个点在新基$\mathrm{\vec u}<em>{1}$和$\mathrm{\vec u}</em>{2}$下的坐标为$\left{y^{(i)} ; i=1, \ldots, m\right}$，那么经过基变换之后，矩阵$X$变成：</p>
<script type="math/tex; mode=display">Y = \left[\begin{array}{ccccc}\vdots & \vdots & \vdots & & \vdots \\ \vec y_{1} & \vec y_{2} & \vec y_{3} & \ldots & \vec y_{m} \\ \vdots & \vdots & \vdots & & \vdots\end{array}\right]</script><p>在这里我们要牢记并理解一个线性代数基础概念：<strong>基变换与坐标变换</strong>，在同济大学《线性代数》第六版第148页，我们这里复习下，不熟悉的同学可以回去翻教材。</p>
<h2 id="基变换与坐标变换"><a href="#基变换与坐标变换" class="headerlink" title="基变换与坐标变换"></a>基变换与坐标变换</h2><p>同一向量在不同的基中有不同的坐标，不同的基与不同的坐标之间有怎样的关系呢?<br>设$\boldsymbol{\alpha}<em>{1}, \cdots, \boldsymbol{\alpha}</em>{n}$及$\boldsymbol{\beta}<em>{1}, \cdots, \boldsymbol{\beta}</em>{n}$是线性空间$V_{n}$中的两个基</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}
\boldsymbol{\beta}_{1}=p_{11} \boldsymbol{\alpha}_{1}+p_{21} \boldsymbol{\alpha}_{2}+\cdots+p_{n 1} \boldsymbol{\alpha}_{n}, \\
\boldsymbol{\beta}_{2}=p_{12} \boldsymbol{\alpha}_{1}+p_{22} \boldsymbol{\alpha}_{2}+\cdots+p_{n 2} \boldsymbol{\alpha}_{n}, \\
\cdots \cdots \cdots \cdots \\
\boldsymbol{\beta}_{n}=p_{1 n} \boldsymbol{\alpha}_{1}+p_{2 n} \boldsymbol{\alpha}_{2}+\cdots+p_{n n} \boldsymbol{\alpha}_{n},
\end{array}\right.</script><p>把$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}<em>{n}$这n个有序向量记作$\left(\boldsymbol{\alpha}</em>{1}, \boldsymbol{\alpha}<em>{2}, \cdots, \boldsymbol{\alpha}</em>{n}\right)$，记n阶矩阵$\boldsymbol{P}=\left(p_{i j}\right)$，利用向量和矩阵的形式，上式可表示为</p>
<script type="math/tex; mode=display">
\left(\boldsymbol{\beta}_{1}, \boldsymbol{\beta}_{2}, \cdots, \boldsymbol{\beta}_{n}\right)=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right) \boldsymbol{P}</script><p>上面两式称为<strong>基变换公式</strong>，矩阵$\boldsymbol{P}$称为由基$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}<em>{n}$到基$\boldsymbol{\beta}</em>{1}, \boldsymbol{\beta}<em>{2}, \cdots, \boldsymbol{\beta}</em>{n}$的<strong>过渡矩阵</strong>。由于$\boldsymbol{\beta}<em>{1}, \boldsymbol{\beta}</em>{2}, \cdots, \boldsymbol{\beta}_{n}$线性无关，故过渡矩阵$\boldsymbol{P}$可逆。</p>
<p><strong>定理</strong> 设$V<em>{n}$中的向量$\boldsymbol{\alpha}$在基$\boldsymbol{\alpha}</em>{1}, \boldsymbol{\alpha}<em>{2}, \cdots, \boldsymbol{\alpha}</em>{n}$中的坐标为$\left(x<em>{1}, x</em>{2}, \cdots, x<em>{n}\right)^{\mathrm{T}}$，在基$\boldsymbol{\beta}</em>{1}, \boldsymbol{\beta}<em>{2}, \cdots, \boldsymbol{\beta}</em>{n}$中的坐标为$\left(x<em>{1}^{\prime}, x</em>{2}^{\prime}, \cdots, x_{n}^{\prime}\right)^{\mathrm{T}}$，若两个基满足上面两个关系式，则有<strong>坐标变换公式</strong>：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=\boldsymbol{P}\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)</script><p>或</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)=\boldsymbol{P}^{-1}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=\boldsymbol{P}^{T}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)</script><p>证明：因为</p>
<script type="math/tex; mode=display">
\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right)\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=\boldsymbol{\alpha}=\left(\boldsymbol{\beta}_{1}, \boldsymbol{\beta}_{2}, \cdots, \boldsymbol{\beta}_{n}\right)\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)</script><script type="math/tex; mode=display">
=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right) \boldsymbol{P}\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)</script><p>由于$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}_{n}$线性无关，证毕。</p>
<hr>
<p>那么根据以上概念，我们从自然基$\mathrm{\vec e}<em>{1} \mathrm{\vec e}</em>{2} \dots \mathrm{\vec e}<em>{n}$到$\mathrm{\vec u}</em>{1} \mathrm{\vec u}<em>{2} \dots \mathrm{\vec u}</em>{n}$的基变换公式是：</p>
<script type="math/tex; mode=display">
\left(\boldsymbol{u}_{1}, \boldsymbol{u}_{2}, \cdots, \boldsymbol{u}_{n}\right)=\left(\boldsymbol{e}_{1}, \boldsymbol{e}_{2}, \cdots, \boldsymbol{e}_{n}\right) \boldsymbol{P}</script><p>其中自然基矩阵为单位阵，所以过渡矩阵$\boldsymbol{P}$为:</p>
<script type="math/tex; mode=display">
\boldsymbol{P} = \left(\boldsymbol{u}_{1}, \boldsymbol{u}_{2}, \cdots, \boldsymbol{u}_{n}\right)</script><p>新的基$\boldsymbol{u}<em>{1}, \boldsymbol{u}</em>{2}, \cdots, \boldsymbol{u}_{n}$彼此正交，长度为1。所以过渡矩阵$\boldsymbol{P}$为正交矩阵！</p>
<p>设$\mathbb{R}^{n}$中的向量$\boldsymbol{\alpha}$在基$\boldsymbol{e}<em>{1}, \boldsymbol{e}</em>{2}, \cdots, \boldsymbol{e}<em>{n}$中的坐标为$\left(x</em>{1}, x<em>{2}, \cdots, x</em>{n}\right)^{\mathrm{T}}$，在基$\boldsymbol{u}<em>{1}, \boldsymbol{u}</em>{2}, \cdots, \boldsymbol{u}<em>{n}$中的坐标为$\left(x</em>{1}^{\prime}, x<em>{2}^{\prime}, \cdots, x</em>{n}^{\prime}\right)^{\mathrm{T}}$，若两个基满足上面关系式，则有<strong>坐标变换公式</strong>：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=\boldsymbol{P}\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)</script><p>或</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)=\boldsymbol{P}^{-1}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=\boldsymbol{P}^{T}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)</script><p>我们通过基变换，有坐标变换：</p>
<script type="math/tex; mode=display">Y = \left[\begin{array}{ccccc}\vdots & \vdots & \vdots & & \vdots \\ \vec y_{1} & \vec y_{2} & \vec y_{3} & \ldots & \vec y_{m} \\ \vdots & \vdots & \vdots & & \vdots\end{array}\right] = \boldsymbol{P}^{T} \left[\begin{array}{ccccc}\vdots & \vdots & \vdots & & \vdots \\ \vec x_{1} & \vec x_{2} & \vec x_{3} & \ldots & \vec x_{m} \\ \vdots & \vdots & \vdots & & \vdots\end{array}\right] = \boldsymbol{P}^{T}X</script><p>于是我们发现：旧自然基下的协方差矩阵$\Sigma=\frac{1}{m} X X^{T}$和新基下的协方差矩阵$\Sigma^{\prime}=\frac{1}{m} Y Y^{T}$之间的关系是：</p>
<script type="math/tex; mode=display">
\Sigma^{\prime}=\frac{1}{m} Y Y^{T}=\frac{1}{m} P^{T} X\left(P^{T} X\right)^{T}=\frac{1}{m} P^{T} X X^{T} P=P^{T} \frac{1}{m}X X^{T} P=P^{T} \Sigma P</script><script type="math/tex; mode=display">
\Sigma=\frac{1}{m} X X^{T}=P\frac{1}{m} Y Y^{T}P^{T}=P\Sigma^{\prime}P^{T}</script><p>其中$\Sigma^{\prime}$是对角阵，$\boldsymbol{P}$是正交矩阵。关键来了！</p>
<h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h2><p>我们对数据集m个样本点在旧自然基下的协方差矩阵$\Sigma=\frac{1}{m} X X^{T}$有奇异值分解：</p>
<p>$\Sigma=\frac{1}{m} X X^{T}=P\Sigma^{\prime}P^{T}$，其中对角矩阵$\Sigma^{\prime}=\frac{1}{m} Y Y^{T}$，$P$是过渡矩阵。</p>
<p>对角阵$\Sigma^{\prime}$的对角线上是从大到小排列的奇异值，我们取前k个奇异值，这k个奇异值对应的奇异向量，就是我们要找的新的k个基$\boldsymbol{u}<em>{1}, \boldsymbol{u}</em>{2}, \cdots, \boldsymbol{u}_{k}$</p>
]]></content>
      <categories>
        <category>降维</category>
      </categories>
      <tags>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title>numpy常用命令笔记</title>
    <url>/2021/09/05/numpy/</url>
    <content><![CDATA[<p>Python for Data Analysis by Wes McKinney (O’Reilly).<br>《利用Python进行数据分析》</p>
<span id="more"></span>
<h2 id="1-Creating-ndarrays-生成ndarray"><a href="#1-Creating-ndarrays-生成ndarray" class="headerlink" title="1. Creating ndarrays 生成ndarray"></a>1. Creating ndarrays 生成ndarray</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>数组生成函数</strong></th>
<th style="text-align:left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">array</td>
<td style="text-align:left">Convert input data (list, tuple, array, or other sequence type) to an ndarray either by inferring a dtype or explicitly specifying a dtype; copies the input data by default</td>
</tr>
<tr>
<td style="text-align:left">asarray</td>
<td style="text-align:left">Convert input to ndarray, but do not copy if the input is already an ndarray</td>
</tr>
<tr>
<td style="text-align:left">arange</td>
<td style="text-align:left">Like the built-in range but returns an ndarray instead of a list</td>
</tr>
<tr>
<td style="text-align:left">ones</td>
<td style="text-align:left">Produce an array of all 1s with the given shape and dtype; ones_like takes another array and produces a ones array of the same shape and dtype</td>
</tr>
<tr>
<td style="text-align:left">ones_like</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">zeros</td>
<td style="text-align:left">Like ones and ones_like but producing arrays of 0s instead</td>
</tr>
<tr>
<td style="text-align:left">zeros_like</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">empty</td>
<td style="text-align:left">Create new arrays by allocating new memory, but do not populate with any values like ones and zeros</td>
</tr>
<tr>
<td style="text-align:left">empty_like</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">full</td>
<td style="text-align:left">Produce an array of the given shape and dtype with all values set to the indicated “fill value” full_like takes another array and produces a filled array of the same shape and dtype</td>
</tr>
<tr>
<td style="text-align:left">full_like</td>
<td style="text-align:left"></td>
</tr>
<tr>
<td style="text-align:left">eye, identity</td>
<td style="text-align:left">Create a square N × N identity matrix (1s on the diagonal and 0s elsewhere)</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">data1 = [<span class="number">6</span>, <span class="number">7.5</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">1</span>] <span class="comment">#列表</span></span><br><span class="line">arr1 = np.array(data1) <span class="comment">#利用列表生成ndarray</span></span><br><span class="line">arr1</span><br><span class="line">array([ <span class="number">6.</span> ,  <span class="number">7.5</span>,  <span class="number">8.</span> ,  <span class="number">0.</span> ,  <span class="number">1.</span> ])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.zeros(<span class="number">10</span>)</span><br><span class="line">array([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>])</span><br><span class="line">np.zeros((<span class="number">3</span>, <span class="number">6</span>))</span><br><span class="line">array([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">       [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.arange(<span class="number">15</span>)</span><br><span class="line">array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>])</span><br></pre></td></tr></table></figure>
<h2 id="2-Transposing-Arrays-and-Swapping-Axes-数组转置和换轴"><a href="#2-Transposing-Arrays-and-Swapping-Axes-数组转置和换轴" class="headerlink" title="2. Transposing Arrays and Swapping Axes 数组转置和换轴"></a>2. Transposing Arrays and Swapping Axes 数组转置和换轴</h2><p>Arrays have the <strong>transpose method</strong> and also the special <strong>T attribute</strong>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">126</span>]: arr = np.arange(<span class="number">15</span>).reshape((<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">In [<span class="number">127</span>]: arr</span><br><span class="line">Out[<span class="number">127</span>]:</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>],</span><br><span class="line">       [ <span class="number">5</span>,  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>], </span><br><span class="line">       [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">13</span>, <span class="number">14</span>]])</span><br><span class="line">In [<span class="number">128</span>]: arr.T</span><br><span class="line">Out[<span class="number">128</span>]:</span><br><span class="line">array([[ <span class="number">0</span>, <span class="number">5</span>, <span class="number">10</span>],</span><br><span class="line">       [ <span class="number">1</span>, <span class="number">6</span>, <span class="number">11</span>], </span><br><span class="line">       [ <span class="number">2</span>, <span class="number">7</span>, <span class="number">12</span>],</span><br><span class="line">       [ <span class="number">3</span>, <span class="number">8</span>, <span class="number">13</span>], </span><br><span class="line">       [ <span class="number">4</span>, <span class="number">9</span>, <span class="number">14</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="3-通用函数：Universal-Functions-Fast-Element-Wise-Array-Functions"><a href="#3-通用函数：Universal-Functions-Fast-Element-Wise-Array-Functions" class="headerlink" title="3. 通用函数：Universal Functions: Fast Element-Wise Array Functions"></a>3. 通用函数：Universal Functions: Fast Element-Wise Array Functions</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>一元通用函数</strong></th>
<th style="text-align:left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">abs, fabs</td>
<td style="text-align:left">Compute the absolute value element-wise for integer, floating-point, or complex values</td>
</tr>
<tr>
<td style="text-align:left">sqrt</td>
<td style="text-align:left">Compute the square root of each element (equivalent to arr ** 0.5)</td>
</tr>
<tr>
<td style="text-align:left">square</td>
<td style="text-align:left">Compute the square of each element (equivalent to arr ** 2)</td>
</tr>
<tr>
<td style="text-align:left">exp</td>
<td style="text-align:left">Compute the exponent ex of each element</td>
</tr>
<tr>
<td style="text-align:left">log, log10, log2, log1p</td>
<td style="text-align:left">Natural logarithm (base e), log base 10, log base 2, and log(1 + x), respectively</td>
</tr>
<tr>
<td style="text-align:left">sign</td>
<td style="text-align:left">Compute the sign of each element: 1 (positive), 0 (zero), or –1 (negative)</td>
</tr>
<tr>
<td style="text-align:left">ceil</td>
<td style="text-align:left">Compute the ceiling of each element (i.e., the smallest integer greater than or equal to that number)</td>
</tr>
<tr>
<td style="text-align:left">floor</td>
<td style="text-align:left">Compute the floor of each element (i.e., the largest integer less than or equal to each element)</td>
</tr>
<tr>
<td style="text-align:left">rint</td>
<td style="text-align:left">Round elements to the nearest integer, preserving the dtype</td>
</tr>
<tr>
<td style="text-align:left">modf</td>
<td style="text-align:left">Return fractional and integral parts of array as a separate array</td>
</tr>
<tr>
<td style="text-align:left">isnan</td>
<td style="text-align:left">Return boolean array indicating whether each value is NaN (Not a Number)</td>
</tr>
<tr>
<td style="text-align:left">isfinite, isinf</td>
<td style="text-align:left">Return boolean array indicating whether each element is finite (non-inf, non-NaN) or infinite, respectively</td>
</tr>
<tr>
<td style="text-align:left">cos, cosh, sin, sinh, tan, tanh</td>
<td style="text-align:left">Regular and hyperbolic trigonometric functions</td>
</tr>
<tr>
<td style="text-align:left">arccos, arccosh, arcsin, arcsinh, arctan, arctanh</td>
<td style="text-align:left">Inverse trigonometric functions</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>二元通用函数</strong></th>
<th style="text-align:left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">add</td>
<td style="text-align:left">Add corresponding elements in arrays</td>
</tr>
<tr>
<td style="text-align:left">subtract</td>
<td style="text-align:left">Subtract elements in second array from first array</td>
</tr>
<tr>
<td style="text-align:left">multiply</td>
<td style="text-align:left">Multiply array elements</td>
</tr>
<tr>
<td style="text-align:left">divide, floor_divide</td>
<td style="text-align:left">Divide or floor divide (truncating the remainder)</td>
</tr>
<tr>
<td style="text-align:left">power</td>
<td style="text-align:left">Raise elements in first array to powers indicated in second array</td>
</tr>
<tr>
<td style="text-align:left">maximum, fmax</td>
<td style="text-align:left">Element-wise maximum; fmax ignores NaN</td>
</tr>
<tr>
<td style="text-align:left">minimum, fmin</td>
<td style="text-align:left">Element-wise minimum; fmin ignores NaN</td>
</tr>
<tr>
<td style="text-align:left">mod</td>
<td style="text-align:left">Element-wise modulus (remainder of division)</td>
</tr>
<tr>
<td style="text-align:left">copysign</td>
<td style="text-align:left">Copy sign of values in second argument to values in first argument</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">137</span>]: arr = np.arange(<span class="number">10</span>)</span><br><span class="line">In [<span class="number">138</span>]: arr</span><br><span class="line">Out[<span class="number">138</span>]: array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line">In [<span class="number">139</span>]: np.sqrt(arr)</span><br><span class="line">Out[<span class="number">139</span>]:</span><br><span class="line">array([ <span class="number">0.</span>    ,  <span class="number">1.</span>    ,  <span class="number">1.4142</span>,  <span class="number">1.7321</span>,  <span class="number">2.</span>    ,  <span class="number">2.2361</span>,  <span class="number">2.4495</span>,</span><br><span class="line">        <span class="number">2.6458</span>,  <span class="number">2.8284</span>,  <span class="number">3.</span>    ])</span><br><span class="line">In [<span class="number">140</span>]: np.exp(arr)</span><br><span class="line">Out[<span class="number">140</span>]:</span><br><span class="line">array([    <span class="number">1.</span>    ,     <span class="number">2.7183</span>,     <span class="number">7.3891</span>,    <span class="number">20.0855</span>,    <span class="number">54.5982</span>,</span><br><span class="line">        <span class="number">148.4132</span>,   <span class="number">403.4288</span>,  <span class="number">1096.6332</span>,  <span class="number">2980.958</span> ,  <span class="number">8103.0839</span>])</span><br></pre></td></tr></table></figure>
<h2 id="4-条件逻辑作为数组操作-Conditional-Logic-as-Array-Operations"><a href="#4-条件逻辑作为数组操作-Conditional-Logic-as-Array-Operations" class="headerlink" title="4.条件逻辑作为数组操作 Conditional Logic as Array Operations"></a>4.条件逻辑作为数组操作 Conditional Logic as Array Operations</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#np.where(cond, xarr, yarr), xarr和yarr可以是标量也可以是数组</span></span><br><span class="line">In [<span class="number">165</span>]: xarr = np.array([<span class="number">1.1</span>, <span class="number">1.2</span>, <span class="number">1.3</span>, <span class="number">1.4</span>, <span class="number">1.5</span>])</span><br><span class="line">In [<span class="number">166</span>]: yarr = np.array([<span class="number">2.1</span>, <span class="number">2.2</span>, <span class="number">2.3</span>, <span class="number">2.4</span>, <span class="number">2.5</span>])</span><br><span class="line">In [<span class="number">167</span>]: cond = np.array([<span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"></span><br><span class="line">In [<span class="number">170</span>]: result = np.where(cond, xarr, yarr)</span><br><span class="line">In [<span class="number">171</span>]: result</span><br><span class="line">Out[<span class="number">171</span>]: array([ <span class="number">1.1</span>,  <span class="number">2.2</span>,  <span class="number">1.3</span>,  <span class="number">1.4</span>,  <span class="number">2.5</span>])</span><br></pre></td></tr></table></figure>
<h2 id="5-数学和统计方法-Mathematical-and-Statistical-Methods"><a href="#5-数学和统计方法-Mathematical-and-Statistical-Methods" class="headerlink" title="5. 数学和统计方法 Mathematical and Statistical Methods"></a>5. 数学和统计方法 Mathematical and Statistical Methods</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>方法</strong></th>
<th style="text-align:left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">sum</td>
<td style="text-align:left">Sum of all the elements in the array or along an axis; zero-length arrays have sum 0</td>
</tr>
<tr>
<td style="text-align:left">mean</td>
<td style="text-align:left">Arithmetic mean; zero-length arrays have NaN mean</td>
</tr>
<tr>
<td style="text-align:left">std, var</td>
<td style="text-align:left">Standard deviation and variance, respectively, with optional degrees of freedom adjustment (default denominator n)</td>
</tr>
<tr>
<td style="text-align:left">min, max</td>
<td style="text-align:left">Minimum and maximum)</td>
</tr>
<tr>
<td style="text-align:left">argmin, argmax</td>
<td style="text-align:left">Indices of minimum and maximum elements, respectively</td>
</tr>
<tr>
<td style="text-align:left">cumsum</td>
<td style="text-align:left">Cumulative sum of elements starting from 0</td>
</tr>
<tr>
<td style="text-align:left">cumprod</td>
<td style="text-align:left">Cumulative product of elements starting from 1</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">177</span>]: arr = np.random.randn(<span class="number">5</span>, <span class="number">4</span>)</span><br><span class="line">In [<span class="number">178</span>]: arr</span><br><span class="line">Out[<span class="number">178</span>]:</span><br><span class="line">array([[ <span class="number">2.1695</span>, -<span class="number">0.1149</span>,  <span class="number">2.0037</span>,  <span class="number">0.0296</span>],</span><br><span class="line">        [ <span class="number">0.7953</span>,  <span class="number">0.1181</span>, -<span class="number">0.7485</span>,  <span class="number">0.585</span> ],</span><br><span class="line">        [ <span class="number">0.1527</span>, -<span class="number">1.5657</span>, -<span class="number">0.5625</span>, -<span class="number">0.0327</span>],</span><br><span class="line">        [-<span class="number">0.929</span> , -<span class="number">0.4826</span>, -<span class="number">0.0363</span>,  <span class="number">1.0954</span>],</span><br><span class="line">        [ <span class="number">0.9809</span>, -<span class="number">0.5895</span>,  <span class="number">1.5817</span>, -<span class="number">0.5287</span>]])</span><br><span class="line">In [<span class="number">179</span>]: arr.mean()</span><br><span class="line">Out[<span class="number">179</span>]: <span class="number">0.19607051119998253</span></span><br><span class="line">In [<span class="number">180</span>]: np.mean(arr)</span><br><span class="line">Out[<span class="number">180</span>]: <span class="number">0.19607051119998253</span></span><br><span class="line">In [<span class="number">181</span>]: arr.<span class="built_in">sum</span>()</span><br><span class="line">Out[<span class="number">181</span>]: <span class="number">3.9214102239996507</span></span><br><span class="line">In [<span class="number">182</span>]: arr.mean(axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">182</span>]: array([ <span class="number">1.022</span> ,  <span class="number">0.1875</span>, -<span class="number">0.502</span> , -<span class="number">0.0881</span>,  <span class="number">0.3611</span>])</span><br><span class="line">In [<span class="number">183</span>]: arr.<span class="built_in">sum</span>(axis=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">183</span>]: array([ <span class="number">3.1693</span>, -<span class="number">2.6345</span>,  <span class="number">2.2381</span>,  <span class="number">1.1486</span>])</span><br><span class="line">In [<span class="number">184</span>]: arr = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>])</span><br><span class="line">In [<span class="number">185</span>]: arr.cumsum()</span><br><span class="line">Out[<span class="number">185</span>]: array([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">15</span>, <span class="number">21</span>, <span class="number">28</span>])</span><br><span class="line">In [<span class="number">186</span>]: arr = np.array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">In [<span class="number">187</span>]: arr</span><br><span class="line">Out[<span class="number">187</span>]:</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">       [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line">In [<span class="number">188</span>]: arr.cumsum(axis=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">188</span>]:</span><br><span class="line">array([[ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],</span><br><span class="line">       [ <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>], </span><br><span class="line">       [ <span class="number">9</span>, <span class="number">12</span>, <span class="number">15</span>]])</span><br><span class="line">In [<span class="number">189</span>]: arr.cumprod(axis=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">189</span>]:</span><br><span class="line">array([[  <span class="number">0</span>,   <span class="number">0</span>,   <span class="number">0</span>],</span><br><span class="line">       [ <span class="number">3</span>,  <span class="number">12</span>,  <span class="number">60</span>],</span><br><span class="line">       [ <span class="number">6</span>,  <span class="number">42</span>, <span class="number">336</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="6-布尔值数组方法-Methods-for-Boolean-Arrays"><a href="#6-布尔值数组方法-Methods-for-Boolean-Arrays" class="headerlink" title="6. 布尔值数组方法 Methods for Boolean Arrays"></a>6. 布尔值数组方法 Methods for Boolean Arrays</h2><p>method <strong>any</strong> tests whether one or more values in an array is True, while <strong>all</strong> checks if every value is True:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">192</span>]: bools = np.array([<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line">In [<span class="number">193</span>]: bools.<span class="built_in">any</span>()</span><br><span class="line">Out[<span class="number">193</span>]: <span class="literal">True</span></span><br><span class="line">In [<span class="number">194</span>]: bools.<span class="built_in">all</span>()</span><br><span class="line">Out[<span class="number">194</span>]: <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="7-排序-Sorting"><a href="#7-排序-Sorting" class="headerlink" title="7. 排序 Sorting"></a>7. 排序 Sorting</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">195</span>]: arr = np.random.randn(<span class="number">6</span>)</span><br><span class="line">In [<span class="number">196</span>]: arr</span><br><span class="line">Out[<span class="number">196</span>]: array([ <span class="number">0.6095</span>, -<span class="number">0.4938</span>,  <span class="number">1.24</span>  , -<span class="number">0.1357</span>,  <span class="number">1.43</span>  , -<span class="number">0.8469</span>])</span><br><span class="line">In [<span class="number">197</span>]: arr.sort()</span><br><span class="line">In [<span class="number">198</span>]: arr</span><br><span class="line">Out[<span class="number">198</span>]: array([-<span class="number">0.8469</span>, -<span class="number">0.4938</span>, -<span class="number">0.1357</span>,  <span class="number">0.6095</span>,  <span class="number">1.24</span>  ,  <span class="number">1.43</span>  ])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">199</span>]: arr = np.random.randn(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line">In [<span class="number">200</span>]: arr</span><br><span class="line">Out[<span class="number">200</span>]:</span><br><span class="line">array([[ <span class="number">0.6033</span>,  <span class="number">1.2636</span>, -<span class="number">0.2555</span>],</span><br><span class="line">        [-<span class="number">0.4457</span>,  <span class="number">0.4684</span>, -<span class="number">0.9616</span>],</span><br><span class="line">        [-<span class="number">1.8245</span>,  <span class="number">0.6254</span>,  <span class="number">1.0229</span>],</span><br><span class="line">        [ <span class="number">1.1074</span>,  <span class="number">0.0909</span>, -<span class="number">0.3501</span>],</span><br><span class="line">        [ <span class="number">0.218</span> , -<span class="number">0.8948</span>, -<span class="number">1.7415</span>]])</span><br><span class="line">In [<span class="number">201</span>]: arr.sort(<span class="number">1</span>)</span><br><span class="line">In [<span class="number">202</span>]: arr</span><br><span class="line">Out[<span class="number">202</span>]:</span><br><span class="line">array([[-<span class="number">0.2555</span>,  <span class="number">0.6033</span>,  <span class="number">1.2636</span>],</span><br><span class="line">        [-<span class="number">0.9616</span>, -<span class="number">0.4457</span>,  <span class="number">0.4684</span>],</span><br><span class="line">        [-<span class="number">1.8245</span>,  <span class="number">0.6254</span>,  <span class="number">1.0229</span>],</span><br><span class="line">        [-<span class="number">0.3501</span>,  <span class="number">0.0909</span>,  <span class="number">1.1074</span>],</span><br><span class="line">        [-<span class="number">1.7415</span>, -<span class="number">0.8948</span>,  <span class="number">0.218</span> ]])</span><br></pre></td></tr></table></figure>
<h2 id="8-唯一值和其他集合逻辑-Unique-and-Other-Set-Logic"><a href="#8-唯一值和其他集合逻辑-Unique-and-Other-Set-Logic" class="headerlink" title="8. 唯一值和其他集合逻辑 Unique and Other Set Logic"></a>8. 唯一值和其他集合逻辑 Unique and Other Set Logic</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">In [<span class="number">206</span>]: names = np.array([<span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Joe&#x27;</span>, <span class="string">&#x27;Will&#x27;</span>, <span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Will&#x27;</span>, <span class="string">&#x27;Joe&#x27;</span>, <span class="string">&#x27;Joe&#x27;</span>])</span><br><span class="line">In [<span class="number">207</span>]: np.unique(names)</span><br><span class="line">Out[<span class="number">207</span>]:</span><br><span class="line">array([<span class="string">&#x27;Bob&#x27;</span>, <span class="string">&#x27;Joe&#x27;</span>, <span class="string">&#x27;Will&#x27;</span>], dtype=<span class="string">&#x27;&lt;U4&#x27;</span>)</span><br><span class="line">In [<span class="number">208</span>]: ints = np.array([<span class="number">3</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>])</span><br><span class="line">In [<span class="number">209</span>]: np.unique(ints)</span><br><span class="line">Out[<span class="number">209</span>]: array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>数组集合操作</strong></th>
<th style="text-align:left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">unique(x)</td>
<td style="text-align:left">Compute the sorted, unique elements in x</td>
</tr>
<tr>
<td style="text-align:left">intersect1d(x,y)</td>
<td style="text-align:left">Compute the sorted, common elements in x and y</td>
</tr>
<tr>
<td style="text-align:left">union1d(x, y)</td>
<td style="text-align:left">Compute the sorted union of elements</td>
</tr>
<tr>
<td style="text-align:left">in1d(x, y)</td>
<td style="text-align:left">Compute a boolean array indicating whether each element ofxis contained iny</td>
</tr>
<tr>
<td style="text-align:left">setdiff1d(x, y)</td>
<td style="text-align:left">Set difference, elements inxthat are not iny</td>
</tr>
<tr>
<td style="text-align:left">setxor1d(x, y)</td>
<td style="text-align:left">Set symmetric differences; elements that are in either of the arrays, but not both</td>
</tr>
</tbody>
</table>
</div>
<hr>
<h1 id="线性代数-Linear-Algebra"><a href="#线性代数-Linear-Algebra" class="headerlink" title="线性代数  Linear Algebra"></a>线性代数  Linear Algebra</h1><p>numpy.linalg模块 </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><strong>常用numpy.linalg函数</strong></th>
<th style="text-align:left"><strong>Description</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">diag</td>
<td style="text-align:left">Return the diagonal (or off-diagonal) elements of a square matrix as a 1D array, or convert a 1D array into a square matrix with zeros on the off-diagonal</td>
</tr>
<tr>
<td style="text-align:left">dot</td>
<td style="text-align:left">Matrix multiplication</td>
</tr>
<tr>
<td style="text-align:left">trace</td>
<td style="text-align:left">Compute the sum of the diagonal elements</td>
</tr>
<tr>
<td style="text-align:left">det</td>
<td style="text-align:left">Compute the matrix determinant</td>
</tr>
<tr>
<td style="text-align:left">eig</td>
<td style="text-align:left">Compute the eigenvalues and eigenvectors of a square matrix</td>
</tr>
<tr>
<td style="text-align:left">inv</td>
<td style="text-align:left">Compute the inverse of a square matrix</td>
</tr>
<tr>
<td style="text-align:left">pinv</td>
<td style="text-align:left">Compute the Moore-Penrose pseudo-inverse of a matrix</td>
</tr>
<tr>
<td style="text-align:left">qr</td>
<td style="text-align:left">Compute the QR decomposition</td>
</tr>
<tr>
<td style="text-align:left">svd</td>
<td style="text-align:left">Compute the singular value decomposition (SVD)</td>
</tr>
<tr>
<td style="text-align:left">solve</td>
<td style="text-align:left">Solve the linear system Ax = b for x, where A is a square matrix</td>
</tr>
<tr>
<td style="text-align:left">lstsq</td>
<td style="text-align:left">Compute the least-squares solution to Ax = b</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>numpy</category>
      </categories>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title>soft-margin SVM</title>
    <url>/2021/07/13/softmargin-svm/</url>
    <content><![CDATA[<p>吴恩达机器学习CS229 支持向量机</p>
<span id="more"></span>
<p>在吴恩达CS229机器学习课程中，对加入 $\ell_{1} \text{ norm regularization}$ 的dual problem只给出结果而没有给出过程推导细节。本人对下面dual problem，给出推导细节：</p>
<script type="math/tex; mode=display">\begin{array}{ll} \max _{\alpha} & W(\alpha)=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j}\left\langle x^{(i)}, x^{(j)}\right\rangle \\ \text { s.t. } & 0 \leq \alpha_{i} \leq C, \quad i=1, \ldots, m \\ & \sum_{i=1}^{m} \alpha_{i} y^{(i)}=0 \end{array}</script><p>首先对于一个training set，一般是线性不可分的，还有一些异常值点outlier。考虑到（1）线性不可分，（2）还有异常值点带来的负面影响，我们允许支持向量机对训练集某些样本点，如 $(x^{(i)}, y^{(i)})$ 的geometric margin有一定误差 $\xi<em>{i}$ 。下图解释下 $\xi</em>{i}$ 的几何意义。</p>
<p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/SVM/soft_margin_svm.png?raw=true" alt="soft_margin"></p>
<p>上图中，左上角的⭕️就是一个异常值点outlier，我们想降低这个异常值点给我们的支持向量机带来的负面影响。虚线是理想的decision boundary，能合适地正确分类⭕️和❌。由于受到了异常值点的影响，虚线变到了实线所在位置。如何减少左上角这个异常值带来的的负面影响？对每个点我们加入误差 $\xi_{i}$ ，它代表的意义就是图中红线的长度，就是一个functional margin &lt;= 1的点的functional margin： $\hat{\gamma}^{(i)}$ ，这个 $\hat{\gamma}^{(i)}$ 可以小于1，也可以小于0（被错误分类）。（functional margin和geometric margin的意义见吴恩达课程讲解）。</p>
<p>对于正确分类的点（除了支持向量），其 $\hat{\gamma}^{(i)}&gt;1,  \xi_{i} = 0$</p>
<p>对于支持向量，其 $\hat{\gamma}^{(i)}=1,  \xi_{i} = 0$</p>
<p>对于异常值点，正确分类了，但是 $\hat{\gamma}^{(i)}&lt;1,  0&lt;\xi_{i}&lt;1$</p>
<p>对于被错误分类的点，其 $\hat{\gamma}^{(i)}<0,  \xi_{i}>1$</p>
<p>对每个样本点加入了误差 $\xi_{i}$ 后，我们就可以处理线性不可分的数据和降低异常值带来的负面影响。我们现在有：</p>
<script type="math/tex; mode=display">\begin{array}{rlr} \underset{w, b, \xi}{\operatorname{minimize}} & \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \xi_{i} & \\ \text { subject to } & y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1-\xi_{i}, & i=1, \ldots, m, \\ & \xi_{i} \geq 0, & i=1, \ldots, m \end{array}</script><p>$C$ 控制着 $\frac{1}{2}|w|^{2}$ 和 $\sum<em>{i=1}^{m} \xi</em>{i}$ 的相对权重，一方面我们希望 $\frac{1}{2}|w|^{2}$ 越小越好，对应着支持向量的geometric margin越大越好。另一方面，我们能够容忍一定程度的误差 $\sum<em>{i=1}^{m} \xi</em>{i}$ ，但是这个误差尽管可以存在，但是也是越小越好。就像一个跷跷板，跷跷板的两端需要一个平衡。二者都要兼顾，雨露均沾，不能只顾一个，冷落了另一个。</p>
<p>我们把上面的优化问题，写出标准形式，并应用拉格朗日乘子法：</p>
<script type="math/tex; mode=display">\begin{array}{rlr} \underset{w, b, \xi}{\operatorname{minimize}} & \frac{1}{2}\|w\|^{2}+C \sum_{i=1}^{m} \xi_{i} & \\ \text { subject to } & 1-\xi_{i}-y^{(i)}\left(w^{T} x^{(i)}+b\right) \leq 0, & i=1, \ldots, m, \\ & -\xi_{i} \leq 0, & i=1, \ldots, m . \end{array}</script><p>在不等式约束条件下，我们有拉格朗日函数generalized Lagrangian：</p>
<p>$\mathcal{L}(w, b, \xi, \alpha, \beta)=\frac{1}{2}|w|^{2}+C \sum<em>{i=1}^{m} \xi</em>{i}+\sum<em>{i=1}^{m} \alpha</em>{i}\left(1-\xi<em>{i}-y^{(i)}\left(w^{T} x^{(i)}+b\right)\right)-\sum</em>{i=1}^{m} \beta<em>{i} \xi</em>{i}$</p>
<p>注意，上面这个拉格朗日函数中，参数 $(w, b, \xi)$ 就相当于拉格朗日乘子法里面的 $x$ ， $(\alpha, \beta)$ 相当于拉格朗日乘子 $\lambda$ 。</p>
<p>给出这个问题的primal and dual optimization problems:</p>
<script type="math/tex; mode=display">\begin{array}{rlrlr} \max _{\alpha, \beta: \alpha_{i} \geq 0, \beta_{i} \geq 0} & \theta_{D}(\alpha, \beta) & \text { where } \theta_{D}(\alpha, \beta):=\min _{w, b, \xi} & \mathcal{L}(w, b, \xi, \alpha, \beta), & & \text { (SVM-D) } \\ \min _{w, b, \xi} & \theta_{P}(w, b, \xi) & \text { where } \theta_{P}(w, b, \xi):=\max _{\alpha, \beta: \alpha_{i} \geq 0, \beta_{i} \geq 0} & \mathcal{L}(w, b, \xi, \alpha, \beta) . & & (\text { SVM-P }) \end{array}</script><p>上面是拉格朗日对偶，不懂需要看CS229支持向量机部分对它的的讲解。</p>
<h2 id="1-Eliminating-the-primal-variables"><a href="#1-Eliminating-the-primal-variables" class="headerlink" title="1 Eliminating the primal variables"></a>1 Eliminating the primal variables</h2><p>为了通过对偶dual问题来求解原始primal问题，我们计算 $\theta_{D}(\alpha, \beta)$ ：</p>
<script type="math/tex; mode=display">\theta_{D}(\alpha, \beta)=\min _{w, b, \xi} \quad \mathcal{L}(w, b, \xi, \alpha, \beta)</script><p>这是一个无约束优化问题 。上面的拉格朗日函数 $\mathcal{L}(w, b, \xi, \alpha, \beta)$ 是可微的differentiable。根据拉格朗日对偶，对于固定的 $(\alpha, \beta)$ ，假设我们找到了 $(\hat{w}, \hat{b}, \hat{\xi})$ 最小化拉格朗日函数，一定有必要条件：拉格朗日函数对参数 $(w, b, \xi)$ 的梯度和偏导数为0向量或0。</p>
<script type="math/tex; mode=display">\begin{aligned} &\nabla_{w} \mathcal{L}(\hat{w}, \hat{b}, \hat{\xi}, \alpha, \beta)=\hat{w}-\sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)}=0 \\ &\frac{\partial}{\partial b} \mathcal{L}(\hat{w}, \hat{b}, \hat{\xi}, \alpha, \beta)=-\sum_{i=1}^{m} \alpha_{i} y^{(i)}=0 \\ &\frac{\partial}{\partial \xi_{i}} \mathcal{L}(\hat{w}, \hat{b}, \hat{\xi}, \alpha, \beta)=C-\alpha_{i}-\beta_{i}=0 \end{aligned}</script><p>把 $-\sum<em>{i=1}^{m} \alpha</em>{i} y^{(i)}=0$ 和 $C-\alpha<em>{i}-\beta</em>{i}=0$ 代回拉格朗日函数：</p>
<p>$\begin{aligned} \theta<em>{D}(\alpha, \beta) &amp;=\mathcal{L}(\hat{w}, \hat{b}, \hat{\xi}) \ &amp;=\frac{1}{2}|\hat{w}|^{2}+C \sum</em>{i=1}^{m} \hat{\xi}<em>{i}+\sum</em>{i=1}^{m} \alpha<em>{i}\left(1-\hat{\xi}</em>{i}-y^{(i)}\left(\hat{w}^{T} x^{(i)}+\hat{b}\right)\right)-\sum<em>{i=1}^{m} \beta</em>{i} \hat{\xi}<em>{i} \ &amp;=\frac{1}{2}|\hat{w}|^{2}+C \sum</em>{i=1}^{m} \hat{\xi}<em>{i}+\sum</em>{i=1}^{m} \alpha<em>{i}\left(1-\hat{\xi}</em>{i}-y^{(i)}\left(\hat{w}^{T} x^{(i)}\right)\right)-\sum<em>{i=1}^{m} \beta</em>{i} \hat{\xi}<em>{i} \ &amp;=\frac{1}{2}|\hat{w}|^{2}+\sum</em>{i=1}^{m} \alpha_{i}\left(1-y^{(i)}\left(\hat{w}^{T} x^{(i)}\right)\right) \end{aligned}$</p>
<p>稍微解释下上面的推导过程，第一个等式就是对固定的 $(\alpha, \beta)$ 找到最佳解 $(\hat{w}, \hat{b}, \hat{\xi})$ ，第二个等式就是generalized Lagrangian拉格朗日函数的定义，第三和第四个等式就是分别把 $-\sum<em>{i=1}^{m} \alpha</em>{i} y^{(i)}=0$ 和 $C-\alpha<em>{i}-\beta</em>{i}=0$ 代入。</p>
<p>再代入 $\hat{w}=\sum<em>{i=1}^{m} \alpha</em>{i} y^{(i)} x^{(i)}$ ，有</p>
<script type="math/tex; mode=display">\begin{aligned} \frac{1}{2}\|\hat{w}\|^{2}+\sum_{i=1}^{m} \alpha_{i}\left(1-y^{(i)}\left(\hat{w}^{T} x^{(i)}\right)\right) &=\sum_{i=1}^{m} \alpha_{i}+\frac{1}{2}\|\hat{w}\|^{2}-\hat{w}^{T} \sum_{i=1}^{m} \alpha_{i} y^{(i)} x^{(i)} \\ &=\sum_{i=1}^{m} \alpha_{i}+\frac{1}{2}\|\hat{w}\|^{2}-\|\hat{w}\|^{2} \\ &=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2}\|\hat{w}\|^{2} \\ &=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{i} y^{(i)} y^{(j)}\left\langle x^{(i)}, x^{(j)}\right\rangle  \end{aligned}</script><p>至此，我们通过 $-\sum<em>{i=1}^{m} \alpha</em>{i} y^{(i)}=0$ 和 $C-\alpha<em>{i}-\beta</em>{i}=0$ 和 $\hat{w}=\sum<em>{i=1}^{m} \alpha</em>{i} y^{(i)} x^{(i)}$ ，在拉格朗日函数中用dual variables $(\alpha, \beta)$ 替代了primal variables $(w, b, \xi)$ 。我们的对偶问题化简为：</p>
<script type="math/tex; mode=display">\begin{aligned} \underset{\alpha, \beta}{\operatorname{maximize}} & \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{i} y^{(i)} y^{(j)}\left\langle x^{(i)}, x^{(j)}\right\rangle \\ \text {s.t. } & \alpha_{i} \geq 0, \\ & \beta_{i} \geq 0, \\ & \alpha_{i}+\beta_{i}=C, \\ & \sum_{i=1}^{m} \alpha_{i} y^{(i)}=0 \end{aligned}</script><h2 id="2-KKT-complementary-KKT互补条件"><a href="#2-KKT-complementary-KKT互补条件" class="headerlink" title="2. KKT complementary KKT互补条件"></a>2. KKT complementary KKT互补条件</h2><p>满足KKT条件，我们有对偶问题的解等于原始问题的解。</p>
<p>KKT complementarity需要对于任意原始最优解 $\left(w^{*}, b^{*}, \xi^{*}\right)$ 和对偶最优解 $\left(\alpha^{*}, \beta^{*}\right)$ ，有：</p>
<script type="math/tex; mode=display">\begin{aligned} \alpha_{i}^{*}\left(1-\xi_{i}^{*}-y^{(i)}\left(w^{* T} x^{(i)}+b^{*}\right)\right) &=0 \\ \beta_{i}^{*} \xi_{i}^{*} &=0 \end{aligned}</script><p>对于支持向量，异常值和分类错误的点有 $\alpha_{i}&gt;0$ ，根据KKT complementarity：</p>
<script type="math/tex; mode=display">\alpha_{i}^{* }\left(1-\xi_{i}^{* }-y^{(i)}\left(w^{ * T} x^{(i)}+b^{* }\right)\right)=0</script><p>有 </p>
<script type="math/tex; mode=display">1-\xi_{i}^{ * }-y^{(i)}\left(w^{ * T} x^{(i)}+b^{ * }\right)=0</script><script type="math/tex; mode=display">1-\xi_{i}^{ * }=y^{(i)}\left(w^{* T} x^{(i)}+b^{* }\right)\leq1</script><p>因为 $\xi^{*} \geq 0$ 。</p>
<p>而其他大多数样本点的 $\alpha<em>{i}=0$ ，有 $y^{(i)}\left(w^{T} x^{(i)}+b\right) \geq 1-\xi</em>{i}$ （primal constraint）。</p>
<p>最后，因为 $\beta<em>{i}^{*}&gt;0$ 等价于 $\alpha</em>{i}^{*}&lt;C\left(\text { since } \alpha^{*}+\beta_{i}^{*}=C\right)$ 。我们可以将KKT条件总结如下:</p>
<script type="math/tex; mode=display">\begin{array}{r} \alpha_{i}^{*}=0 \Rightarrow y^{(i)}\left(w^{* T} x^{(i)}+b^{*}\right) \geq 1 \\ 0<\alpha_{i}^{*}<C \Rightarrow y^{(i)}\left(w^{* T} x^{(i)}+b^{*}\right)=1 \\ \alpha_{i}^{*}=C \Rightarrow y^{(i)}\left(w^{* T} x^{(i)}+b^{*}\right) \leq 1 \end{array}</script><h2 id="3-化简"><a href="#3-化简" class="headerlink" title="3. 化简"></a>3. 化简</h2><p>我们观察到两个约束条件： $\beta<em>{i} \geq 0 \quad  \alpha</em>{i}+\beta_{i}=C$</p>
<p>等价于单一约束条件： $\alpha_{i} \leq C$</p>
<p>我们解决如下优化问题：</p>
<script type="math/tex; mode=display">\begin{array}{ll} \underset{\alpha, \beta}{\operatorname{maximize}} & \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{i} y^{(i)} y^{(j)}\left\langle x^{(i)}, x^{(j)}\right\rangle \\ \text { subject to } & 0 \leq \alpha_{i} \leq C, \\ & \sum_{i=1}^{m} \alpha_{i} y^{(i)}=0 \end{array}</script><p>然后设 $\beta<em>{i}=C-\alpha</em>{i}$ ，那么 $(\alpha, \beta)$ 将是上述对偶问题的最优解。以上对偶优化问题就是CS229课程中吴恩达推导出的加入 $$ 后的最终形式，只是比没加入 $\ell<em>{1} \text{ norm regularization}$ 之前多出一个 $\alpha</em>{i} \leq C$ 。</p>
]]></content>
      <categories>
        <category>支持向量机</category>
      </categories>
      <tags>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title>SMO算法</title>
    <url>/2021/07/12/svm/</url>
    <content><![CDATA[<p>参考资料:  吴恩达CS229 机器学习</p>
<span id="more"></span>
<h2 id="1-Coordinate-ascent"><a href="#1-Coordinate-ascent" class="headerlink" title="1.Coordinate ascent"></a>1.Coordinate ascent</h2><p>为解决SVM的对偶问题，John Platt提出了SMO(顺序最小优化)算法。为了引出SMO算法，让我们先讨论coordinate ascent algorithm（坐标上升算法）。</p>
<p>我们已经学过两种优化算法，gradient ascent和Newton’ method。我们现在学习的新算法叫做coordinate ascent。考虑一个无约束优化问题：</p>
<script type="math/tex; mode=display">\max _{\alpha} W\left(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}\right)</script><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">Loop until convergence: &#123;</span><br><span class="line">    For i = <span class="number">1</span>,...,m, &#123;</span><br><span class="line">        αi := argmax_&#123;αˆi&#125; <span class="built_in">W</span>(α<span class="number">1</span>,...,αi−<span class="number">1</span>,αˆi,αi+<span class="number">1</span>,...,αm).</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最内层的循环每次只更新一个参数，来最大化 $W\left(\alpha<em>{1}, \alpha</em>{2}, \ldots, \alpha_{m}\right)$ ，不停循环，直到收敛。<br>coordinate ascent的实际运行效果如图所示：<br><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/SVM/截屏2021-07-12%20下午12.20.05.png?raw=true" alt="avatar"></p>
<h2 id="2-SMO"><a href="#2-SMO" class="headerlink" title="2. SMO"></a>2. SMO</h2><p>在支持向量机的课程中，加入 $\ell_{1} \text{ norm regularization} $ 后的dual optimization problem为：</p>
<script type="math/tex; mode=display">\begin{aligned} \max _{\alpha} \quad& W(\alpha)=\sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i, j=1}^{m} y^{(i)} y^{(j)} \alpha_{i} \alpha_{j}\left\langle x^{(i)}, x^{(j)}\right\rangle . \\ \text { s.t. } \quad & 0 \leq \alpha_{i} \leq C, \quad i=1, \ldots, m \\ & \sum_{i=1}^{m} \alpha_{i} y^{(i)}=0 \end{aligned}</script><p>由于约束 $\sum<em>{i=1}^{m} \alpha</em>{i} y^{(i)}=0$ 的存在，让我们以优化 $\alpha_{1}$ 为例。因为，</p>
<script type="math/tex; mode=display">\alpha_{1} y^{(1)}=-\sum_{i=2}^{m} \alpha_{i} y^{(i)}</script><p>等式两边同时乘以 $y^{(1)}$ ，因为 $y^{(1)} \in{-1,1}, \Rightarrow \left(y^{(1)}\right)^{2}=1$</p>
<script type="math/tex; mode=display">\alpha_{1}=-y^{(1)} \sum_{i=2}^{m} \alpha_{i} y^{(i)}</script><p>所以，如果保持 $\alpha<em>{2}, \ldots, \alpha</em>{m}$ 不变， $\alpha<em>{1}$ 是不可能变动的。所以我们只能同时更新至少两个变量，才能满足约束条件 $\sum</em>{i=1}^{m} \alpha_{i} y^{(i)}=0$ 。这就是SMO算法：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Repeat till convergence &#123;</span><br><span class="line">    1. Select some pair αi and αj to update next (using a heuristic that tries to</span><br><span class="line">       pick the two that will allow us to make the biggest progress towards the </span><br><span class="line">       global maximum).</span><br><span class="line">    2. Reoptimize W(α) with respect to αi and αj, while holding all the other αk’s</span><br><span class="line">       (k != i, j) fixed.</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>SMO sequential minimal optimization，minimal代表什么含义呢？就是在一次更新中，选择最小数量的参数 $\alpha_{i}$ 去更新。</p>
<p>让我们先以更新 $\alpha<em>{1},\alpha</em>{2}$ 为例，保持 $\alpha<em>{3}, \ldots, \alpha</em>{m}$ 不变。根据约束 $\sum<em>{i=1}^{m} \alpha</em>{i} y^{(i)}=0$ 。我们有：</p>
<script type="math/tex; mode=display">\alpha_{1} y^{(1)}+\alpha_{2} y^{(2)}=-\sum_{i=3}^{m} \alpha_{i} y^{(i)}=\zeta</script><p>上式等式右手边是常数 $\zeta$ ，因为 $\alpha<em>{3}, \ldots, \alpha</em>{m}$ 不变。</p>
<p>根据加入 $\ell<em>{1} \text{ norm regularization} $ KKT对偶条件 $0 \leq \alpha</em>{i} \leq C$ 。以及 $\alpha<em>{1} y^{(1)}+\alpha</em>{2} y^{(2)}=\zeta$</p>
<p><img src="https://github.com/Splendid-sun/Hexo-photo/blob/master/SVM/截屏2021-07-12%20下午3.53.05.png?raw=true" alt="avatar"></p>
<p>这是一种box constraints， $\alpha<em>{1},\alpha</em>{2}$ 既要在 $[0, C] \times[0, C]$ 这个方形区域内，也要在 $\alpha<em>{1} y^{(1)}+\alpha</em>{2} y^{(2)}=\zeta$ 这条直线上。根据 $\alpha<em>{1} y^{(1)}+\alpha</em>{2} y^{(2)}=\zeta$ ，我们有：</p>
<script type="math/tex; mode=display">\alpha_{1}=\left(\zeta-\alpha_{2} y^{(2)}\right) y^{(1)} \\</script><p>把这个代入 $W(\alpha)$ ， $W$ 是一个凸函数：</p>
<script type="math/tex; mode=display">W\left(\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}\right)=W\left(\left(\zeta-\alpha_{2} y^{(2)}\right) y^{(1)}, \alpha_{2}, \ldots, \alpha_{m}\right)</script><p>上式是一个关于 $\alpha<em>{2}$ 的一元二次函数： $a \alpha</em>{2}^{2}+b \alpha<em>{2}+c$ 。这就是一个简单的初中就学过的二次函数求极值的问题。求出 $\alpha</em>{2}^{n e w}$ 之后，再代回等式 $\alpha<em>{1}=\left(\zeta-\alpha</em>{2} y^{(2)}\right) y^{(1)}$ 就求出 $\alpha_{1}^{n e w}$ 。</p>
]]></content>
      <categories>
        <category>支持向量机</category>
      </categories>
      <tags>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title>softmax反向传播</title>
    <url>/2021/05/24/softmax/</url>
    <content><![CDATA[<p>参考：<a href="https://blog.csdn.net/weixin_43217928/article/details/104772424">https://blog.csdn.net/weixin_43217928/article/details/104772424</a></p>
<span id="more"></span>
<h1 id="softmax的反向传播梯度计算推导"><a href="#softmax的反向传播梯度计算推导" class="headerlink" title="softmax的反向传播梯度计算推导"></a>softmax的反向传播梯度计算推导</h1><p>对于深度学习多分类问题，神经网络最后一层常用softmax处理多项分布（multinomial distribution）。</p>
<p>假设最后一层有n个神经元，就是n个类别：</p>
<p><strong>输入</strong> 输出层神经元：$Z=\left[z<em>{1}, z</em>{2}, \ldots, z<em>{n}\right]$<br><strong>分类标签</strong>：$Y=\left[y</em>{1}, y<em>{2}, \ldots, y</em>{n}\right]$ （Y是one-hot标签，只有一个$y<em>{i}$值为1，其他为0）<br><strong>输出</strong>：$\mathrm{A}=\operatorname{softmax}(\mathrm{Z})=\left[\hat{y}</em>{1}, \hat{y}<em>{2}, \ldots, \hat{y}</em>{n}\right]$，$\hat{y}_{i}$为预测的每个类别的概率。</p>
<p>对于n分类问题，softmax的式子为：</p>
<script type="math/tex; mode=display">
\hat{y}_{i}=\frac{e^{z_{i}}}{\sum_{j=1}^{n} e^{z_{j}}}</script><p>损失函数采用交叉熵，交叉熵损失函数式子为：</p>
<script type="math/tex; mode=display">
L=-\sum_{i=1}^{n} y_{i} l n \hat{y}_{i}</script><p><strong>交叉熵</strong>：用来衡量在给定真实分布下，使用非真实分布所指定的策略消除系统不确定性所需付出努力的大小。交叉熵越低，预测的分布就与真实分布越接近，最低的交叉熵就是使用了真实分布所计算出来的信息熵，此时$y<em>{i} = \hat{y}</em>{i}$。此时交叉熵等于信息熵，这就是为什么在机器学习分类算法中，总是最小化交叉熵，因为交叉熵越低，算法策略越接近最优策略，算法的非真实分布越接近真实分布。</p>
<h2 id="反向传播梯度计算推导过程"><a href="#反向传播梯度计算推导过程" class="headerlink" title="反向传播梯度计算推导过程"></a>反向传播梯度计算推导过程</h2><h2 id="结论：向量形式为-frac-partial-L-partial-Z-hat-Y-Y"><a href="#结论：向量形式为-frac-partial-L-partial-Z-hat-Y-Y" class="headerlink" title="结论：向量形式为$\frac{\partial L}{\partial Z}=\hat Y-Y$"></a>结论：向量形式为$\frac{\partial L}{\partial Z}=\hat Y-Y$</h2><p>证明：<br>在这里我们假设，第k个神经元为正确标签，即$Y=\left[y<em>{1}, y</em>{2}, \ldots, y<em>{n}\right]$中$y</em>{k}=1$，其他$y_{i}$都是0。</p>
<p>首先求L对A的导数</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial \hat{y}_{i}}=\frac{\partial-\sum_{i=1}^{n} y_{i} l n \hat{y}_{i}}{\partial \hat{y}_{i}}=-\frac{y_{i}}{\hat{y}_{i}}</script><p>再求L对Z的导数：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial z_{i}}=\sum_{j=1}^{n} \frac{\partial L}{\partial \hat{y}_{j}} \cdot \frac{\partial \hat{y}_{j}}{\partial z_{i}}=\sum_{j=1}^{n}-\frac{y_{j}}{\hat{y}_{j}} \cdot \frac{\partial \hat{y}_{j}}{\partial z_{i}}</script><p>因为$y<em>{k}=1$, 其他 $y</em>{i}$ 都为0，因此只有$\frac{\partial L}{\partial a_{k}}$不为0，其他都为0。进一步化简：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial z_{i}}=\frac{\partial L}{\partial \hat{y}_{k}} \cdot \frac{\partial \hat{y}_{k}}{\partial z_{i}}</script><p>由此可见，接下来我们重点就是求$\frac{\partial \hat{y}<em>{k}}{\partial z</em>{i}}$了。对于$\frac{\partial \hat{y}<em>{k}}{\partial z</em>{i}}$的求解，要分两种情况：<br>若$i=k$</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac{\partial \hat{y}_{k}}{\partial z_{i}}=\frac{\partial \hat{y}_{k}}{\partial z_{k}}=\frac{\partial\left(\frac{e^{z_{k}}}{\sum_{j=1}^{n} e^{z_{j}}}\right)}{\partial z_{k}}=& \frac{e^{z_{k}}\left(\sum_{j=1}^{n} e^{z_{j}}\right)-\left(e^{z_{k}}\right)^{2}}{\left(\sum_{j=1}^{n} e^{z_{j}}\right)^{2}}=\frac{e^{z_{k}}}{\sum_{j=1}^{n} e^{z_{j}}}-\left(\frac{e^{z_{k}}}{\sum_{j=1}^{n} e^{z_{j}}}\right)^{2} \\
&=\hat{y}_{k}-\hat{y}_{k}^{2}=\hat{y}_{k}\left(1-\hat{y}_{k}\right)
\end{aligned}</script><p>若$i \neq k$</p>
<script type="math/tex; mode=display">
\frac{\partial \hat{y}_{k}}{\partial z_{i}}=\frac{\partial\left(\frac{e^{z_{k}}}{\sum_{j=1}^{n} e^{z_{j}}}\right)}{\partial z_{i}}=\frac{-e^{z_{k}} \cdot e^{z_{i}}}{\left(\sum_{j=1}^{n} e^{z_{j}}\right)^{2}}=-\frac{e^{z_{k}}}{\sum_{j=1}^{n} e^{z_{j}}} \cdot \frac{e^{z_{i}}}{\sum_{j=1}^{n} e^{z_{j}}}</script><script type="math/tex; mode=display">
=-\hat{y}_{k} \hat{y}_{i}</script><p>结合我们求出的$\frac{\partial L}{\partial \hat{y}<em>{i}}$和$\frac{\partial \hat{y}</em>{i}}{\partial z_{j}}$，写出L对整个Z的导数，我们可以得到</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial Z}=\left[\begin{array}{c}
\frac{\partial L}{\partial z_{1}} \\
\cdots \\
\frac{\partial L}{\partial z_{k}} \\
\ldots \\
\frac{\partial L}{\partial z_{n}}
\end{array}\right]=\left[\begin{array}{cc}
\sum_{i=1}^{n} \frac{\partial L}{\partial \hat{y}_{i}} \cdot \frac{\partial \hat{y}_{i}}{\partial z_{1}} \\
\cdots \\
\sum_{i=1}^{n} \frac{\partial L}{\partial \hat{y}_{i}} \cdot \frac{\partial \hat{y}_{i}}{\partial z_{k}} \\
\cdots \\
\sum_{i=1}^{n} \frac{\partial L}{\partial \hat{y}_{i}} \cdot \frac{\partial \hat{y}_{i}}{\partial z_{n}}
\end{array}\right]=\left[\begin{array}{c}
\frac{\partial L}{\partial \hat{y}_{k}} \cdot \frac{\partial \hat{y}_{k}}{\partial z_{1}} \\
\cdots \\
\frac{\partial L}{\partial \hat{y}_{k}} \cdot \frac{\partial \hat{y}_{k}}{\partial z_{k}} \\
\ldots \\
\frac{\partial L}{\partial \hat{y}_{k}} \cdot \frac{\partial \hat{y}_{k}}{\partial z_{n}}
\end{array}\right]=\left[\begin{array}{c}
-\frac{1}{\hat{y}_{k}} \cdot\left(-\hat{y}_{k} \hat{y}_{1}\right) \\
\cdots \\
-\frac{1}{\hat{y}_{k}} \cdot \hat{y}_{k}\left(1-\hat{y}_{k}\right) \\
\cdots \\
-\frac{1}{\hat{y}_{k}} \cdot\left(-\hat{y}_{k} \hat{y}_{n}\right)
\end{array}\right]=\left[\begin{array}{c}
\hat{y}_{1} \\
\ldots \\
\hat{y}_{k}-1 \\
\ldots \\
\hat{y}_{n}
\end{array}\right]</script><p>我们知道$y<em>{k}=1$, 除了 $y</em>{k}$ 其他 $y_{i}$ 都等于0，因此上式可以进一步写为：</p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial Z}=\left[\begin{array}{c}
\hat{y}_{1} -y_{1} \\
\cdots \\
\hat{y}_{k}-y_{k} \\
\ldots \\
\hat{y}_{n}-y_{n}
\end{array}\right]=\hat Y-Y</script><p>证毕～</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（16）：伪逆（pseudoinverse）</title>
    <url>/2021/04/26/linear_algebra_16/</url>
    <content><![CDATA[<p>清华大学线性代数（2）课程第六讲：伪逆</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<p><strong>问题</strong>：对$m \times n$的矩阵$A$，定义其伪逆（pseudoinverse）$A^{\dagger}$，使得当$A$为n阶可逆矩阵时，有$A^{\dagger}=A^{-1}$。</p>
<p><strong>思路</strong>：设$m \times n$实矩阵$A=U \Sigma V^{T}  \quad(SVD)$，其中$U, V$分别为$m$阶，$n$阶正交阵；$\Sigma$为$m \times n$矩阵，前$r=r(A)$个“对角元”为$A$的奇异值$\sigma<em>{1} \geq \cdots \geq \sigma</em>{r}&gt;0$<br>即：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
A \mathbf{v}_{j}=\sigma_{j} \mathbf{u}_{j} \quad(1 \leq j \leq r) \\
A \mathbf{v}_{j}=\mathbf{0} \quad(r+1 \leq j \leq n)
\end{array}</script><p>若$A$可逆，则$A^{-1} \mathbf{u}<em>{j}=\frac{1}{\sigma</em>{j}} \mathbf{v}<em>{j} \quad(1 \leq j \leq r=m=n)$<br>对$A</em>{m \times n}$，令</p>
<script type="math/tex; mode=display">
A^{\dagger}\left(\begin{array}{lllll}
\mathbf{u}_{1} & \cdots & \mathbf{u}_{r} \mathbf{u}_{r+1} & \cdots & \mathbf{u}_{m}
\end{array}\right)=\left(\begin{array}{llll}
\mathbf{v}_{1} & \cdots & \mathbf{v}_{r} \mathbf{v}_{r+1} & \cdots & \mathbf{v}_{n}
\end{array}\right)\left(\begin{array}{cccc}
\frac{1}{\sigma_{1}} & & & \\
& \ddots & & \\
& & \frac{1}{\sigma_{r}} & \\
& & & 0
\end{array}\right)_{n \times m}</script><p>即：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
A^{\dagger} \mathbf{u}_{j}=\frac{1}{\sigma_{j}} \mathbf{v}_{j} \quad(1 \leq j \leq r) \\
A^{\dagger} \mathbf{u}_{j}=\mathbf{0} \quad(r+1 \leq j \leq m)
\end{array}</script><h2 id="跟据奇异值分解SVD，有伪逆定义：-A-n-times-m-dagger-V-Sigma-dagger-U-T"><a href="#跟据奇异值分解SVD，有伪逆定义：-A-n-times-m-dagger-V-Sigma-dagger-U-T" class="headerlink" title="跟据奇异值分解SVD，有伪逆定义：$A_{n \times m}^{\dagger}:=V \Sigma^{\dagger} U^{T}$"></a>跟据奇异值分解SVD，有伪逆定义：$A_{n \times m}^{\dagger}:=V \Sigma^{\dagger} U^{T}$</h2><p>伪逆的性质：<br>（1）若$A$可逆，则$r=m=n$，则</p>
<script type="math/tex; mode=display">
A^{-1}=\left(U \Sigma V^{T}\right)^{-1}=V \Sigma^{-1} U^{T}=A^{\dagger}</script><p>（2）</p>
<script type="math/tex; mode=display">
A A^{\dagger}=\left(U \Sigma V^{T}\right)\left(V \Sigma^{+} U^{T}\right)=U \Sigma \Sigma^{+} U^{T}=U\left(\begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array}\right)_{m \times m} U^{T}</script><p>$\bullet$ 可以明显看出$A A^{\dagger}$是<strong>对称矩阵</strong>：$\left(A A^{\dagger}\right)^{T}=A A^{\dagger}$</p>
<p>$\bullet$ $A A^{\dagger}=\mathbf{u}<em>{1} \mathbf{u}</em>{1}^{T}+\cdots+\mathbf{u}<em>{r} \mathbf{u}</em>{r}^{T}$，也可以看出$A A^{\dagger}$为r个秩一投影矩阵之和。$\mathbf{u}<em>{1}$到$\mathbf{u}</em>{r}$为$A$的列空间的标准正交基。<br>所以$A A^{\dagger}$为$\mathbb{R}^{m}$到$C(A)$的正交投影矩阵，有</p>
<script type="math/tex; mode=display">
\left(\left.A A^{\dagger}\right|_{C(A)}=i d,\left.A A^{\dagger}\right|_{N\left(A^{T}\right)}=0\right)</script><p>（id为恒同变换）</p>
<p>（3）</p>
<script type="math/tex; mode=display">
A^{\dagger} A=\left(V \Sigma^{\dagger} U^{T}\right)\left(U \Sigma V^{T}\right)=V \Sigma^{\dagger} \Sigma V^{T}=V\left(\begin{array}{cc}
I_{r} & 0 \\
0 & 0
\end{array}\right)_{n \times n} V^{T}</script><p>$\bullet$ 可以明显看出$A^{\dagger} A$是<strong>对称矩阵</strong>，$\left(A^{\dagger} A\right)^{T}=A^{\dagger} A$</p>
<p>$\bullet$ $A^{\dagger} A=\mathbf{v}<em>{1} \mathbf{v}</em>{1}^{T}+\cdots+\mathbf{v}<em>{r} \mathbf{v}</em>{r}^{T}$，也可以看出$A^{\dagger} A$为r个秩一投影矩阵之和。$\mathbf{v}<em>{1}$到$\mathbf{v}</em>{r}$为$A$的行空间的标准正交基。所以$A^{\dagger} A$为$\mathbb{R}^{n}$到$C\left(A^{T}\right)$的正交投影矩阵。</p>
<script type="math/tex; mode=display">
\left(\left.A^{\dagger} A\right|_{C\left(A^{T}\right)}=i d,\left.A^{\dagger} A\right|_{N(A)}=0\right)</script><h2 id="左逆和右逆"><a href="#左逆和右逆" class="headerlink" title="左逆和右逆"></a>左逆和右逆</h2><p>若$r=n$，（$A$列满秩），则$A^{\dagger} A=V V^{T}=I_{n}$，称$A^{\dagger}$为$A$的<strong>左逆</strong>。</p>
<p>若$r=m$，（$A$行满秩），则$A A^{\dagger}=U U^{T}=I_{m}$，称$A^{\dagger}$为$A$的<strong>右逆</strong>。</p>
<p>若$r=m=n$，（$A$满秩），则$A A^{\dagger}=A^{\dagger} A=I, A^{\dagger}=A^{-1}$，则称$A^{\dagger}$为$A$的<strong>双边逆</strong>。</p>
<h2 id="Moore-Penrose伪逆"><a href="#Moore-Penrose伪逆" class="headerlink" title="Moore-Penrose伪逆"></a>Moore-Penrose伪逆</h2><p>Eliakim Hastings Moore（1862-1932），美国数学家，二十世纪初美国数学奠基人</p>
<p>Roger Penrose（1931- ）英国著名数学物理学家，1988Wolf奖得主，与Stephen Hawking合作证明了广义相对论奇点存在性</p>
<p>对于$m \times n$矩阵$A$，Moore意义下的伪逆满足</p>
<script type="math/tex; mode=display">
A X=P_{C(A)}, \quad X A=P_{C(X)}</script><p>的$n \times m$矩阵$X$，$P_{V}$表示到空间$V$的正交投影矩阵</p>
<p>1955年，英国剑桥大学博士研究生Penrose给出了伪逆的如下定义：<br>设$A$为$m \times n$实矩阵，若$n \times m$矩阵$X$满足如下方程组：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
A X A=A \\
X A X=X \\
(A X)^{T}=A X \\
(X A)^{T}=X A
\end{array}</script><p>则称$X$为矩阵$A$的Penrose伪逆</p>
<p><strong>命题</strong>：给定任一$m \times n$实矩阵$A$，$A$的伪逆$A^{\dagger}$是满足Penrose伪逆要求的唯一$n \times m$矩阵</p>
<p><strong>证明</strong>：<br>存在性：由$A^{\dagger}=V \Sigma^{\dagger} U^{T}$，$A=U \Sigma V^{T}$容易验证出$A^{\dagger}$满足Penrose方程组。<br>唯一性：若$X$和$Y$均为矩阵$A$的Penrose伪逆，则可以证明$X=Y$</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>之前我们学习最小二乘法，当方程$A \mathbf{x}=\mathbf{b}$无解时求解方程的最佳近似解。<br>根据normal equation：$A^{T} A \widehat{\mathbf{x}}=A^{T} \mathbf{b}$。因为$r\left(A^{T} A\right)=r(A)=n$。当$r(A)=n$（当$A$列满秩），则$A^{T} A$可逆。在这种情况下，有唯一最小二乘解：$\widehat{\mathbf{x}}=\left(A^{T} A\right)^{-1} A^{T} \mathbf{b}$。</p>
<p>但是当$r(A)&lt;n$，（A列相关）时，有$r\left(A^{T} A\right)=r(A)&lt;n$，normal equation的解不唯一。</p>
<p><strong>命题</strong>：$\mathbf{x}^{\dagger}:=A^{\dagger} \mathbf{b}$为一个最小二乘解</p>
<p><strong>证明</strong>：$A^{T} \mathbf{b}-A^{T} A \mathbf{x}^{\dagger}=A^{T}\left(\mathbf{b}-A \mathbf{x}^{\dagger}\right)=A^{T}\left(\mathbf{b}-A A^{\dagger} \mathbf{b}\right)$<br>由于$A A^{\dagger}$ 为 $\mathbb{R}^{m}$到$C(A)$的正交投影矩阵，故$\mathbf{b}-A A^{\dagger} \mathbf{b} \in N\left(A^{T}\right)$，于是有$A^{T} \mathbf{b}-A^{T} A \mathbf{x}^{+}=A^{T}\left(\mathbf{b}-A A^{+} \mathbf{b}\right)=\mathbf{0}$，得证。</p>
<p><strong>命题</strong>：在$A \mathbf{x}=\mathbf{b}$的所有最小二乘解中，$\mathbf{x}^{\dagger}$的长度最小，称$\mathbf{x}^{\dagger}=A^{\dagger} \mathbf{b}$为$A \mathbf{x}=\mathbf{b}$的最佳最小二乘解。</p>
<p><strong>证明</strong>：<br>设$\widehat{\mathbf{x}}$也是$A^{T} A \widehat{\mathbf{x}}=A^{T} \mathbf{b}$的一个解，即一个最小二乘解，于是</p>
<script type="math/tex; mode=display">
\begin{aligned}
A^{T} A \widehat{\mathbf{x}}=A^{T} \mathbf{b} \\
A^{T} A \mathbf{x}^{\dagger}=A^{T} \mathbf{b}
\end{aligned} \Rightarrow A^{T} A\left(\widehat{\mathbf{x}}-\mathbf{x}^{\dagger}\right)=\mathbf{0} \Rightarrow \widehat{\mathbf{x}}-\mathbf{x}^{\dagger} \in N\left(A^{T} A\right)=N(A)</script><p>而$\mathbf{x}^{\dagger}=A^{\dagger} \mathbf{b} \in C\left(A^{T}\right)$，故$\mathbf{x}^{\dagger} \perp \widehat{\mathbf{x}}-\mathbf{x}^{\dagger}$，有</p>
<script type="math/tex; mode=display">
\|\widehat{\mathbf{x}}\|^{2}=\|\left(\widehat{\mathbf{x}}-\mathbf{x}^{\dagger}\right)+\mathbf{x}^{\dagger}\|^{2}=\|\widehat{\mathbf{x}}-\mathbf{x}^{\dagger}\|^{2}+\|\mathbf{x}^{\dagger}\|^{2} \geq\| \mathbf{x}^{\dagger} \|^{2}</script><p>即$\mathbf{x}^{\dagger}$是长度最小的最小二乘解。<br>$\mathbf{x}^{\dagger}=A^{\dagger} \mathbf{b}$空间关系如下图所示：<br><img src="https://cdn.mathpix.com/snip/images/POUtW2JcVaIekHBRDVg_HKzTRd3vpp5QSOVUJBkLoDo.original.fullsize.png" alt=""></p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（15）：线性空间与线性变换</title>
    <url>/2021/04/08/linear_algebra_15/</url>
    <content><![CDATA[<p>清华大学线性代数（2）课程第四、五讲：线性变换</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<h1 id="线性空间与线性变换"><a href="#线性空间与线性变换" class="headerlink" title="线性空间与线性变换"></a>线性空间与线性变换</h1><p>向量空间又称线性空间，是线性代数中一个最基本的概念。</p>
<h2 id="线性空间的定义和性质"><a href="#线性空间的定义和性质" class="headerlink" title="线性空间的定义和性质"></a>线性空间的定义和性质</h2><p><strong>定义1</strong>  设$V$是一个非空集合，$\mathbb{R}$为实数域。如果在$V$中定义了一个<strong>加法</strong>，即对于任意两个元素$\boldsymbol{\alpha}, \boldsymbol{\beta} \in V$，总有惟一的一个元素$\boldsymbol{\gamma} \in V$与之对应，称为$\boldsymbol{\alpha}$与$\boldsymbol{\beta}$的<strong>和</strong>，记作$\boldsymbol{\gamma}=\boldsymbol{\alpha}+\boldsymbol{\beta}$；在$V$中又定义了一个数与元素的乘法（简称<strong>数乘</strong>），即对于任一数$\lambda \in \mathbb{R}$ 与任一元素$\boldsymbol{\alpha} \in V$，总有惟一的一个元素$\boldsymbol{\delta} \in V$与之对应，称为$\lambda$与$\boldsymbol{\alpha}$的<strong>数量乘积</strong>，记作$\boldsymbol{\delta}=\lambda \boldsymbol{\alpha}$，并且这两种运算满足以下八条运算规律 ( 设$\boldsymbol{\alpha}, \boldsymbol{\beta}, \boldsymbol{\gamma} \in V, \lambda, \mu \in \mathbb{R}$）：<br>（i） $\boldsymbol{\alpha}+\boldsymbol{\beta}=\boldsymbol{\beta}+\boldsymbol{\alpha} ;$<br>（ii）$(\boldsymbol{\alpha}+\boldsymbol{\beta})+\boldsymbol{\gamma}=\boldsymbol{\alpha}+(\boldsymbol{\beta}+\boldsymbol{\gamma}) ;$<br>（iii）在 $V$ 中存在<strong>零元素</strong>$\mathbf{0}$,对任何 $\boldsymbol{\alpha} \in V$,都有 $\boldsymbol{\alpha}+\mathbf{0}=\boldsymbol{\alpha} ;$<br>（iv） 对任何 $\boldsymbol{\alpha} \in V$, 都有 $\boldsymbol{\alpha}$ 的<strong>负元素</strong> $\boldsymbol{\beta} \in V,$ 使 $\boldsymbol{\alpha}+\boldsymbol{\beta}=\mathbf{0} ;$<br>（v） $1 \boldsymbol{\alpha}=\boldsymbol{\alpha}$<br>（vi） $\lambda(\mu \boldsymbol{\alpha})=(\lambda \mu) \boldsymbol{\alpha} ;$<br>（vii） $(\lambda+\mu) \boldsymbol{\alpha}=\lambda \boldsymbol{\alpha}+\mu \boldsymbol{\alpha} ;$<br>（viii） $\lambda(\boldsymbol{\alpha}+\boldsymbol{\beta})=\lambda \boldsymbol{\alpha}+\lambda \boldsymbol{\beta},$<br>那么，$V$就称为（实数域$\mathbb{R}$上的）<strong>向量空间</strong>（或<strong>线性空间</strong>），$V$中元素不论其本来的性质如何，统称为 <strong>（实）向量</strong></p>
<p>简言之，凡满足上述八条规律的加法和数乘运算，就称为<strong>线性运算</strong>；凡定义了线性运算的集合，就称为<strong>向量空间</strong>，其中的元素就称为向量。</p>
<hr>
<p>过去我们习惯把有序数组叫做向量，现在我们把向量和向量空间的概念推广，使向量及向量空间的概念更具一般性。当然推广后向量的概念更加抽象化了。</p>
<ol>
<li>向量不一定是有序数组</li>
<li>向量空间中的运算只要求满足上述八条运算规律</li>
</ol>
<p>下面举一些不是有序数组的向量空间以及向量的实例：</p>
<p><strong>例1：</strong> 次数不超过n的多项式的全体，记作$P[x]_{n}$，即</p>
<script type="math/tex; mode=display">
P[x]_{n}=\left\{\boldsymbol{p}=a_{n} x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x+a_{0} \mid a_{n}, \cdots, a_{1}, a_{0} \in \mathbb{R}\right\}</script><p>对于通常的多项式加法、数乘多项式的乘法构成向量空间。这是因为：通常的多项式加法、数乘多项式的乘法两种运算显然满足线性运算规律</p>
<p><strong>例2：</strong>  正弦函数的集合</p>
<script type="math/tex; mode=display">
S[x]=\{\boldsymbol{s}=A \sin (x+B) \mid A, B \in \mathbb{R}\}</script><p>对于通常的函数加法及数乘函数的乘法构成向量空间。这是因为：通常的函数加法及数乘运算显然满足线性运算规律</p>
<hr>
<h3 id="下面讨论线性空间的性质"><a href="#下面讨论线性空间的性质" class="headerlink" title="下面讨论线性空间的性质"></a>下面讨论线性空间的<strong>性质</strong></h3><p>1.零向量是唯一的</p>
<p>2.任一向量的负向量是唯一的，$\boldsymbol{\alpha}$的负向量记作$-\boldsymbol{\alpha}$</p>
<p>3.$0 \boldsymbol{\alpha}=\mathbf{0},(-1) \boldsymbol{\alpha}=-\boldsymbol{\alpha}, \lambda \mathbf{0}=\mathbf{0}$</p>
<p>4.如果$\lambda \boldsymbol{\alpha}=\mathbf{0}$，则$\lambda=0$ 或 $\boldsymbol{\alpha}=\mathbf{0}$</p>
<hr>
<p><strong>定义2</strong>  设$V$是一个线性空间，$L$是$V$的一个非空子集，如果$L$对于$V$中所定义的加法和数乘两种运算也构成一个线性空间，则称$L$为$V$的<strong>子空间</strong>。</p>
<p><strong>定理</strong>   <strong>线性空间$V$的非空子集$L$构成子空间的充分必要条件是:$L$对于$V$中的线性运算封闭。</strong></p>
<hr>
<h2 id="线性空间同构的定义"><a href="#线性空间同构的定义" class="headerlink" title="线性空间同构的定义"></a>线性空间<strong>同构</strong>的定义</h2><p>一般地，设$V$与$U$是两个线性空间，如果在它们的向量之间有一一对应关系，且这个对应关系保持线性组合的对应，那么就说线性空间$V$与$U$<strong>同构</strong>。</p>
<hr>
<h2 id="基变换与坐标变换"><a href="#基变换与坐标变换" class="headerlink" title="基变换与坐标变换"></a>基变换与坐标变换</h2><p>同一向量在不同的基中有不同的坐标，不同的基与不同的坐标之间有怎样的关系呢?<br>设$\boldsymbol{\alpha}<em>{1}, \cdots, \boldsymbol{\alpha}</em>{n}$及$\boldsymbol{\beta}<em>{1}, \cdots, \boldsymbol{\beta}</em>{n}$是线性空间$V_{n}$中的两个基</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}
\boldsymbol{\beta}_{1}=p_{11} \boldsymbol{\alpha}_{1}+p_{21} \boldsymbol{\alpha}_{2}+\cdots+p_{n 1} \boldsymbol{\alpha}_{n}, \\
\boldsymbol{\beta}_{2}=p_{12} \boldsymbol{\alpha}_{1}+p_{22} \boldsymbol{\alpha}_{2}+\cdots+p_{n 2} \boldsymbol{\alpha}_{n}, \\
\cdots \cdots \cdots \cdots \\
\boldsymbol{\beta}_{n}=p_{1 n} \boldsymbol{\alpha}_{1}+p_{2 n} \boldsymbol{\alpha}_{2}+\cdots+p_{n n} \boldsymbol{\alpha}_{n},
\end{array}\right.</script><p>把$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}<em>{n}$这n个有序向量记作$\left(\boldsymbol{\alpha}</em>{1}, \boldsymbol{\alpha}<em>{2}, \cdots, \boldsymbol{\alpha}</em>{n}\right)$，记n阶矩阵$\boldsymbol{P}=\left(p_{i j}\right)$，利用向量和矩阵的形式，上式可表示为</p>
<script type="math/tex; mode=display">
\left(\boldsymbol{\beta}_{1}, \boldsymbol{\beta}_{2}, \cdots, \boldsymbol{\beta}_{n}\right)=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right) \boldsymbol{P}</script><p>上面两式称为<strong>基变换公式</strong>，矩阵$\boldsymbol{P}$称为由基$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}<em>{n}$到基$\boldsymbol{\beta}</em>{1}, \boldsymbol{\beta}<em>{2}, \cdots, \boldsymbol{\beta}</em>{n}$的<strong>过渡矩阵</strong>。由于$\boldsymbol{\beta}<em>{1}, \boldsymbol{\beta}</em>{2}, \cdots, \boldsymbol{\beta}_{n}$线性无关，故过渡矩阵$\boldsymbol{P}$可逆。</p>
<p><strong>定理</strong> 设$V<em>{n}$中的向量$\boldsymbol{\alpha}$在基$\boldsymbol{\alpha}</em>{1}, \boldsymbol{\alpha}<em>{2}, \cdots, \boldsymbol{\alpha}</em>{n}$中的坐标为$\left(x<em>{1}, x</em>{2}, \cdots, x<em>{n}\right)^{\mathrm{T}}$，在基$\boldsymbol{\beta}</em>{1}, \boldsymbol{\beta}<em>{2}, \cdots, \boldsymbol{\beta}</em>{n}$中的坐标为$\left(x<em>{1}^{\prime}, x</em>{2}^{\prime}, \cdots, x_{n}^{\prime}\right)^{\mathrm{T}}$，若两个基满足上面两个关系式，则有<strong>坐标变换公式</strong>：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=\boldsymbol{P}\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)</script><p>或</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)=\boldsymbol{P}^{-1}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)</script><p>证明：因为</p>
<script type="math/tex; mode=display">
\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right)\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)=\boldsymbol{\alpha}=\left(\boldsymbol{\beta}_{1}, \boldsymbol{\beta}_{2}, \cdots, \boldsymbol{\beta}_{n}\right)\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)</script><script type="math/tex; mode=display">
=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right) \boldsymbol{P}\left(\begin{array}{c}
x_{1}^{\prime} \\
x_{2}^{\prime} \\
\vdots \\
x_{n}^{\prime}
\end{array}\right)</script><p>由于$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}_{n}$线性无关，证毕。</p>
<hr>
<h2 id="线性变换"><a href="#线性变换" class="headerlink" title="线性变换"></a>线性变换</h2><p><strong>定义</strong>  设$V<em>{n}, U</em>{m}$分别是n维和m维线性空间，$T$是一个从$V<em>{n}$到$U</em>{m}$的映射，如果映射$T$满足：<br>（i）任给$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2} \in V<em>{n}$（从而$\boldsymbol{\alpha}</em>{1}+\boldsymbol{\alpha}<em>{2} \in V</em>{n}$）有</p>
<script type="math/tex; mode=display">
T\left(\boldsymbol{\alpha}_{1}+\boldsymbol{\alpha}_{2}\right)=T\left(\boldsymbol{\alpha}_{1}\right)+T\left(\boldsymbol{\alpha}_{2}\right)</script><p>（ii）任给$\boldsymbol{\alpha} \in V<em>{n}, \lambda \in \mathbb{R}$（从而$\lambda \boldsymbol{\alpha} \in V</em>{n}$），有</p>
<script type="math/tex; mode=display">
T(\lambda \boldsymbol{\alpha})=\lambda T(\boldsymbol{\alpha})</script><p>那么，$T$就称为从$V<em>{n}$到$U</em>{m}$的线性映射，或称为线性变换。简言之，线性映射就是保持线性组合的对应的映射。</p>
<p>线性变换具有下述基本性质：</p>
<p>（i）$T \mathbf{0}=\mathbf{0}, T(-\boldsymbol{\alpha})=-T \boldsymbol{a}$<br>（ii）若$\boldsymbol{\beta}=k<em>{1} \boldsymbol{\alpha}</em>{1}+k<em>{2} \boldsymbol{\alpha}</em>{2}+\cdots+k<em>{m} \boldsymbol{\alpha}</em>{m}$，则</p>
<script type="math/tex; mode=display">
T \boldsymbol{\beta}=k_{1} T \boldsymbol{\alpha}_{1}+k_{2} T \boldsymbol{\alpha}_{2}+\cdots+k_{m} T \boldsymbol{\alpha}_{m}</script><p>（iii）线性变换$T$的像集$T\left(V_{n}\right)$是一个线性空间，称为线性变换$T$的<strong>像空间</strong>。<br>（iv）使$T \boldsymbol{\alpha}=\mathbf{0}$的$\boldsymbol{\alpha}$的全体</p>
<script type="math/tex; mode=display">
N_{T}=\left\{\boldsymbol{\alpha} \mid \boldsymbol{\alpha} \in V_{n}, T \boldsymbol{\alpha}=\mathbf{0}\right\}</script><p>也是一个线性空间，$N_{T}$称为线性变换$T$的<strong>核</strong></p>
<hr>
<h2 id="线性变换的矩阵表示"><a href="#线性变换的矩阵表示" class="headerlink" title="线性变换的矩阵表示"></a>线性变换的矩阵表示</h2><p>$\mathbb{R}^{n}$中的线性变换$T$，都能用关系式</p>
<script type="math/tex; mode=display">
T(\boldsymbol{x})=\boldsymbol{A} \boldsymbol{x}\left(\boldsymbol{x} \in \mathbb{R}^{n}\right)</script><p>表示，其中$\boldsymbol{A}=\left(T\left(\boldsymbol{e}<em>{1}\right), \cdots, T\left(\boldsymbol{e}</em>{n}\right)\right)$</p>
<p><strong>定义</strong>  设$T$是线性空间$V<em>{n}$中的线性变换，在$V</em>{n}$中取定一个基$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots,<br>\boldsymbol{\alpha}_{n}$，如果这个基在变换$T$下的像(用这个基线性表示)为</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}
T\left(\boldsymbol{\alpha}_{1}\right)=a_{11} \boldsymbol{\alpha}_{1}+a_{21} \boldsymbol{\alpha}_{2}+\cdots+a_{n 1} \boldsymbol{\alpha}_{r}, \\
T\left(\boldsymbol{\alpha}_{2}\right)=a_{12} \boldsymbol{\alpha}_{1}+a_{22} \boldsymbol{\alpha}_{2}+\cdots+a_{n 2} \boldsymbol{\alpha}_{r}, \\
\cdots \cdots \cdots \cdots \\
T\left(\boldsymbol{\alpha}_{n}\right)=a_{1 n} \boldsymbol{\alpha}_{1}+a_{2 n} \boldsymbol{\alpha}_{2}+\cdots+a_{n n} \boldsymbol{\alpha}_{r},
\end{array}\right.</script><p>记$T\left(\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}<em>{n}\right)=\left(T\left(\boldsymbol{\alpha}</em>{1}\right), T\left(\boldsymbol{\alpha}<em>{2}\right), \cdots, T\left(\boldsymbol{\alpha}</em>{n}\right)\right)$，上式可以表示为</p>
<script type="math/tex; mode=display">
T\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right)=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right) \boldsymbol{A}</script><p>其中</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{21} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n n}
\end{array}\right)</script><p>那么，$A$就称为<strong>线性变换$T$在基$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}_{n}$下的矩阵.</strong></p>
<p>显然，矩阵$A$由基的像$T\left(\boldsymbol{\alpha}<em>{1}\right), \cdots, T\left(\boldsymbol{\alpha}</em>{n}\right)$惟一确定。</p>
<p>如果给出一个矩阵$A$作为线性变换$T$在基$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}<em>{n}$下的矩阵，也就是给出了这个基在变换$T$下的像，那么，根据变换$T$保持线性关系的特性，我们来 推导变换$T$必须满足的关系式。<br>$V</em>{n}$中的任意元素记为$\boldsymbol{\alpha}=\sum<em>{i=1}^{n} x</em>{i} \boldsymbol{\alpha}$有</p>
<script type="math/tex; mode=display">
T\left(\sum_{i=1}^{n} x_{i} \boldsymbol{\alpha}_{i}\right)=\sum_{i=1}^{n} x_{i} T\left(\boldsymbol{\alpha}_{i}\right)</script><script type="math/tex; mode=display">
=\left(T\left(\boldsymbol{\alpha}_{1}\right), T\left(\boldsymbol{\alpha}_{2}\right), \cdots, T\left(\boldsymbol{\alpha}_{n}\right)\right)\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)</script><script type="math/tex; mode=display">
=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right) \boldsymbol{A}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)</script><p>即</p>
<script type="math/tex; mode=display">
T\left[\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right)\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)\right]=\left(\boldsymbol{\alpha}_{1}, \boldsymbol{\alpha}_{2}, \cdots, \boldsymbol{\alpha}_{n}\right) \boldsymbol{A}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right) .</script><p>在$V_{n}$中取定一个基以后，由线性变换$T$可惟一地确定一个矩阵$A$，由一个矩阵$A$也可惟一地确定一个线性变换$T$，这样，在线性变换与矩阵之间就有一一对应的关系。</p>
<p>由上面的关系式，可见$\boldsymbol{\alpha}$与$T(\boldsymbol{\alpha})$在基$\boldsymbol{\alpha}<em>{1}, \boldsymbol{\alpha}</em>{2}, \cdots, \boldsymbol{\alpha}_{n}$下的坐标分别是</p>
<script type="math/tex; mode=display">
\boldsymbol{\alpha}=\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right), T(\boldsymbol{\alpha})=\boldsymbol{A}\left(\begin{array}{c}
x_{1} \\
x_{2} \\
\vdots \\
x_{n}
\end{array}\right)</script><p>即按坐标表示，有</p>
<script type="math/tex; mode=display">
T(\boldsymbol{\alpha})=\boldsymbol{A} \boldsymbol{\alpha}</script>]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>期望 方差 协方差</title>
    <url>/2020/09/25/probability_2/</url>
    <content><![CDATA[<p>归纳总结概率论：期望 方差 协方差的定义和性质</p>
<span id="more"></span>
<h2 id="1-数学期望"><a href="#1-数学期望" class="headerlink" title="1.数学期望"></a>1.数学期望</h2><p><strong>定义</strong> 设 <strong>离散型随机变量</strong> $X$的分布律为</p>
<script type="math/tex; mode=display">
P\left\{X=x_{k}\right\}=p_{k}, \quad k=1,2, \cdots</script><p>若级数</p>
<script type="math/tex; mode=display">
\sum_{k=1}^{\infty} x_{k} p_{k}</script><p>绝对收敛，则称级数$\sum<em>{k=1}^{\infty} x</em>{k} p_{k}$的和为随机变量$X$的<strong>数学期望</strong>，记为$E(X)$，即</p>
<script type="math/tex; mode=display">
E(X)=\sum_{k=1}^{\infty} x_{k} p_{k}</script><p>设 <strong>连续型随机变量</strong> $X$的概率密度为$f(x)$，若积分</p>
<script type="math/tex; mode=display">
\int_{-\infty}^{\infty} x f(x) \mathrm{d} x</script><p>绝对收敛，则称积分$\int_{-\infty}^{\infty} x f(x) \mathrm{d} x$的值为随机变量$X$的<strong>数学期望</strong>，记为$E(X)$，即</p>
<script type="math/tex; mode=display">
E(X)=\int_{-\infty}^{\infty} x f(x) \mathrm{d} x</script><p><strong>随机变量的函数的数学期望</strong>：<br><strong>定理</strong> 设$Y$是随机变量$X$的函数：$Y=g(X)$（$g$是连续函数）<br>（i）如果$X$是离散型随机变量，它的分布律为$P\left{X=x<em>{k}\right}=p</em>{k}, k=1,2, \cdots$，若$\sum<em>{k=1}^{\infty} g\left(x</em>{k}\right) p_{k}$绝对收敛，则有</p>
<script type="math/tex; mode=display">
E(Y)=E[g(X)]=\sum_{k=1}^{\infty} g\left(x_{k}\right) p_{k}</script><p>（ii）如果$X$是连续型随机变量，它的概率密度为$f(x)$，若$\int_{-\infty}^{\infty} g(x) f(x) \mathrm{d} x$绝对收敛，则有</p>
<script type="math/tex; mode=display">
E(Y)=E[g(X)]=\int_{-\infty}^{\infty} g(x) f(x) \mathrm{d} x</script><p>上述定理可以推广到两个或两个以上随机变量的函数的情况<br>例如，设$Z$是随机变量$X, Y$的函数$Z=g(X, Y)$，（$g$是连续函数），那么，$Z$是一个一维随机变量，若二维随机变量$(X, Y)$的概率密度为$f(x, y)$，则有</p>
<script type="math/tex; mode=display">
E(Z)=E[g(X, Y)]=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x, y) f(x, y) \mathrm{d} x \mathrm{d} y</script><p>这里设上式右边的积分绝对收敛，又若$(X, Y)$为离散型随机变量，其分布律为$P\left{X=x<em>{i}, Y=y</em>{i}\right}=p_{i j}, i, j=1,2, \cdots$，则有</p>
<script type="math/tex; mode=display">
E(Z)=E[g(X, Y)]=\sum_{j=1}^{\infty} \sum_{i=1}^{\infty} g\left(x_{i}, y_{j}\right) p_{i j}</script><p>这里设上式右边的级数绝对收敛</p>
<p><strong>数学期望的重要性质</strong></p>
<p>1，设$C$是常数，则有$E(C)=C$<br>2，设$X$是一个随机变量，$C$是常数，则有</p>
<script type="math/tex; mode=display">
E(C X)=C E(X)</script><p>3，设$X, Y$是两个随机变量，则有</p>
<script type="math/tex; mode=display">
E(X+Y)=E(X)+E(Y)</script><p>4，设$X, Y$是相互独立的随机变量，则有</p>
<script type="math/tex; mode=display">
E(X Y)=E(X) E(Y)</script><p>这一性质可以推广到任意有限个相互独立的随机变量之积的情况</p>
<h2 id="2-方差"><a href="#2-方差" class="headerlink" title="2.方差"></a>2.方差</h2><p><strong>定义</strong> 设$X$是一个随机变量，若$E\left{[X-E(X)]^{2}\right}$存在，则称$E\left{[X-E(X)]^{2}\right}$为$X$的<strong>方差</strong>，记为$D(X)$或$\operatorname{Var}(X)$，即</p>
<script type="math/tex; mode=display">
D(X)=\operatorname{Var}(X)=E\left\{[X-E(X)]^{2}\right\}</script><p>在应用上还引入量$\sqrt{D(X)}$，记为$\sigma(X)$，称为<strong>标准差</strong></p>
<p>由定义知，方差实际上就是随机变量$X$的函数$g(X)=(X-E(X))^{2}$的数学期望，于是对于离散型随机变量，有</p>
<script type="math/tex; mode=display">
D(X)=\sum_{k=1}^{\infty}\left[x_{k}-E(X)\right]^{2} p_{k}</script><p>其中$P\left{X=x<em>{k}\right}=p</em>{k}, k=1,2, \cdots$，是$X$的分布律</p>
<p>对于连续型随机变量，有</p>
<script type="math/tex; mode=display">
D(X)=\int_{-\infty}^{\infty}[x-E(X)]^{2} f(x) \mathrm{d} x</script><p>其中$f(x)$是$X$的概率密度</p>
<hr>
<p>随机变量$X$的方差可按下列公式计算</p>
<script type="math/tex; mode=display">
D(X)=E\left(X^{2}\right)-[E(X)]^{2}</script><p>证明：由数学期望的性质，得</p>
<script type="math/tex; mode=display">
D(X)=E\left\{[X-E(X)]^{2}\right\}=E\left\{X^{2}-2 X E(X)+[E(X)]^{2}\right\}</script><script type="math/tex; mode=display">
=E\left(X^{2}\right)-2 E(X) E(X)+[E(X)]^{2}</script><script type="math/tex; mode=display">
=E\left(X^{2}\right)-[E(X)]^{2}</script><hr>
<p><strong>标准化变量</strong> 设随机变量$X$具有数学期望$E(X)=\mu$，方差$D(X)=\sigma^{2} \neq 0$，记</p>
<script type="math/tex; mode=display">
X^{*}=\frac{X-\mu}{\sigma}</script><p>则</p>
<script type="math/tex; mode=display">
E\left(X^{*}\right)=\frac{1}{\sigma} E(X-\mu)=\frac{1}{\sigma}[E(X)-\mu]=0</script><script type="math/tex; mode=display">
D\left(X^{*}\right)=E\left(X^{* 2}\right)-\left[E\left(X^{*}\right)\right]^{2}=E\left[\left(\frac{X-\mu}{\sigma}\right)^{2}\right]</script><script type="math/tex; mode=display">
=\frac{1}{\sigma^{2}} E\left[(X-\mu)^{2}\right]=\frac{\sigma^{2}}{\sigma^{2}}=1</script><p>即$X^{*}=\frac{X-\mu}{\sigma}$的数学期望为0，方差为1，$X^{*}$称为$X$的 <strong>标准化变量</strong></p>
<hr>
<p>方差的重要性质</p>
<p>1，设$C$是常数，则$D(C)=0$<br>2，设$X$是随机变量，$C$是常数，则有</p>
<script type="math/tex; mode=display">
D(C X)=C^{2} D(X), \quad D(X+C)=D(X)</script><p>3，设$X$，$Y$是两个随机变量，则有</p>
<script type="math/tex; mode=display">
D(X+Y)=D(X)+D(Y)+2 E\{(X-E(X))(Y-E(Y))\}</script><p>4，$D(X)=0$的充要条件是$X$以概率1取常数$E(X)$，即</p>
<script type="math/tex; mode=display">
P\{X=E(X)\}=1</script><h2 id="3-切比雪夫（Chebyshev）不等式"><a href="#3-切比雪夫（Chebyshev）不等式" class="headerlink" title="3.切比雪夫（Chebyshev）不等式"></a>3.切比雪夫（Chebyshev）不等式</h2><p>设随机变量$X$具有数学期望$E(X)=\mu$，方差$D(X)=\sigma^{2}$，则对于任意正数$\varepsilon$，不等式</p>
<script type="math/tex; mode=display">
P\{|X-\mu| \geqslant \varepsilon\} \leqslant \frac{\sigma^{2}}{\varepsilon^{2}}</script><p>成立，这一不等式称为<strong>切比雪夫（Chebyshev）不等式</strong><br>切比雪夫不等式也可以写成如下的形式：</p>
<script type="math/tex; mode=display">
P\{|X-\mu|<\varepsilon\} \geqslant 1-\frac{\sigma^{2}}{\varepsilon^{2}}</script>]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>Basic Concepts of Probability</title>
    <url>/2020/02/23/Basic_Concepts_of_Probability/</url>
    <content><![CDATA[<p>归纳总结概率论重要的基本概念</p>
<span id="more"></span>
<h1 id="第一章-概率论的基本概念"><a href="#第一章-概率论的基本概念" class="headerlink" title="第一章 概率论的基本概念"></a>第一章 概率论的基本概念</h1><h2 id="1-随机试验"><a href="#1-随机试验" class="headerlink" title="1.随机试验"></a>1.随机试验</h2><p><strong>确定性现象</strong>：在一定条件下必然发生的现象</p>
<p><strong>统计规律性</strong>：在大量重复试验或观察中所呈现出的固有规律性</p>
<p><strong>随机现象</strong>：在个别试验中其结果呈现出不确定性，在大量重复试验中其结果又具有统计规律性的现象</p>
<p>概率论与数理统计是研究和揭示随机现象统计规律性的一门数学学科</p>
<p><strong>随机试验</strong>：具有下述三个特点的试验称为随机试验<br>    1.可以在相同的条件下重复地进行<br>    2.每次试验的可能结果不止一个，并且能事先明确试验的所有可能结果<br>    3.进行一次试验之前不能确定哪一个结果会出现</p>
<h2 id="2-样本空间、随机事件"><a href="#2-样本空间、随机事件" class="headerlink" title="2.样本空间、随机事件"></a>2.样本空间、随机事件</h2><p>随机试验$E$的所有可能结果组成的集合称为$E$的<strong>样本空间</strong>，记为$S$，样本空间的元素，即$E$的每个结果，称为<strong>样本点</strong>。</p>
<p>随机试验$E$的样本空间$S$的子集为$E$的<strong>随机事件</strong></p>
<p>由一个样本点组成的单点集，称为<strong>基本事件</strong></p>
<p>样本空间$S$包含所有的样本点，它是$S$自身的子集，在每次试验中它总是发生的，$S$称为<strong>必然事件</strong></p>
<p>空集$\varnothing$不包含任何样本点，它也作为样本空间的子集，在每次试验中都不发生，$\varnothing$称为<strong>不可能事件</strong></p>
<p>若$A \cap B=\varnothing$，则称事件$A$和$B$是<strong>互不相容</strong>的，或<strong>互斥的</strong>，事件$A$和$B$不能同时发生，基本事件是两两互不相容的</p>
<p>若$A \cup B=S$且$A \cap B=\varnothing$，则称事件$A$和$B$互为<strong>逆事件</strong>、<strong>对立事件</strong>，对每次试验而言，事件$A$和$B$必有一个发生，且仅有一个发生。$A$的对立事件记为$\bar{A}$，$\bar{A}=S-A$</p>
<p>事件运算定律，设$A, B, C$为事件，则有<br>交换律：$A \cup B=B \cup A ; A \cap B=B \cap A$<br>结合律：$A \cup(B \cup C)=(A \cup B) \cup C$<br>              $A \cap(B \cap C)=(A \cap B) \cap C$<br>分配律：$A \cup(B \cap C)=(A \cup B) \cap(A \cup C)$<br>              $A \cap(B \cup C)=(A \cap B) \cup(A \cap C)$<br><strong>德摩根律</strong>：$\overline{A \cup B}=\bar{A} \cap \bar{B}$<br>                       $\overline{A \cap B}=\bar{A} \cup \bar{B}$</p>
<h2 id="3-频率与概率"><a href="#3-频率与概率" class="headerlink" title="3.频率与概率"></a>3.频率与概率</h2><p>频率与概率，古典概型，条件概率比较基础，略过</p>
<p>设$S$为试验$E$的样本空间，$B<em>{1}, B</em>{2}, \cdots, B<em>{n}$为$E$的一组事件，若<br>（i）$B</em>{i} B<em>{j}=\varnothing, i \neq j, i, j=1,2, \cdots, n$<br>（ii）$B</em>{1} \cup B<em>{2} \cup \cdots \cup B</em>{n}=S$<br>则称$B<em>{1}, B</em>{2}, \cdots, B_{n}$为样本空间$S$的一个<strong>划分</strong></p>
<p><strong>全概率公式</strong> 设试验$E$的样本空间为$S$，$A$为$E$的事件，$B<em>{1}, B</em>{2}, \cdots, B<em>{n}$为$S$的一个划分，且$P\left(B</em>{i}\right)&gt;0(i=1,2, \cdots, n)$，则</p>
<script type="math/tex; mode=display">
P(A)=P\left(A \mid B_{1}\right) P\left(B_{1}\right)+P\left(A \mid B_{2}\right) P\left(B_{2}\right)+\cdots+P\left(A \mid B_{n}\right) P\left(B_{n}\right)</script><p>为全概率公式</p>
<p><strong>贝叶斯（Bayes）公式</strong> 设试验$E$的样本空间为$S$，$A$为$E$的事件，$B<em>{1}, B</em>{2}, \cdots, B<em>{n}$为$S$的一个划分，且$P(A)&gt;0, P\left(B</em>{i}\right)&gt;0(i=1,2, \cdots, n)$，则</p>
<script type="math/tex; mode=display">
P\left(B_{i} \mid A\right)=\frac{P\left(A \mid B_{i}\right) P\left(B_{i}\right)}{\sum_{j=1}^{n} P\left(A \mid B_{j}\right) P\left(B_{j}\right)}, \quad i=1,2, \cdots, n</script><p>称为<strong>贝叶斯（Bayes）公式</strong></p>
<h2 id="4-独立性"><a href="#4-独立性" class="headerlink" title="4.独立性"></a>4.独立性</h2><p>设$A, B$是两事件，如果满足等式</p>
<script type="math/tex; mode=display">
P(A B)=P(A) P(B)</script><p>则称事件$A, B$相互独立，简称$A, B$独立，若事件$A$和$B$相互独立，则它们与它们的对立事件之间也互相独立</p>
<p>一般，设$A<em>{1}, A</em>{2}, \cdots, A<em>{n}$是$n(n \geqslant 2)$个事件，如果对于其中任意2个，任意3个，…，任意$n$个事件的积事件的概率，都等于各事件概率之积，则称事件$A</em>{1}, A<em>{2}, \cdots, A</em>{n}$<strong>相互独立</strong></p>
<h1 id="第二章-随机变量及其分布"><a href="#第二章-随机变量及其分布" class="headerlink" title="第二章 随机变量及其分布"></a>第二章 随机变量及其分布</h1><h2 id="1-随机变量"><a href="#1-随机变量" class="headerlink" title="1.随机变量"></a>1.随机变量</h2><p><strong>定义</strong> 设随机试验的样本空间为$S={e}$。$X=X(e)$是定义在样本空间$S$上的实值单值函数。称$X=X(e)$为随机变量</p>
<h2 id="2-离散型随机变量及其分布律"><a href="#2-离散型随机变量及其分布律" class="headerlink" title="2.离散型随机变量及其分布律"></a>2.离散型随机变量及其分布律</h2><p>全部可能取值是有限个或可列无限多个的随机变量称为<strong>离散型随机变量</strong></p>
<p>设离散型随机变量$X$所有可能取的值为$x<em>{k}(k=1,2, \cdots)$，$X$取各个可能值的概率，即事件$\left{X=x</em>{k}\right}$的概率，为</p>
<script type="math/tex; mode=display">
P\left\{X=x_{k}\right\}=p_{k}, k=1,2, \cdots</script><p>称上式为离散型随机变量$X$的<strong>分布律</strong>，<br>由概率的定义，$p<em>{k}$满足如下两个条件：<br>1，$p</em>{k} \geqslant 0, k=1,2, \cdots$<br>2，$\sum<em>{k=1}^{\infty} p</em>{k}=1$</p>
<h2 id="3-随机变量的分布函数"><a href="#3-随机变量的分布函数" class="headerlink" title="3.随机变量的分布函数"></a>3.随机变量的分布函数</h2><p><strong>定义</strong> 设$X$是一个随机变量，$x$是任意实数，函数</p>
<script type="math/tex; mode=display">
F(x)=P\{X \leqslant x\},-\infty<x<\infty</script><p>称为$X$的分布函数</p>
<p>分布函数$F(x)$具有以下基本性质<br>1，$F(x)$是一个不减函数<br>2，$0 \leqslant F(x) \leqslant 1$，且</p>
<script type="math/tex; mode=display">
F(-\infty)=\lim _{x \rightarrow-\infty} F(x)=0</script><script type="math/tex; mode=display">
F(\infty)=\lim _{x \rightarrow \infty} F(x)=1</script><p>3，$F(x+0)=F(x)$，$F(x)$是右连续的</p>
<h2 id="4-连续型随机变量及其概率密度"><a href="#4-连续型随机变量及其概率密度" class="headerlink" title="4.连续型随机变量及其概率密度"></a>4.连续型随机变量及其概率密度</h2><p>如果对于随机变量$X$的分布函数$F(x)$，存在非负函数$f(x)$，使对于任意实数x，有</p>
<script type="math/tex; mode=display">
F(x)=\int_{-\infty}^{x} f(t) \mathrm{d} t</script><p>则称$X$为<strong>连续型随机变量</strong>，其中函数$f(x)$称为$X$的<strong>概率密度函数</strong>。</p>
<p>概率密度$f(x)$具有以下性质：<br>1，$f(x) \geqslant 0$<br>2，$\int<em>{-\infty}^{\infty} f(x) \mathrm{d} x=1$<br>3，对于任意实数$x</em>{1}, x<em>{2}\left(x</em>{1} \leqslant x_{2}\right)$，</p>
<script type="math/tex; mode=display">
P\left\{x_{1}<X \leqslant x_{2}\right\}=F\left(x_{2}\right)-F\left(x_{1}\right)=\int_{x_{1}}^{x_{2}} f(x) \mathrm{d} x</script><p>4，若$f(x)$在点$x$处连续，则有$F^{\prime}(x)=f(x)$</p>
<h2 id="5-随机变量的函数的分布"><a href="#5-随机变量的函数的分布" class="headerlink" title="5.随机变量的函数的分布"></a>5.随机变量的函数的分布</h2><p>讨论如何由已知的随机变量$X$的概率密度去求得它的函数$Y=g(X)$（$g(\cdot)$是已知的连续函数）的概率分布。这里$Y$是这样的随机变量，当$X$取值$x$时，$Y$取值$g(x)$</p>
<p>这里我们仅对$Y=g(X)$，其中$g(\cdot)$是严格单调函数的情况，写出一般结果</p>
<p><strong>定理</strong> 设随机变量$X$具有概率密度$f_{X}(x)$，又设函数$g(x)$处处可导且恒有$g^{\prime}(x)&gt;0$，（或恒有$g^{\prime}(x)&lt;0$），则$Y=g(X)$是连续型随机变量，其概率密度为</p>
<script type="math/tex; mode=display">
f_{Y}(y)=\left\{\begin{array}{ll}
f_{X}[h(y)]\left|h^{\prime}(y)\right|, & \alpha<y<\beta \\
0, & \text { 其他 },
\end{array}\right.</script><p>其中$\alpha=\min {g(-\infty), g(\infty)}, \beta=\max {g(-\infty), g(\infty)}$，$h(y)$是$g(x)$的反函数。</p>
<p>若$f(x)$在有限区间$[a, b]$以外等于零，则只需假设$[a, b]$上$g(x)$是单调函数，此时</p>
<script type="math/tex; mode=display">
\alpha=\min \{g(a), g(b)\}, \beta=\max \{g(a), g(b)\}</script><h1 id="第三章-多维随机变量及其分布"><a href="#第三章-多维随机变量及其分布" class="headerlink" title="第三章 多维随机变量及其分布"></a>第三章 多维随机变量及其分布</h1><h2 id="1-二维随机变量"><a href="#1-二维随机变量" class="headerlink" title="1.二维随机变量"></a>1.二维随机变量</h2><p>设$E$是一个随机试验，它的样本空间是$S={e}$，设$X=X(e)$和$Y=Y(e)$是定义在$S$上的随机变量，由它们构成一个向量$(X, Y)$，叫做<strong>二维随机变量</strong></p>
<p><strong>定义</strong> 设$(X, Y)$是二维随机变量，对于任意实数$x, y$，二元函数：</p>
<script type="math/tex; mode=display">
F(x, y)=P\{(X \leqslant x) \cap(Y \leqslant y)\}=P\{X \leqslant x, Y \leqslant y\}</script><p>称为二维随机变量$(X, Y)$的<strong>分布函数</strong>，或称为随机变量$X$和$Y$的<strong>联合分布函数</strong></p>
<p>分布函数$F(x, y)$具有以下的基本性质：<br>1，$F(x, y)$是变量$x$和$y$的不减函数<br>2，$0 \leqslant F(x, y) \leqslant 1$<br>对于任意固定的$y$，$F(-\infty, y)=0$<br>对于任意固定的$x$，$F(x,-\infty)=0$<br>$F(-\infty,-\infty)=0, F(\infty, \infty)=1$<br>3，$F(x+0, y)=F(x, y), F(x, y+0)=F(x, y)$，即$F(x, y)$关于$x$右连续，关于$y$也右连续</p>
<hr>
<p>如果二维随机变量$(X, Y)$全部可能取到的值是有限对或可列无限多对，则称$(X, Y)$是<strong>离散型的随机变量</strong><br>设二维离散型随机变量$(X, Y)$所有可能取的值为$\left(x<em>{i}, y</em>{j}\right), i, j=1,2, \cdots$，记$P\left{X=x<em>{i}, Y=y</em>{j}\right}=p_{i j}, i, j=1,2, \cdots$，则由概率的定义有</p>
<script type="math/tex; mode=display">
p_{i j} \geqslant 0, \quad \sum_{i=1}^{\infty} \sum_{j=1}^{\infty} p_{i j}=1</script><p>我们称$P\left{X=x<em>{i}, Y=y</em>{j}\right}=p_{i j}, i, j=1,2, \cdots$为二维离散型随机变量$(X, Y)$的<strong>分布律</strong>，或随机变量$X$和$Y$的<strong>联合分布律</strong></p>
<p>将$(X, Y)$看成一个随机点的坐标，离散型随机变量$X$和$Y$的联合分布函数为</p>
<script type="math/tex; mode=display">
F(x, y)=\sum_{x_{i} \leqslant x} \sum_{y_{j} \leqslant y}p_{i j}</script><p>其中和式是对一切满足$x<em>{i} \leqslant x, y</em>{j} \leqslant y$的$i, j$来求和的</p>
<hr>
<p>与一维随机变量相似，对于二维随机变量$(X, Y)$的分布函数$F(x, y)$，如果存在非负可积函数$f(x, y)$使对于任意$x, y$有</p>
<script type="math/tex; mode=display">
F(x, y)=\int_{-\infty}^{y} \int_{-\infty}^{x} f(u, v) \mathrm{d} u \mathrm{d} v</script><p>则称$(X, Y)$是<strong>连续型的二维随机变量</strong>，函数$f(x, y)$称为二维随机变量$(X, Y)$的<strong>概率密度</strong>，或称为随机变量$X$和$Y$的<strong>联合概率密度</strong><br>按定义，概率密度$f(x, y)$具有以下性质：<br>1，$f(x, y) \geqslant 0$<br>2，$\int<em>{-\infty}^{\infty} \int</em>{-\infty}^{\infty} f(x, y) \mathrm{d} x \mathrm{d} y=F(\infty, \infty)=1$<br>3，设$G$是$x O y$平面上的区域，点$f(x, y)$落在$G$内的概率为</p>
<script type="math/tex; mode=display">
P\{(X, Y) \in G\}=\iint_{G} f(x, y) \mathrm{d} x \mathrm{d} y</script><p>4，若$f(x, y)$在$(x, y)$连续，则有</p>
<script type="math/tex; mode=display">
\frac{\partial^{2} F(x, y)}{\partial x \partial y}=f(x, y)</script><p><strong>n维随机变量</strong>：设$E$是一个随机试验，它的样本空间是$S={e}$，设$X<em>{1}=X</em>{1}(e), X<em>{2}=X</em>{2}(e), \cdots, X<em>{n}=X</em>{n}(e)$是定义在$S$上的随机变量，由它们构成的一个n维向量$\left(X<em>{1}, X</em>{2}, \cdots, X_{n}\right)$叫做n维随机变量。</p>
<p>对于任意n个实数$x<em>{1}, x</em>{2}, \cdots, x_{n}$，n元函数</p>
<script type="math/tex; mode=display">
F\left(x_{1}, x_{2}, \cdots, x_{n}\right)=P\left\{X_{1} \leqslant x_{1}, X_{2} \leqslant x_{2}, \cdots, X_{n} \leqslant x_{n}\right\}</script><p>称为n维随机变量$\left(X<em>{1}, X</em>{2}, \cdots, X<em>{n}\right)$的<strong>分布函数</strong>或随机变量$X</em>{1}, X<em>{2}, \cdots, X</em>{n}$的<strong>联合分布函数</strong></p>
<h2 id="2-边缘分布"><a href="#2-边缘分布" class="headerlink" title="2.边缘分布"></a>2.边缘分布</h2><p>二维随机变量$(X, Y)$作为一个整体，具有分布函数$F(x, y)$，而$X$和$Y$都是随机变量，各自也有分布函数，将它们分别记为$F<em>{X}(x), F</em>{Y}(y)$，依次称为二维随机变量$(X, Y)$关于$X$和关于$Y$的边缘分布函数。边缘分布函数可以由$(X, Y)$的分布函数$F(x, y)$所确定，事实上有</p>
<script type="math/tex; mode=display">
F_{X}(x)=F(x, \infty)</script><script type="math/tex; mode=display">
F_{Y}(y)=F(\infty, y)</script><hr>
<p>对于离散型随机变量：</p>
<script type="math/tex; mode=display">
F_{X}(x)=F(x, \infty)=\sum_{x_{i} \leqslant x} \sum_{j=1}^{\infty} p_{i j}</script><p>$X$的分布律</p>
<script type="math/tex; mode=display">
P\left\{X=x_{i}\right\}=\sum_{j=1}^{\infty} p_{i j}, \quad i=1,2, \cdots</script><p>$Y$的分布律</p>
<script type="math/tex; mode=display">
P\left\{Y=y_{j}\right\}=\sum_{i=1}^{\infty} p_{i j}, \quad j=1,2, \cdots</script><p>记</p>
<script type="math/tex; mode=display">
\begin{aligned}
p_{i} . &=\sum_{j=1}^{\infty} p_{i j}=P\left\{X=x_{i}\right\}, \quad i=1,2, \cdots \\
p . _{j} &=\sum_{i=1}^{\infty} p_{i j}=P\left\{Y=y_{j}\right\}, \quad j=1,2, \cdots
\end{aligned}</script><p>分别称$p<em>{i} .(i=1,2, \cdots)$和$p .</em>{j}(j=1,2, \cdots)$为$(X, Y)$关于$X$和关于$Y$的边缘分布律</p>
<hr>
<p>对于连续型随机变量$(X, Y)$，设它的概率密度为$f(x, y)$，有</p>
<script type="math/tex; mode=display">
F_{X}(x)=F(x, \infty)=\int_{-\infty}^{x}\left[\int_{-\infty}^{\infty} f(x, y) \mathrm{d} y\right] \mathrm{d} x</script><p>$X$是一个连续型随机变量，其概率密度为</p>
<script type="math/tex; mode=display">
f_{X}(x)=\int_{-\infty}^{\infty} f(x, y) \mathrm{d} y</script><p>$Y$是一个连续型随机变量，其概率密度为</p>
<script type="math/tex; mode=display">
f_{Y}(y)=\int_{-\infty}^{\infty} f(x, y) \mathrm{d} x</script><p>分别称$f<em>{X}(x), f</em>{Y}(y)$为$(X, Y)$关于$X$和关于$Y$的<strong>边缘概率密度</strong></p>
<h2 id="3-条件分布"><a href="#3-条件分布" class="headerlink" title="3.条件分布"></a>3.条件分布</h2><p><strong>定义</strong> 设$(X, Y)$是二维离散型随机变量，对于固定的$j$，若$P\left{Y=y_{j}\right}&gt;0$，则称</p>
<script type="math/tex; mode=display">
P\left\{X=x_{i} \mid Y=y_{j}\right\}=\frac{P\left\{X=x_{i}, Y=y_{j}\right\}}{P\left\{Y=y_{i}\right\}}=\frac{p_{i j}}{p._{j}}, i=1,2, \cdots</script><p>为在$Y=y_{j}$条件下随机变量$X$的<strong>条件分布律</strong></p>
<p>同样，对于固定的$i$，若$P\left{X=x_{i}\right}&gt;0$，则称</p>
<script type="math/tex; mode=display">
P\left\{Y=y_{j} \mid X=x_{i}\right\}=\frac{P\left\{X=x_{i}, Y=y_{j}\right\}}{P\left\{X=x_{i}\right\}}=\frac{p_{i j}}{p_{i}.}, j=1,2, \cdots</script><p>为在$X=x_{i}$条件下随机变量$Y$的<strong>条件分布律</strong></p>
<hr>
<p><strong>定义</strong> 设二维随机变量$(X, Y)$的概率密度为$f(x, y)$，$(X, Y)$关于$Y$的边缘概率密度为$f<em>{Y}(y)$。若对于固定的y，$f</em>{Y}(y)&gt;0$，则称$\frac{f(x, y)}{f_{Y}(y)}$为在$Y=y$的条件下$X$的<strong>条件概率密度</strong>，记为</p>
<script type="math/tex; mode=display">
f_{X \mid Y}(x \mid y)=\frac{f(x, y)}{f_{Y}(y)}</script><p>称</p>
<script type="math/tex; mode=display">
F_{X \mid Y}(x \mid y)=P\{X \leqslant x \mid Y=y\}=\int_{-\infty}^{x} \frac{f(x, y)}{f_{Y}(y)} \mathrm{d} x</script><p>为在$Y=y$的条件下$X$的条件分布函数</p>
<p>类似地，可以定义</p>
<script type="math/tex; mode=display">
f_{Y \mid X}(y \mid x)=\frac{f(x, y)}{f_{X}(x)}</script><script type="math/tex; mode=display">
F_{Y \mid X}(y \mid x)=\int_{-\infty}^{y} \frac{f(x, y)}{f_{X}(x)} \mathrm{d} y</script><h2 id="4-相互独立的随机变量"><a href="#4-相互独立的随机变量" class="headerlink" title="4.相互独立的随机变量"></a>4.相互独立的随机变量</h2><p>设$F(x, y)$及$F<em>{X}(x), F</em>{Y}(y)$分别是二维随机变量$(X, Y)$的分布函数及边缘分布函数，若对于所有$x, y$有</p>
<script type="math/tex; mode=display">
P\{X \leqslant x, Y \leqslant y\}=P\{X \leqslant x\} P\{Y \leqslant y\}</script><p>即</p>
<script type="math/tex; mode=display">
F(x, y)=F_{X}(x) F_{Y}(y)</script><p>则称随机变量$X$和$Y$是<strong>相互独立</strong>的</p>
<p>设$(X, Y)$是连续型随机变量，$f(x, y), f<em>{X}(x), f</em>{Y}(y)$分别是$(X, Y)$的概率密度和边缘概率密度，则$X$和$Y$相互独立的条件等价于：等式</p>
<script type="math/tex; mode=display">
f(x, y)=f_{X}(x) f_{Y}(y)</script><p>在平面上几乎处处成立</p>
<p>当$(X, Y)$是离散型随机变量时，$X$和$Y$相互独立的条件等价于，对于$(X, Y)$的所有可能取的值$\left(x<em>{i}, y</em>{j}\right)$有</p>
<script type="math/tex; mode=display">
P\left\{X=x_{i}, Y=y_{j}\right\}=P\left\{X=x_{i}\right\} P\left\{Y=y_{j}\right\}</script><h2 id="5-两个随机变量的函数的分布"><a href="#5-两个随机变量的函数的分布" class="headerlink" title="5.两个随机变量的函数的分布"></a>5.两个随机变量的函数的分布</h2><p>（一）$Z=X+Y$的分布</p>
<p>若$X$和$Y$相互独立，设$(X, Y)$关于$X$，$Y$的边缘密度分别为$f<em>{X}(x)$，$f</em>{Y}(y)$，<br>有<strong>卷积公式</strong></p>
<script type="math/tex; mode=display">
f_{X+Y}(z)=\int_{-\infty}^{\infty} f_{X}(z-y) f_{Y}(y) \mathrm{d} y</script><script type="math/tex; mode=display">
f_{X+Y}(z)=\int_{-\infty}^{\infty} f_{X}(x) f_{Y}(z-x) \mathrm{d} x</script><p>卷积公式的重要应用<br><strong>相互独立正态随机变量之和的分布</strong><br>n个独立正态随机变量之和的情况，若$X<em>{i} \sim N\left(\mu</em>{i}, \sigma<em>{i}^{2}\right)(i=1,2, \cdots, n)$，且它们相互独立，则它们的和$Z=X</em>{1}+X<em>{2}+\cdots+X</em>{n}$仍然服从正态分布，且有$Z \sim N\left(\mu<em>{1}+\mu</em>{2}+\cdots+\mu<em>{n}, \sigma</em>{1}^{2}+\sigma<em>{2}^{2}+\cdots+\sigma</em>{n}^{2}\right)$。<br>更一般地，可以证明<strong>有限个相互独立的正态随机变量的线性组合仍然服从正态分布</strong></p>
<p>（二）$Z=\frac{Y}{X}$的分布、$Z=X Y$的分布</p>
<p>（三）$M=\max {X, Y}$及$N=\min {X, Y}$的分布</p>
<p>设$X$，$Y$是两个相互独立的随机变量，它们的分布函数分别是$F<em>{X}(x)$和$F</em>{Y}(y)$，现在来求$M=\max {X, Y}$和$N=\min {X, Y}$的分布函数<br>由于$M=\max {X, Y}$不大于$z$等价于$X$和$Y$都不大于$z$，故有</p>
<script type="math/tex; mode=display">
P\{M \leqslant z\}=P\{X \leqslant z, Y \leqslant z\}</script><p>又由于$X$和$Y$相互独立，得到$M=\max {X, Y}$的分布函数为</p>
<script type="math/tex; mode=display">
F_{\max }(z)=P\{M \leqslant z\}=P\{X \leqslant z, Y \leqslant z\}=P\{X \leqslant z\} P\{Y \leqslant z\}</script><p>既有</p>
<script type="math/tex; mode=display">
F_{\max }(z)=F_{X}(z) F_{Y}(z)</script><p>类似地，可得$N=\min {X, Y}$的分布函数为<br>$F_{\min }(z)=P{N \leqslant z}=1-P{N&gt;z}$<br>$=1-P{X&gt;z, Y&gt;z}=1-P{X&gt;z} \cdot P{Y&gt;z}$<br>即</p>
<script type="math/tex; mode=display">
F_{\min }(z)=1-\left[1-F_{X}(z)\right]\left[1-F_{Y}(z)\right]</script><p>以上结果容易推广到n个相互独立的随机变量的情况，设$X<em>{1}, X</em>{2}, \cdots, X<em>{n}$是n个相互独立的随机变量。它们的分布函数分别是$F</em>{X<em>{i}}\left(x</em>{i}\right)(i=1,2, \cdots, n)$，则$M=\max \left{X<em>{1}, X</em>{2}, \cdots, X<em>{n}\right}$及$N=\min \left{X</em>{1}, X<em>{2}, \cdots, X</em>{n}\right}$的分布函数分别是</p>
<script type="math/tex; mode=display">
F_{\max }(z)=F_{X_{1}}(z) F_{X_{2}}(z) \cdots F_{X_{n}}(z)</script><script type="math/tex; mode=display">
F_{\min }(z)=1-\left[1-F_{X_{1}}(z)\right]\left[1-F_{X_{2}}(z)\right] \cdots\left[1-F_{X_{n}}(z)\right]</script>]]></content>
      <categories>
        <category>概率论</category>
      </categories>
      <tags>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（14）：奇异值分解（Singular Value Decomposition）</title>
    <url>/2020/02/23/linear_algebra_14/</url>
    <content><![CDATA[<p>清华大学线性代数（2）课程第三讲：奇异值分解</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<h1 id="问题：如何“对角化”-m-times-n-矩阵？"><a href="#问题：如何“对角化”-m-times-n-矩阵？" class="headerlink" title="问题：如何“对角化”$m \times n$矩阵？"></a>问题：如何“对角化”$m \times n$矩阵？</h1><h2 id="1-奇异值分解（Singular-Value-Decomposition）"><a href="#1-奇异值分解（Singular-Value-Decomposition）" class="headerlink" title="1.奇异值分解（Singular Value Decomposition）"></a>1.奇异值分解（Singular Value Decomposition）</h2><p>设$A$是一个$m \times n$矩阵，则存在$m$阶正交矩阵$U$和$n$阶正交矩阵$V$，满足</p>
<script type="math/tex; mode=display">
A=U\left(\begin{array}{cccc}
{\sigma_{1}} & {} & {} & {} \\
{} & {\ddots} & {} & {} \\
{} & {} & {\sigma_{r}} & {} \\
{} & {} & {} & {0}
\end{array}\right) V^{T}=: U \Sigma V^{T}</script><p>其中$r=\operatorname{rank} A$。习惯上，设$\sigma<em>{1} \geq \sigma</em>{2} \geq \cdots \geq \sigma<em>{r}&gt;0$。称$\sigma</em>{1}, \cdots, \sigma_{r}$为<strong>奇异值（singular value）</strong>。称$U$和$V$的前r列向量为<strong>奇异向量（singular vector）</strong>。这个分解为<strong>奇异值分解</strong>，简称$SVD$，它是线性代数中最重要的一类分解。</p>
<p>其中矩阵$U$和矩阵$V$用列向量表示为：</p>
<script type="math/tex; mode=display">
U=\left(\mathbf{u}_{1} \cdots \mathbf{u}_{r} \mathbf{u}_{r+1} \cdots \mathbf{u}_{m}\right), V=\left(\mathbf{v}_{1} \cdots \mathbf{v}_{r} \mathbf{v}_{r+1} \cdots \mathbf{v}_{n}\right)</script><p>因为$U$和$V$都是正交矩阵，所以有</p>
<script type="math/tex; mode=display">
A_{m \times n} V_{n \times n}=U_{m \times m} \Sigma_{m \times n}</script><p>其中$V^{T} V=I<em>{n}, U^{T} U=I</em>{m}$，也可以把奇异值分解描述为$r$个秩一矩阵之和</p>
<script type="math/tex; mode=display">
\begin{aligned}
&A=U \Sigma V^{T},(SVD)\\
&A=\sigma_{1} \mathbf{u}_{1} \mathbf{v}_{1}^{T}+\cdots+\sigma_{r} \mathbf{u}_{r} \mathbf{v}_{r}^{T}
\end{aligned}</script><p>还有</p>
<script type="math/tex; mode=display">
\begin{aligned}
A \mathbf{v}_{i} &=\sigma_{i} \mathbf{u}_{i}\quad(i=1, \cdots, r) \\
A^{T} \mathbf{u}_{i} &=\sigma_{i} \mathbf{v}_{i}\quad(i=1, \cdots, r) \\
A \mathbf{v}_{j}=& \mathbf{0}\quad(j=r+1, \cdots, n) \\
A^{T} \mathbf{u}_{k} &=\mathbf{0}\quad(k=r+1, \cdots, m)
\end{aligned}</script><p>有</p>
<script type="math/tex; mode=display">
\Rightarrow A^{T} A \mathbf{v}_{i}=\sigma_{i}^{2} \mathbf{v}_{i}, \quad A A^{T} \mathbf{u}_{i}=\sigma_{i}^{2} \mathbf{u}_{i} \quad(1 \leq i \leq r)</script><p>$\mathbf{v}<em>{i}$是矩阵$A^{T} A$对应与特征值$\sigma</em>{i}^{2}$的特征向量，$\mathbf{u}<em>{i}$是矩阵$A A^{T}$对应与特征值$\sigma</em>{i}^{2}$的特征向量。</p>
<h3 id="矩阵-A-A-T-与-A-T-A-的特征值和特征向量的性质"><a href="#矩阵-A-A-T-与-A-T-A-的特征值和特征向量的性质" class="headerlink" title="矩阵$A A^{T}$与$A^{T} A$的特征值和特征向量的性质"></a>矩阵$A A^{T}$与$A^{T} A$的特征值和特征向量的性质</h3><p>设$A$是秩为$r$的$m \times n$实矩阵，则$A A^{T}$为$m$阶实对称矩阵，$A^{T} A$为$n$阶实对称矩阵<br>（1）$A A^{T}$与$A^{T} A$的特征值为非负数<br>证明：设$A^{T} A \mathbf{x}=\lambda \mathbf{x}(\mathbf{x} \neq \mathbf{0})$，则$\mathbf{x}^{T} A^{T} A \mathbf{x}=\lambda \mathbf{x}^{T} \mathbf{x}$，即</p>
<script type="math/tex; mode=display">
\|A \mathbf{x}\|^{2}=\lambda\|\mathbf{x}\|^{2}</script><p>故$\lambda \geq 0$。同理，$A A^{T}$的特征值也全为非负数。</p>
<p>（2）$A A^{T}$与$A^{T} A$的非零特征值集合相同<br>证明：因为$r\left(A A^{T}\right)=r\left(A^{T}\right)=r(A)=r\left(A^{T} A\right)=r$。$A^{T} A$与对角矩阵相似，相似矩阵秩相同。对角矩阵的秩等于非零特征值数量。故$A A^{T}$的非零特征值数量等于$A^{T} A$的非零特征值数量，等于他们的秩。<br>设$\lambda$是$A^{T} A$的非零特征值，则存在非零向量$\mathbf{x}$，使得$A^{T} A \mathbf{x}=\lambda \mathbf{x}$。则有$A A^{T} A \mathbf{x}=\lambda A \mathbf{x}$。其中$A \mathbf{x}不等于零向量，$故$\lambda$也是$A A^{T}$的非零特征值。反之亦然。因此$A A^{T}$与$A^{T} A$具有相同的非零特征值。</p>
<p>（3）不妨设$A A^{T}$和$A^{T} A$的这$r$个非零特征值为$\sigma<em>{1}^{2} \geq \cdots \geq \sigma</em>{r}^{2}&gt;0$，其中$\sigma<em>{i}&gt;0$。<br>设$\mathbf{v}</em>{1}, \cdots, \mathbf{v}_{n} \in \mathbb{R}^{n}$为$n$阶实对称方阵$A^{T} A$的单位正交特征向量</p>
<script type="math/tex; mode=display">
A^{T} A\left(\mathbf{v}_{1} \cdots \mathbf{v}_{n}\right)=\left(\mathbf{v}_{1} \cdots \mathbf{v}_{n}\right)\left(\begin{array}{cccc}
{\sigma_{1}^{2}} & {} & {} & {} \\
{} & {\ddots} & {} & {} \\
{} & {} & {\sigma_{r}^{2}} \\
{} & {} & {} & {0}
\end{array}\right)</script><p>记$V=\left(\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{n}\right)$，因为$V$为正交矩阵，有$V^{T} V=I<em>{n}$。<br>注意到$A^{T} A \mathbf{v}</em>{i}=\sigma<em>{i}^{2} \mathbf{v}</em>{i} \quad(1 \leq i \leq r)$<br>故$\mathbf{v}<em>{i}^{T} A^{T} A \mathbf{v}</em>{i}=\sigma<em>{i}^{2} \mathbf{v}</em>{i}^{T} \mathbf{v}<em>{i}$，即$\left|A \mathbf{v}</em>{i}\right|^{2}=\sigma<em>{i}^{2}$。<br>令$\mathbf{u}</em>{i}:=\frac{A \mathbf{v}<em>{i}}{\sigma</em>{i}} \in \mathbb{R}^{m}(1 \leq i \leq r)$，则$A A^{T} \mathbf{u}<em>{i}=\sigma</em>{i}^{2} \mathbf{u}_{i}$，并且</p>
<script type="math/tex; mode=display">
\mathbf{u}_{i}^{T} \mathbf{u}_{j}=\frac{\left(A \mathbf{v}_{i}\right)^{T}}{\sigma_{i}} \frac{A \mathbf{v}_{j}}{\sigma_{j}}=\frac{\mathbf{v}_{i}^{T}\left(A^{T} A \mathbf{v}_{j}\right)}{\sigma_{i} \sigma_{j}}=\frac{\sigma_{j}^{2} \mathbf{v}_{i}^{T} \mathbf{v}_{j}}{\sigma_{i} \sigma_{j}}=\frac{\sigma_{j}}{\sigma_{i}} \delta_{i j}=\delta_{i j}</script><p>故$\left{\mathbf{u}_{i} | 1 \leq i \leq r\right}$是$A A^{T}$的单位正交特征向量。<br>又有</p>
<script type="math/tex; mode=display">
A \mathbf{v}_{i}=\sigma_{i} \mathbf{u}_{i}, \quad A^{T} \mathbf{u}_{i}=\sigma_{i} \mathbf{v}_{i} \quad 1 \leq i \leq r</script><p>$\mathbf{u}<em>{i}$在矩阵$A$的列空间里，$\mathbf{v}</em>{i}$在矩阵$A$的行空间里，故<br>$\left{\mathbf{u}<em>{1}, \cdots, \mathbf{u}</em>{r}\right}$为$C(A)$的一组单位正交基<br>$\left{\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{r}\right}$为$C(A^{T})$的一组单位正交基</p>
<h2 id="奇异值分解的几何意义"><a href="#奇异值分解的几何意义" class="headerlink" title="奇异值分解的几何意义"></a>奇异值分解的几何意义</h2><p>一般地，秩为$r$的$m \times n$矩阵$A$有SVD：$A_{m \times n}=U \Sigma V^{T}$，则从$\mathbb{R}^{n}$到$\mathbb{R}^{m}$的线性变换$\mathbf{x} \mapsto A \mathbf{x}$可以看成是以下三步的复合：<br>（1）$\mathbb{R}^{n}$中的旋转$\mathbf{x} \mapsto V^{T} \mathbf{x}$<br>（2）$\mathbb{R}^{n}$中的向量$V^{T} \mathbf{x}$的前$r$个分量做伸缩，其余分量变为0：</p>
<script type="math/tex; mode=display">
V^{T} \mathbf{x} \mapsto \Sigma V^{T} \mathbf{x}</script><p>（3）再在$\mathbb{R}^{m}$中做旋转</p>
<script type="math/tex; mode=display">
\Sigma V^{T} \mathbf{x} \mapsto U \Sigma V^{T} \mathbf{x}</script><h2 id="SVD与矩阵的四个基本子空间"><a href="#SVD与矩阵的四个基本子空间" class="headerlink" title="SVD与矩阵的四个基本子空间"></a>SVD与矩阵的四个基本子空间</h2><p>设$A=U \Sigma V^{T}$是$m \times n$实矩阵$A$的奇异值分解，$r=r(A)$，则<br>$\ast$正交矩阵$U$的前$r$列是$C\left(A \right)$的一组标准正交基<br>$\ast$正交矩阵$U$的后$m-r$列是$N\left(A^{T} \right)$的一组标准正交基<br>$\ast$正交矩阵$V$的前$r$列是$C\left(A^{T} \right)$的一组标准正交基<br>$\ast$正交矩阵$V$的后$n-r$列是$N\left(A \right)$的一组标准正交基</p>
<h2 id="SVD与图像压缩"><a href="#SVD与图像压缩" class="headerlink" title="SVD与图像压缩"></a>SVD与图像压缩</h2><p>设秩$r$的$m \times n$矩阵$A$的奇异值分解为</p>
<script type="math/tex; mode=display">
A=U \Sigma V^{T}=\sigma_{1} \mathbf{u}_{1} \mathbf{v}_{1}^{T}+\cdots+\sigma_{r} \mathbf{u}_{r} \mathbf{v}_{r}^{T}</script><p>其中$\sigma<em>{1} \geq \sigma</em>{2} \geq \cdots \geq \sigma<em>{r}&gt;0$<br>令$A</em>{k}=\sigma<em>{1} \mathbf{u}</em>{1} \mathbf{v}<em>{1}^{T}+\cdots+\sigma</em>{k} \mathbf{u}<em>{k} \mathbf{v}</em>{k}^{T} \quad(1 \leq k&lt;r)$<br>称为$A$的<strong>$k$阶逼近</strong>。特别地，$k=1$时，$A_{1}$是1阶逼近。</p>
<p>例如：一幅规格为$m \times n$像素的照片可用一个$m \times n$矩阵来存储。利用矩阵的奇异值分解，只需存储矩阵的奇异值$\sigma<em>{i}$，奇异向量$\mathbf{u}</em>{i}, \mathbf{v}<em>{i}$的分量，总计$r \cdot(m+n+1)$个数据，而不是原始的$m \times n$个数据。通常$r \ll m, r \ll n$，则$r \cdot(m+n+1) \ll m \cdot n$。比值$\frac{m \cdot n}{r \cdot(m+n+1)}$称为图像的压缩比（其倒数称为数据压缩率）<br>若$\sigma</em>{1}, \cdots \sigma<em>{k}$远大于$\sigma</em>{k+1}, \cdots \sigma<em>{r}$，则$A</em>{k} \approx A$图像不失真且压缩了存储量。对于较大的$k$，可以获得保真度较高的还原数据。而较小的$k$，可以获得较高的传输效率。</p>
<h2 id="SVD与特征值"><a href="#SVD与特征值" class="headerlink" title="SVD与特征值"></a>SVD与特征值</h2><p>命题：设$|\lambda|_{\max }$是矩阵$A$的特征值的模长的最大值，则</p>
<script type="math/tex; mode=display">
\sigma_{1} \geq|\lambda|_{\max }, \sigma_{1} \geq\left|a_{i j}\right|, \quad \forall i, j</script><p>证明：设$A$有奇异值分解$A=U \Sigma V^{T}$，则对任意向量$\mathbf{x}$，有</p>
<script type="math/tex; mode=display">
\|A \mathbf{x}\|=\left\|U \Sigma V^{T} \mathbf{x}\right\|=\left\|\Sigma V^{T} \mathbf{x}\right\| \leq \sigma_{1}\left\|V^{T} \mathbf{x}\right\|=\sigma_{1}\|\mathbf{x}\|</script><p>特别地，若$A \mathbf{x}=\lambda \mathbf{x}$，其中$\mathbf{x}$为对应于$\lambda$的特征向量。则$|A \mathbf{x}|=|\lambda| \cdot|\mathbf{x}|$，故$\sigma<em>{1} \geq|\lambda|$，特别有$\sigma</em>{1} \geq|\lambda|<em>{\max }$。<br>又若取$\mathbf{x}=(1,0, \cdots, 0)$，则$A \mathbf{x}$表示$A$的第一列向量，且$|A \mathbf{x}| \leq \sigma</em>{1}|\mathbf{x}|=\sigma_{1}$，而</p>
<script type="math/tex; mode=display">
\left|a_{i 1}\right| \leq \sqrt{a_{11}^{2}+\cdots+a_{n 1}^{2}} \leq \sigma_{1}</script><p>同理，任何一列的某一个分量都有$\left|a<em>{i j}\right| \leq \sigma</em>{1}$，即矩阵的任意一个元素的绝对值都小于等于矩阵$A$的最大的奇异值。</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>勒让德多项式（legendre polynomials）</title>
    <url>/2020/02/04/legendre_polynomial/</url>
    <content><![CDATA[<p>参考资料：<br><a href="https://ccjou.wordpress.com/專題探究/傅立葉分析專題/">线代启示录：傅立叶分析专题</a></p>
<span id="more"></span>
<p>本节运用Gram-Schmidt正交化程序推导<strong>实多项式空间</strong>的<strong>一组正交基底</strong>  <strong>Legendre多项式</strong>。<br>令$\mathcal{P}$为定义于区间$[a, b]$的连续实函数所构成的内积空间，设$f$和$g$属于$\mathcal{P}$，我们定义$f$和$g$的内积如下：</p>
<script type="math/tex; mode=display">
\langle f, g\rangle \stackrel{\text { def }}{=} \int_{a}^{b} f(x) g(x) d x</script><p>在某些情况下，若无法取得区间$[a, b]$中完整的连续函数值，可使用离散运算逼近：</p>
<script type="math/tex; mode=display">
\langle f, g\rangle=\sum_{i} f\left(x_{i}\right) g\left(x_{i}\right)</script><p>若$\langle f, g\rangle= 0$，我们称$f$和$g$正交。</p>
<p>令$\mathcal{P}<em>{n}$表示定义于区间$[-1,1]$的$n$次实多项式形成的函数空间，对于$f, g \in \mathcal{P}</em>{n}$，定义其内积为</p>
<script type="math/tex; mode=display">
\langle f, g\rangle=\int_{-1}^{1} f(x) g(x) d x</script><p>运用Gram-Schmidt正交化可获得$\mathcal{P}<em>{n}$的一组正交基底，表示为$\left{p</em>{0}(x), p<em>{1}(x), \ldots, p</em>{n}(x)\right}$，其中$p<em>{k}(x)$为$k$次多项式，且当$i \neq j, \quad\left\langle p</em>{i}, p_{j}\right\rangle= 0$</p>
<p>下面给出推导过程，针对$\mathcal{P}<em>{n}=\operatorname{span}\left{1, x, x^{2}, \ldots, x^{n}\right}$，先令$p</em>{0}(x)=1$。在区间$[-1,1]$，1正交于$x$，立即得$p<em>{1}(x)=x$。再将$x^{2}$投影至$p</em>{0}(x), p_{1}(x)$的分量扣除：</p>
<script type="math/tex; mode=display">
x^{2}-\frac{\left\langle x^{2}, 1\right\rangle}{\langle 1,1\rangle} 1-\frac{\left\langle x^{2}, x\right\rangle}{\langle x, x\rangle} x=x^{2}-\frac{2 / 3}{2} 1-\frac{0}{2 / 3} x=x^{2}-\frac{1}{3}</script><p>因为投影残量同时正交$p<em>{0}(x), p</em>{1}(x)$，故令$p<em>{2}(x)=x^{2}-\frac{1}{3}$。同样地，继续将$x^{3}$投影至$p</em>{0}(x), p<em>{1}(x), p</em>{2}(x)$的分量扣除：</p>
<script type="math/tex; mode=display">
x^{3}-\frac{\left\langle x^{3}, 1\right\rangle}{\langle 1,1\rangle} 1-\frac{\left\langle x^{3}, x\right\rangle}{\langle x, x\rangle} x-\frac{\left\langle x^{3}, x^{2}-\frac{1}{3}\right\rangle}{\left\langle x^{2}-\frac{1}{3}, x^{2}-\frac{1}{3}\right\rangle}\left(x^{2}-\frac{1}{3}\right)=x^{3}-\frac{3}{5} x</script><p>也就得到$p<em>{3}(x)=x^{3}-\frac{3}{5} x$。重复上述步骤即可导出$\mathcal{P}</em>{n}$的一组完整正交基底，以下是前几个多项式：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
{p_{0}(x)=1} \\
{p_{1}(x)=x} \\
{p_{2}(x)=x^{2}-\frac{1}{3}} \\
{p_{3}(x)=x^{3}-\frac{3}{5} x} \\
{p_{4}(x)=x^{4}-\frac{6}{7} x^{2}+\frac{3}{35}} \\
{p_{5}(x)=x^{5}-\frac{10}{9} x^{3}+\frac{5}{21} x} \\
{\vdots}
\end{array}</script><p>如果我们对多项式正规化使得$p_{k}(1)=1, \quad k=0,1,2, \ldots$，下面给出前几个正规化多项式：</p>
<script type="math/tex; mode=display">
\begin{array}{l}
{p_{0}(x)=1} \\
{p_{1}(x)=x} \\
{p_{2}(x)=\frac{1}{2}\left(3 x^{2}-1\right)} \\
{p_{3}(x)=\frac{1}{2}\left(5 x^{3}-3 x\right)} \\
{p_{4}(x)=\frac{1}{8}\left(35 x^{4}-30 x^{2}+3\right)} \\
{p_{5}(x)=\frac{1}{8}\left(63 x^{5}-70 x^{3}+15 x\right)} \\
{\vdots}
\end{array}</script><p>满足：</p>
<script type="math/tex; mode=display">
\left\langle p_{k}, p_{k}\right\rangle=\frac{2}{2 k+1}</script>]]></content>
      <categories>
        <category>legendre polynomials</category>
      </categories>
      <tags>
        <tag>legendre polynomials</tag>
      </tags>
  </entry>
  <entry>
    <title>傅立叶分析专题：傅立叶级数和傅立叶变换</title>
    <url>/2020/02/04/fourier_series/</url>
    <content><![CDATA[<p>参考资料：<br><a href="https://ccjou.wordpress.com/專題探究/傅立葉分析專題/">线代启示录：傅立叶分析专题</a></p>
<span id="more"></span>
<h2 id="1-函数空间"><a href="#1-函数空间" class="headerlink" title="1. 函数空间"></a>1. 函数空间</h2><p>函数空间是一个希尔伯特（Hilbert）空间，即一个保有一般几何性质的无限维实向量空间。<br>函数也是向量，而仅包含有限长度的函数可以形成向量空间。<br>以$\langle f, g\rangle$表示函数$f$和$g$的内积：</p>
<script type="math/tex; mode=display">
\langle f, g\rangle=\int_{0}^{T} f(x) g(x) d x</script><p>函数长度可由内积求得：$|f|^{2}=\langle f, f\rangle$</p>
<p>函数空间最有名的例子为傅立叶级数（Fourier series），函数$f(x)$表示为正弦函数和余弦函数的展开式：</p>
<script type="math/tex; mode=display">
f(x)=a_{0}+a_{1} \cos x+b_{1} \sin x+a_{2} \cos 2 x+b_{2} \sin 2 x+\cdots</script><p>傅立叶级数的基底函数包含：</p>
<script type="math/tex; mode=display">
1, \cos x, \sin x, \cos 2 x, \sin 2 x, \cdots</script><h2 id="2-傅立叶级数"><a href="#2-傅立叶级数" class="headerlink" title="2. 傅立叶级数"></a>2. 傅立叶级数</h2><p>考虑一有限维内积空间$\mathcal{V}$，且$\operatorname{dim} \mathcal{V}=n$。任意$\mathbf{x}, \mathbf{y} \in \mathcal{V}$的内积记为$\langle\mathbf{x}, \mathbf{y}\rangle$。令$\boldsymbol{\beta}=\left{\mathbf{v}<em>{1}, \ldots, \mathbf{v}</em>{n}\right}$为$\mathcal{V}$的一组<strong>单位正交基底</strong>（orthonormal basis），也就是说$\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j}\right\rangle= 1$若$i=j$，$\left\langle\mathbf{v}<em>{i}, \mathbf{v}</em>{j}\right\rangle= 0$若$i \neq j$，则有</p>
<script type="math/tex; mode=display">
\begin{aligned}
\left\langle\mathbf{v}_{i}, \mathbf{x}\right\rangle &=\left\langle\mathbf{v}_{i}, c_{1} \mathbf{v}_{1}+\cdots+c_{n} \mathbf{v}_{n}\right\rangle \\
&= c_{1}\left\langle\mathbf{v}_{i}, \mathbf{v}_{1}\right\rangle+\cdots+ c_{n}\left\langle\mathbf{v}_{i}, \mathbf{v}_{n}\right\rangle \\
&= c_{i}\left\langle\mathbf{v}_{i}, \mathbf{v}_{i}\right\rangle= c_{i}
\end{aligned}</script><p>故$\mathbf{x}$有下列正交分解展开式：</p>
<script type="math/tex; mode=display">
\mathbf{x}=\left\langle\mathbf{v}_{1}, \mathbf{x}\right\rangle \mathbf{v}_{1}+\cdots+\left\langle\mathbf{v}_{n}, \mathbf{x}\right\rangle \mathbf{v}_{n}</script><p>其中 $\left\langle\mathbf{v}_i,\mathbf{x}\right\rangle\mathbf{v}_i $即为 $\mathbf{x}$ 至“超直线” $\mathrm{span}{\mathbf{v}_i}$ 的正交投影分量。</p>
<hr>
<h3 id="周期函数-f-x-可展开成傅立叶级数的条件："><a href="#周期函数-f-x-可展开成傅立叶级数的条件：" class="headerlink" title="周期函数$f(x)$可展开成傅立叶级数的条件："></a>周期函数$f(x)$可展开成傅立叶级数的条件：</h3><p><strong>收敛定理</strong>，<strong>狄利克雷（Dirichlet）充分条件</strong>：<br>（1）此函数必须是有界的（bounded），即对于任意$x$，$|f(x)|&lt;M$，$M$是一正实数<br>（2）在任意区间内，除了有限个不连续点，$f(x)$必须是连续函数<br>（3）在任意区间内，$f(x)$必须仅包含有限个极值<br>（4）在一周期内，$|f(x)|$的积分必须收敛</p>
<hr>
<h3 id="三角函数系的正交性："><a href="#三角函数系的正交性：" class="headerlink" title="三角函数系的正交性："></a>三角函数系的正交性：</h3><p>所谓三角函数系</p>
<script type="math/tex; mode=display">
1, \cos x, \sin x, \cos 2 x, \sin 2 x, \cdots, \cos n x, \sin n x, \cdots</script><p>在区间$[-\pi, \pi]$上<strong>正交</strong>，就是指在三角函数系中任何不同的两个函数的乘积$[-\pi, \pi]$上的积分等于零，即</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\int_{-\pi}^{\pi} \cos n x \mathrm{d} x=0 \quad(n=1,2,3, \cdots)\\
&\int_{-\pi}^{\pi} \sin n x \mathrm{d} x=0 \quad(n=1,2,3, \cdots)\\
&\int_{-\pi}^{\pi} \sin k x \cos n x \mathrm{d} x=0 \quad(k, n=1,2,3, \cdots)\\
&\int_{-\pi}^{\pi} \cos k x \cos n x \mathrm{d} x=0 \quad(k, n=1,2,3, \cdots, k \neq n)\\
&\int_{-\pi}^{\pi} \sin k x \sin n x \mathrm{d} x=0 \quad(k, n=1,2,3, \cdots, k \neq n)
\end{aligned}</script><p>以上等式，都可以通过计算定积分来验证。<br>在三角函数系中，两个相同函数的乘积在区间$[-\pi, \pi]$上的积分不等于零，即</p>
<script type="math/tex; mode=display">
\int_{-\pi}^{\pi} 1^{2} \mathrm{d} x=2 \pi, \int_{-\pi}^{\pi} \sin ^{2} n x \mathrm{d} x=\pi, \int_{-\pi}^{\pi} \cos ^{2} n x \mathrm{d} x=\pi \quad(n=1,2,3, \cdots)</script><hr>
<p>令$\mathcal{V}$是由定义于区间$[-\pi, \pi]$（亦可选择$[0,2 \pi]$）的实函数所成的内积空间，函数空间$\mathcal{V}$是一无限维空间。对于$\mathcal{V}$中两实函数$f(x)$和$g(x)$，其内积定义如下（有些学者采用不含$1 / \pi$的内积定义，如此一来，$1 / \sqrt{\pi}$则需乘入余弦和正弦函数）：</p>
<script type="math/tex; mode=display">
\langle f, g\rangle \stackrel{\text { def }}{=} \frac{1}{\pi} \int_{-\pi}^{\pi} f(x) g(x) d x</script><p>则$f(x)$的<strong>长度</strong>或<strong>范数（norm）</strong> 为</p>
<script type="math/tex; mode=display">
\|f\|=\langle f, f\rangle^{1 / 2}=\left(\frac{1}{\pi} \int_{-\pi}^{\pi}(f(x))^{2} d x\right)^{1 / 2}</script><p>考虑无穷函数集合</p>
<script type="math/tex; mode=display">
\boldsymbol{\beta}=\left\{\frac{1}{\sqrt{2}}, \cos x, \sin x, \cos (2 x), \sin (2 x), \ldots\right\}</script><p>有</p>
<script type="math/tex; mode=display">
\begin{array}{l}
{\frac{1}{\pi} \int_{-\pi}^{\pi}\left(\frac{1}{\sqrt{2}}\right)^{2} d x=1} \\
{\frac{1}{\pi} \int_{-\pi}^{\pi} \cos ^{2}(m x) d x=1, m=1,2, \ldots} \\
{\frac{1}{\pi} \int_{-\pi}^{\pi} \sin ^{2}(m x) d x=1, m=1,2, \ldots}
\end{array}</script><p>故$\boldsymbol{\beta}$是一个单位正交集。</p>
<hr>
<h3 id="周期为-2-pi-实函数-f-x-的傅立叶级数"><a href="#周期为-2-pi-实函数-f-x-的傅立叶级数" class="headerlink" title="周期为$2 \pi$实函数$f(x)$的傅立叶级数"></a>周期为$2 \pi$实函数$f(x)$的傅立叶级数</h3><p>周期为$2 \pi$实函数$f(x)$的傅立叶级数$F(x)$为余弦和正弦函数组成的无穷级数：</p>
<script type="math/tex; mode=display">
F(x)=\frac{a_{0}}{2}+\sum_{k=1}^{\infty} a_{k} \cos (k x)+\sum_{k=1}^{\infty} b_{k} \sin (k x)</script><p>其中傅立叶系数$a<em>{k}$和$b</em>{k}$的计算公式如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
&a_{k}=\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \cos (k x) d x, \quad k=0,1,2, \ldots\\
&b_{k}=\frac{1}{\pi} \int_{-\pi}^{\pi} f(x) \sin (k x) d x, \quad k=1,2, \ldots
\end{aligned}</script><p>若$f(x)$为奇函数，则$f(x) \cos (k x)$为奇函数，奇函数在关于原点对称区间上的积分为零，故$a<em>{k}=0, \quad k=0,1, 2\dots$。另一方面若$f(x)$为偶函数，则$f(x) \sin (k x)$为奇函数，有$b</em>{k}=0, \quad k=1,2, \dots$。</p>
<h3 id="周期为-T-实函数-f-x-的傅立叶级数"><a href="#周期为-T-实函数-f-x-的傅立叶级数" class="headerlink" title="周期为$T$实函数$f(x)$的傅立叶级数"></a>周期为$T$实函数$f(x)$的傅立叶级数</h3><p>周期为$T$，定义于区间$[-T / 2, T / 2]$的周期函数$f(t)$。利用变数变换$t / T=x /(2 \pi)$，可使区间$[-\pi, \pi]$变换至$[-T / 2, T / 2]$，将$x=2 \pi t / T$代入$F(x)$，即得到$f(t)$的傅立叶级数：</p>
<script type="math/tex; mode=display">
F(t)=\frac{a_{0}}{2}+\sum_{k=1}^{\infty} a_{k} \cos \left(\frac{2 \pi k t}{T}\right)+\sum_{k=1}^{\infty} b_{k} \sin \left(\frac{2 \pi k t}{T}\right)</script><p>将$d x=2 \pi d t / T$代入$f(x)$的傅立叶系数的积分公式，可得</p>
<script type="math/tex; mode=display">
\begin{aligned}
&a_{k}=\frac{2}{T} \int_{-T / 2}^{T / 2} f(t) \cos \left(\frac{2 \pi k t}{T}\right) d t, k=0,1,2, \ldots\\
&b_{k}=\frac{2}{T} \int_{-T / 2}^{T / 2} f(t) \sin \left(\frac{2 \pi k t}{T}\right) d t, k=1,2, \ldots
\end{aligned}</script><p>对于周期为$T$的函数$f(t)$，任何区间$\left[t<em>{0}, t</em>{0}+T\right]$皆可使用，如何选择$t<em>{0}$值取决于便利性和个人偏好，常见的设定有$t</em>{0}=0$或$t_{0}=-T / 2$</p>
<h3 id="指数傅立叶级数"><a href="#指数傅立叶级数" class="headerlink" title="指数傅立叶级数"></a>指数傅立叶级数</h3><p>利用欧拉公式$e^{i x}=\cos x+i \sin x$，我们可以写出更为精简的傅立叶级数表达式。</p>
<script type="math/tex; mode=display">
\cos x=\frac{e^{i x}+e^{-i x}}{2}, \sin x=\frac{e^{i x}-e^{-i x}}{2 i}</script><p>再将傅立叶级数$F(x)$中$\cos (k x)$和$\sin (k x)$的线形组合式改写如下：</p>
<script type="math/tex; mode=display">
\begin{aligned}
a_{k} \cos (k x)+b_{k} \sin (k x) &=a_{k}\left(\frac{e^{i k x}+e^{-i k x}}{2}\right)+b_{k}\left(\frac{e^{i k x}-e^{-i k x}}{2 i}\right) \\
&=\left(\frac{a_{k}-i b_{k}}{2}\right) e^{i k x}+\left(\frac{a_{k}+i b_{k}}{2}\right) e^{-i k x} \\
&=c_{k} e^{i k x}+c_{-k} e^{-i k x}, \quad k=1,2, \ldots
\end{aligned}</script><p>其中$e^{i k x}$和$e^{-i k x}$的系数分别为</p>
<script type="math/tex; mode=display">
c_{k}=\frac{a_{k}-i b_{k}}{2}, \quad c_{-k}=\frac{a_{k}+i b_{k}}{2}</script><p>若$k=0$，就有$c<em>{0}=a</em>{0} / 2$。将以上结果代回$2 \pi$周期函数$f(x)$的傅立叶级数即得指数傅立叶级数：</p>
<script type="math/tex; mode=display">
F(x)=\sum_{k=-\infty}^{\infty} c_{k} e^{i k x}</script><p>复傅立叶系数$c_{k}$有计算公式：</p>
<script type="math/tex; mode=display">
c_{k}=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x)(\cos (k x)-i \sin (k x)) d x=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) e^{-i k x} d x</script><p>类似地，$T$周期函数$f(t)$的指数傅立叶级数如下：</p>
<script type="math/tex; mode=display">
F(t)=\sum_{k=-\infty}^{\infty} c_{k} e^{2 \pi i k t / T}</script><p>复傅立叶系数则为</p>
<script type="math/tex; mode=display">
c_{k}=\frac{1}{T} \int_{-T / 2}^{T / 2} f(t) e^{-2 \pi i k t / T} d t</script><p>如果从一开始考虑指数函数集$\boldsymbol{\beta}^{\prime}=\left{e^{i k x}, k \in \mathbb{Z}\right}$，把$f(x)$正交投影至集合$\beta^{\prime}$扩张出的子空间上，可以导出$2 \pi$周期实函数的指数傅立叶级数，但$e^{i k x}$是复指数，因此实函数的内积定义必须修改为复函数内积。对于区间$[-\pi, \pi]$的两个复函数$f(x)$和$g(x)$，我们定义内积如下：</p>
<script type="math/tex; mode=display">
\langle f, g\rangle \stackrel{\text { def }}{=} \frac{1}{2 \pi} \int_{-\pi}^{\pi} \overline{f(x)} g(x) d x</script><p>复函数$f(x)$的长度或范数（norm）为</p>
<script type="math/tex; mode=display">
\begin{aligned}
\|f\| &=\langle f, f\rangle^{1 / 2} \\
&=\left(\frac{1}{2 \pi} \int_{-\pi}^{\pi} \overline{f(x)} f(x) d x\right)^{1 / 2} \\
&=\left(\frac{1}{2 \pi} \int_{-\pi}^{\pi}|f(x)|^{2} d x\right)^{1 / 2}
\end{aligned}</script><p>复函数内积引入常数$1 / 2$的目的在于使$\beta^{\prime}$成为一个单位正交（orthonormal）函数集。</p>
<script type="math/tex; mode=display">
\begin{aligned}
\int_{-\pi}^{\pi} e^{-i m x} e^{i n x} d x &=\int_{-\pi}^{\pi}(\cos (m x)-i \sin (m x))(\cos (n x)+i \sin (n x)) d x \\
&=\int_{-\pi}^{\pi} \cos (m x) \cos (n x) d x+\int_{-\pi}^{\pi} \sin (m x) \sin (n x) d x \\
&+i \int_{-\pi}^{\pi} \cos (m x) \sin (n x) d x-i \int_{-\pi}^{\pi} \sin (m x) \cos (n x) d x \\
&=2 \pi \delta_{m n}
\end{aligned}</script><p>其中$\delta<em>{m n}=1$，若$m = n$，$\delta</em>{m n}=0$若$m \neq n$。因此证明$\boldsymbol{\beta}^{\prime}=\left{e^{i k x}, k \in \mathbb{Z}\right}$是一个单位正交集。函数$f(x)$的指数傅立叶级数即为$f(x)$至$\beta^{\prime}$扩张成的子空间的正交投影：</p>
<script type="math/tex; mode=display">
F(x)=\sum_{k=-\infty}^{\infty}\left\langle e^{i k x}, f(x)\right\rangle e^{i k x}</script><p>上式中，正交分解所含的系数就是傅立叶系数：</p>
<script type="math/tex; mode=display">
c_{k}=\left\langle e^{i k x}, f(x)\right\rangle=\frac{1}{2 \pi} \int_{-\pi}^{\pi} f(x) e^{-i k x} d x</script><h2 id="2-离散傅立叶变换"><a href="#2-离散傅立叶变换" class="headerlink" title="2. 离散傅立叶变换"></a>2. 离散傅立叶变换</h2><p>考虑定义于区间$[-T / 2, T / 2]$的周期为$T$的函数$f(t)$的指数傅立叶级数</p>
<script type="math/tex; mode=display">
F(t)=\sum_{k=-\infty}^{\infty} c_{k} e^{2 \pi i k t / T}</script><p>其中复傅立叶系数为</p>
<script type="math/tex; mode=display">
c_{k}=\frac{1}{T} \int_{-T / 2}^{T / 2} f(t) e^{-2 \pi i k t / T} d t, \quad k \in \mathbb{Z}</script><p>若$f(t)$满足Dirichlet条件，可以证明$|f-F|^{2}=0$，从现在开始将解除$f(t)$是周期函数的限制，假设$\int_{-\infty}^{\infty}|f(t)| d t$是有界的。当$T \rightarrow \infty$，傅立叶级数可推广为傅立叶转换（Fourier transform）。进一步地，若$f(t)$不再是连续函数而是一有限数列，傅立叶级数又可延伸为离散傅立叶转换（discrete Fourier transform，简称DFT）。</p>
<p>傅立叶转换是傅立叶级数于$T \rightarrow \infty$的表达形式。令$\nu<em>{k}=k / T$，且$\Delta \nu=\nu</em>{k+1}-\nu<em>{k}=1 / T$，则$f</em>{T}(t)$的傅立叶级数为</p>
<script type="math/tex; mode=display">
f_{T}(t)=\sum_{k=-\infty}^{\infty} c_{k}(T) e^{2 \pi i \nu_{k} t}</script><p>其中我们令</p>
<script type="math/tex; mode=display">
c_{k}(T)=\frac{1}{T} \int_{-T / 2}^{T / 2} f_{T}(t) e^{-2 \pi i \nu_{k} t} dt</script><p>将$c<em>{k}(T)$代回傅立叶级数$f</em>{T}(t)$，可得</p>
<script type="math/tex; mode=display">
\begin{aligned}
f_{T}(t) &=\sum_{k=-\infty}^{\infty}\left[\frac{1}{T} \int_{-T / 2}^{T / 2} f_{T}(t) e^{-2 \pi i \nu_{k} t} d t\right] e^{2 \pi i \nu_{k} t} \\
&=\sum_{k=-\infty}^{\infty}\left[\int_{-T / 2}^{T / 2} f_{T}(t) e^{-2 \pi i \nu_{k} t} d t\right] e^{2 \pi i \nu_{k} t} \Delta \nu
\end{aligned}</script><p>注意，上式中括弧内的$t$是虚拟变量（dummy variable）。当$T$逐渐增大时，$\Delta \nu=1 / T$变成一微小量$d \nu$，使得$\nu<em>{k}$逼近连续变量$\mathcal{V}$，总和因此趋于积分。令$T \rightarrow \infty$，则$f</em>{T}(t) \rightarrow f(t)$，可得下列等式：</p>
<script type="math/tex; mode=display">
f(t)=\int_{-\infty}^{\infty}\left[\int_{-\infty}^{\infty} f(t) e^{-2 \pi i \nu t} d t\right] e^{2 \pi i \nu t} d \nu</script><p>欲看清整个关系，将上式拆开为两部分。令括弧内的函数为</p>
<script type="math/tex; mode=display">
\hat{f}(\nu)=\int_{-\infty}^{\infty} f(t) e^{-2 \pi i \nu t} d t</script><p>称为$f(t)$的傅立叶转换，其中变量$\nu$代表频率。因此得到</p>
<script type="math/tex; mode=display">
f(t)=\int_{-\infty}^{\infty} \hat{f}(\nu) e^{2 \pi i \nu t} d \nu</script><p>称为$\hat{f}(\nu)$的逆傅立叶转换。</p>
<hr>
<h3 id="傅立叶矩阵"><a href="#傅立叶矩阵" class="headerlink" title="傅立叶矩阵"></a>傅立叶矩阵</h3><p>在数值计算上，因为电脑仅能处理有限维向量，连续型态的傅立叶转换必须换装成离散傅立叶转换。离散傅立叶转换的输入不再是无限维函数$f(t)$，我们仅能运用一周期$[0, T]$内$n$个等间距函数值$x<em>{j} \equiv f\left(t</em>{j}\right)$，其中$t<em>{j}=j T / n, \quad j=0,1, \ldots, n-1$。其次离散傅立叶转换的输出是复傅立叶系数构成的相同长度数列$c</em>{k}, k=0,1, \ldots, n-1$，而非无穷数列。因为傅立叶系数$c<em>{k}$与函数$f(t)$的关系是线性的（积分是线性函数），我们也预期离散傅立叶转换是数列$\left{x</em>{j}\right}$和$\left{c_{k}\right}$之间的一可逆线性转换，故可表示为$n \times n$阶可逆矩阵。在连续情况下，将区间正规化，令$T=1$，复傅立叶级数即为：</p>
<script type="math/tex; mode=display">
c_{k}=\int_{0}^{1} f(t) e^{-2 \pi i k t} d t</script><p>在离散情况下，以$t_{j}=j / n$取代$t$，将连续函数的傅立叶系数改为计算和，可得对应的离散公式：对于$k=0,1, \ldots, n-1$</p>
<script type="math/tex; mode=display">
y_{k}=\sum_{j=0}^{n-1} x_{j} e^{-2 \pi i k j / n}</script><p>此即<strong>离散傅立叶转换</strong>。为了与傅立叶系数$c<em>{k}$区分，我们以$y</em>{k}$代表离散傅立叶转换结果。使用欧拉公式$e^{i \theta}=\cos \theta+i \sin \theta$，令</p>
<script type="math/tex; mode=display">
w=e^{-2 \pi i / n}=\cos \left(\frac{2 \pi}{n}\right)-i \sin \left(\frac{2 \pi}{n}\right)</script><p>离散傅立叶转换可表示成矩阵形式$\mathbf{y}=F \mathbf{x}$，如下：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
{y_{0}} \\
{y_{1}} \\
{y_{2}} \\
{\vdots} \\
{y_{n-1}}
\end{array}\right]=\left[\begin{array}{ccccc}
{1} & {1} & {1} & {\cdots} & {1} \\
{1} & {w} & {w^{2}} & {\cdots} & {w^{n-1}} \\
{1} & {w^{2}} & {w^{4}} & {\cdots} & {w^{2(n-1)}} \\
{\vdots} & {\vdots} & {\vdots} & {\ddots} & {\vdots} \\
{1} & {w^{n-1}} & {w^{2(n-1)}} & {\cdots} & {w^{(n-1)^{2}}}
\end{array}\right]\left[\begin{array}{c}
{x_{0}} \\
{x_{1}} \\
{x_{2}} \\
{\vdots} \\
{x_{n-1}}
\end{array}\right]</script><p>上面的$n \times n$阶矩阵$F$称为傅立叶矩阵。$F$是一种特殊的Vandermonde矩阵。</p>
<p>复数集$\left{1, w, w^{2}, \ldots, w^{n-1}\right}$为方程式$z^{n}=1$的根，称为$n$次方单位根，这些根在复数平面上单位圆按顺时针旋转并以相同间距排列。若$k \geq n$，则$w^{k}=w^{k \bmod n}$。对于任意整数$k$，由欧拉公式可确认</p>
<script type="math/tex; mode=display">
\overline{w^{k}}=\overline{e^{-2 \pi i k / n}}=\cos \left(\frac{2 \pi k}{n}\right)+i \sin \left(\frac{2 \pi k}{n}\right)=e^{2 \pi i k / n}=w^{-k}</script><p>考虑下式</p>
<script type="math/tex; mode=display">
w^{k}\left(1+w^{k}+w^{2 k}+\cdots+w^{(n-2) k}+w^{(n-1) k}\right)=w^{k}+w^{2 k}+\cdots+w^{(n-1) k}+1</script><script type="math/tex; mode=display">
\left(w^{k}-1\right)\left(1+w^{k}+w^{2 k}+\cdots+w^{(n-2) k}+w^{(n-1) k}\right)=0</script><p>若$k \text { mod } n \neq 0$，则$w^{k} \neq 1$，可推得</p>
<script type="math/tex; mode=display">
\sum_{j=0}^{n-1} w^{j k}=1+w^{k}+w^{2 k}+\cdots+w^{(n-2) k}+w^{(n-1) k}=0</script><p>可以看出，傅立叶矩阵除了第一行，其余行的所有元素之和为0<br>下面我们运用此等式计算逆傅立叶矩阵$F^{-1}$<br>令$\mathbf{f}<em>{p}$和$\mathbf{f}</em>{q}$代表$F$的第$p$行和第$q$行，若$p \neq q$，则$(q-p) \text { mod } n \neq 0$，两向量的内积为</p>
<script type="math/tex; mode=display">
\left\langle\mathbf{f}_{p}, \mathbf{f}_{q}\right\rangle \stackrel{\text { def }}{=} \mathbf{f}_{p}^{*} \mathbf{f}_{q}=\sum_{j=0}^{n-1} \overline{w^{p j}} w^{j q}=\sum_{j=0}^{n-1} w^{j(q-p)}=0</script><p>并且</p>
<script type="math/tex; mode=display">
\left\|\mathbf{f}_{p}\right\|^{2}=\mathbf{f}_{p}^{*} \mathbf{f}_{p}=\sum_{j=0}^{n-1} \overline{w^{p j}} w^{j p}=\sum_{j=0}^{n-1} 1=n</script><p>有$\left|\mathbf{f}_{p}\right|=\sqrt{n}$。若将$F$的各行向量单位化，$\frac{1}{\sqrt{n}} F$为一酉矩阵（unitary）。因为$F^{T}=F$，就有</p>
<script type="math/tex; mode=display">
\left(\frac{1}{\sqrt{n}} F\right)^{-1}=\left(\frac{1}{\sqrt{n}} F\right)^{H}=\frac{1}{\sqrt{n}} \bar{F}</script><p>故$F^{-1}=\bar{F} / n$，即$\left(F^{-1}\right)_{p q}=w^{-p q} / n$，或明确写出</p>
<script type="math/tex; mode=display">
F^{-1}=\frac{1}{n}\left[\begin{array}{ccccc}
{1} & {1} & {1} & {\cdots} & {1} \\
{1} & {w^{-1}} & {w^{-2}} & {\cdots} & {w^{-(n-1)}} \\
{1} & {w^{-2}} & {w^{-4}} & {\cdots} & {w^{-2(n-1)}} \\
{\vdots} & {\vdots} & {\vdots} & {\ddots} & {\vdots} \\
{1} & {w^{-(n-1)}} & {w^{-2(n-1)}} & {\cdots} & {w^{-(n-1)^{2}}}
\end{array}\right]</script><p>所以，逆离散傅立叶转换$\mathbf{x}=F^{-1} \mathbf{y}$：对于$j=0,1, \ldots, n-1$</p>
<script type="math/tex; mode=display">
x_{j}=\frac{1}{n} \sum_{k=0}^{n-1} y_{k} e^{2 \pi i k j / n}</script><p>离散傅立叶转换$F_{\mathbf{X}}$和逆转换$F^{-1} \mathbf{y}$有相同的运算方式，因为</p>
<script type="math/tex; mode=display">
F^{-1} \mathbf{y}=\frac{\bar{F} \mathbf{y}}{n}=\frac{\overline{F \overline{\mathbf{y}}}}{n}</script><p>假设我们设计出一个离散傅立叶转换程序DFT(·)，则逆转换$\mathbf{x}=F^{-1} \mathbf{y}$的算法如下：<br>（1）计算共轭向量$\overline{\mathbf{y}}$<br>（2）计算离散傅立叶转换$\mathbf{z}=\operatorname{DFT}(\bar{\mathbf{y}})$<br>（3）计算共轭向量的纯量乘法$\mathbf{x}=(1 / n) \overline{\mathbf{z}}$</p>
]]></content>
      <categories>
        <category>傅立叶分析</category>
      </categories>
      <tags>
        <tag>傅立叶分析</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（13）：正定矩阵（positive definite matrix）</title>
    <url>/2019/12/29/linear_algebra_13/</url>
    <content><![CDATA[<p>清华大学线性代数（2）课程第一讲：正定矩阵</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<h2 id="1-二次型及其标准形"><a href="#1-二次型及其标准形" class="headerlink" title="1.二次型及其标准形"></a>1.二次型及其标准形</h2><p>定义：含有n个变量$x<em>{1}, x</em>{2}, \cdots, x_{n}$的二次齐次函数</p>
<script type="math/tex; mode=display">
f\left(x_{1}, x_{2}, \cdots, x_{n}\right)=a_{11} x_{1}^{2}+a_{22} x_{2}^{2}+\cdots+a_{n n} x_{n}^{2}+</script><script type="math/tex; mode=display">
2 a_{12} x_{1} x_{2}+2 a_{13} x_{1} x_{3}+\cdots+2 a_{n-1, n} x_{n-1} x_{n}</script><p>称为<strong>二次型</strong>（<strong>quadratic form</strong>）<br>对于二次型，我们讨论的主要问题是：寻求可逆的线性变换</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}
{x_{1}=c_{11} y_{1}+c_{12} y_{2}+\cdots+c_{1 n} y_{n}} \\
{x_{2}=c_{21} y_{1}+c_{22} y_{2}+\cdots+c_{2 n} y_{n}} \\
{\cdots \cdots \cdots \cdots \cdots} \\
{x_{n}=c_{n 1} y_{1}+c_{n 2} y_{2}+\cdots+c_{n n} y_{n}}
\end{array}\right.</script><p>使二次型只含平方项，能使</p>
<script type="math/tex; mode=display">
f=k_{1} y_{1}^{2}+k_{2} y_{2}^{2}+\cdots+k_{n} y_{n}^{2}</script><p>这种只含平方项的二次型，称为二次型的<strong>标准形</strong>。<br>如果标准形的系数$k<em>{1}, k</em>{2}, \cdots, k<em>{n}$只在$1,-1,0$三个数中取值，有：<br>$f=y</em>{1}^{2}+\cdots+y<em>{p}^{2}-y</em>{p+1}^{2}-\cdots-y_{r}^{2}$<br>则称上式为二次型的<strong>规范形</strong>。<br>利用矩阵，二次型可表示为</p>
<script type="math/tex; mode=display">
f=x_{1}\left(a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}\right)+x_{2}\left(a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}\right)+\cdots+</script><script type="math/tex; mode=display">
x_{n}\left(a_{n 1} x_{1}+a_{n 2} x_{2}+\cdots+a_{n n} x_{n}\right)</script><script type="math/tex; mode=display">
=\left(x_{1}, x_{2}, \cdots, x_{n}\right)\left(\begin{array}{c}
{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}} \\
{a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}} \\
{\vdots} \\
{a_{n 1} x_{1}+a_{n 2} x_{2}+\cdots+a_{n n} x_{n}}
\end{array}\right)</script><script type="math/tex; mode=display">
=\left(x_{1}, x_{2}, \cdots, x_{n}\right)\left(\begin{array}{cccc}
{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\
{a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\
{\vdots} & {\vdots} & {} & {\vdots} \\
{a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}}
\end{array}\right)\left(\begin{array}{c}
{x_{1}} \\
{x_{2}} \\
{\vdots} \\
{x_{n}}
\end{array}\right)</script><p>记</p>
<script type="math/tex; mode=display">
A=\left(\begin{array}{cccc}
{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\
{a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\
{\vdots} & {\vdots} & {} & {\vdots} \\
{a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}}
\end{array}\right), \mathbf{x}=\left(\begin{array}{c}
{x_{1}} \\
{x_{2}} \\
{\vdots} \\
{x_{n}}
\end{array}\right)</script><p>则二次型可用矩阵记作</p>
<script type="math/tex; mode=display">
f=\mathbf{x}^{\mathrm{T}} A \mathbf{x}=\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j} x_{i} x_{j}</script><p>其中$A$为对称矩阵<br>任给一个二次型，就唯一确定一个对称矩阵；反之，任给一个对称矩阵，也唯一地确定一个二次型。这样，二次型和对称矩阵之间存在一一对应的关系。因此我们把对称矩阵$A$叫做<strong>二次型$f$的矩阵</strong>，也把$f$叫做<strong>对称矩阵$A$的二次型</strong>，对称矩阵A的秩就叫做<strong>二次型$f$的秩</strong>。</p>
<hr>
<p>记$C=\left(c_{i j}\right)$，把可逆变换记作</p>
<script type="math/tex; mode=display">
\mathbf{x}=C \mathbf{y}</script><p>有$f=\mathbf{x}^{\mathrm{T}} A \mathbf{x}=(C \mathbf{y})^{\mathrm{T}} A C \mathbf{y}=\mathbf{y}^{\mathrm{T}}\left(C^{\mathrm{T}} A C\right) \mathbf{y}$。</p>
<p>定义：设$A$和$B$是n阶矩阵，若有可逆矩阵$C$，使$B=C^{\mathrm{T}} A C$，则称矩阵$A$和$B$<strong>合同</strong>。<br>显然，若$A$为对称矩阵，则$B=C^{\mathrm{T}} A C$也为对称矩阵，且$R(B)=R(A)$。</p>
<p>由此可知，经过可逆变换$\mathbf{x}=C \mathbf{y}$后，二次型$f$的矩阵由$A$变为与$A$合同的矩阵$C^{\mathrm{T}} A C$，且二次型的秩不变。</p>
<p><strong>惯性定理（Sylvester）</strong>：实对称矩阵$\mathbf{A}$与矩阵$\mathbf{C}^{T} \mathbf{A} \mathbf{C}$具有相同数目的正特征值、负特征值和零特征值<br>换言之，特征值的符号在合同变换下保持不变。</p>
<hr>
<p><strong>主轴定理</strong>：任给二次型$f=\sum<em>{i, j=1}^{n} a</em>{i j} x<em>{i} x</em>{j} \quad\left(a<em>{i j}=a</em>{j i}\right)$，总有正交变换$\mathbf{x}=P \mathbf{y}$，使$f$化为标准形</p>
<script type="math/tex; mode=display">
f=\lambda_{1} y_{1}^{2}+\lambda_{2} y_{2}^{2}+\cdots+\lambda_{n} y_{n}^{2}</script><p>其中$\lambda<em>{1}, \lambda</em>{2}, \cdots, \lambda<em>{n}$是$f$的矩阵$A=\left(a</em>{i j}\right)$的特征值</p>
<p>推论：任给n元二次型$f(\mathbf{x})=\mathbf{x}^{\mathrm{T}} A \mathbf{x} \quad\left(A^{\mathrm{T}}=A\right)$，总有可逆变换$\mathbf{x}=\mathbf{C} \mathbf{z}$，使$f(\mathbf{C} \mathbf{z})$为规范形。</p>
<p>证明：根据主轴定理有</p>
<script type="math/tex; mode=display">
f(P \mathbf{y})=\mathbf{y}^{\mathrm{T}} \mathbf{\Lambda} \mathbf{y}=\lambda_{1} y_{1}^{2}+\cdots+\lambda_{n} y_{n}^{2}</script><p>设二次型$f$的秩为r，则特征值$\lambda<em>{i}$中恰有r个不为0，不妨设$\lambda</em>{1}, \cdots, \lambda<em>{r}$不等于0，$\lambda</em>{r+1}=\cdots=\lambda_{n}=0$，令</p>
<script type="math/tex; mode=display">
K=\left(\begin{array}{cccc}
{k_{1}} & {} & {} & {} \\
{} & {k_{2}} & {} & {} \\
{} & {} & {\ddots} & {} \\
{} & {} & {} & {k_{n}}
\end{array}\right)</script><p>其中</p>
<script type="math/tex; mode=display">
k_{i}=\left\{\begin{array}{cc}
{\frac{1}{\sqrt{\left|\lambda_{i}\right|}}, i \leqslant r} \\
{1,i>r}
\end{array}\right.</script><p>则$\mathbf{K}$可逆，变换$\mathbf{y}=K \mathbf{z}$把$f(\mathbf{P} \mathbf{y})$化为</p>
<script type="math/tex; mode=display">
f(\mathbf{P} \mathbf{K} \mathbf{z})=\mathbf{z}^{\mathrm{T}} \mathbf{K}^{\mathrm{T}} \mathbf{P}^{\mathrm{T}} \mathbf{A} \mathbf{P} \mathbf{K} z=\mathbf{z}^{\mathrm{T}} \mathbf{K}^{\mathrm{T}} \mathbf{\Lambda} \mathbf{K} \mathbf{z}</script><p>而</p>
<script type="math/tex; mode=display">
\mathbf{K}^{\mathrm{T}} \mathbf{\Lambda} \mathbf{K}=\operatorname{diag}\left(\frac{\lambda_{1}}{\left|\lambda_{1}\right|}, \cdots, \frac{\lambda_{r}}{\left|\lambda_{r}\right|}, 0, \cdots, 0\right)</script><p>记$\mathbf{C}=\mathbf{P} \mathbf{K}$，则知可逆变换$\mathbf{x}=C \mathbf{z}$把$f$化为规范形</p>
<script type="math/tex; mode=display">
f(\mathbf{C} z)=\frac{\lambda_{1}}{\left|\lambda_{1}\right|} z_{1}^{2}+\cdots+\frac{\lambda_{r}}{\left|\lambda_{r}\right|} z_{r}^{2}</script><h2 id="2-正定二次型"><a href="#2-正定二次型" class="headerlink" title="2.正定二次型"></a>2.正定二次型</h2><p>二次型的标准形显然不是唯一的，但标准形中所含项数是确定的（即是二次型的秩）。不仅如此，在限定变换为实变换时，标准形中正系数的个数是不变的（从而负系数的个数也不变），也就是有<br><strong>惯性定理</strong>：二次型$f=\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}$的秩为r，且有两个可逆变换$\mathbf{x}=C \mathbf{y}$和$\mathbf{x}=P \mathbf{z}$使得</p>
<script type="math/tex; mode=display">
f=k_{1} y_{1}^{2}+k_{2} y_{2}^{2}+\cdots+k_{r} y_{r}^{2} \quad\left(k_{i} \neq 0\right)</script><p>以及</p>
<script type="math/tex; mode=display">
f=\lambda_{1} z_{1}^{2}+\lambda_{2} z_{2}^{2}+\cdots+\lambda_{r} z_{r}^{2} \quad\left(\lambda_{i} \neq 0\right)</script><p>则$k<em>{1}, \cdots, k</em>{r}$中正数的个数与$\lambda<em>{1}, \cdots, \lambda</em>{r}$中正数的个数相等，其中二次型的标准形中正系数的个数称为二次型的<strong>正惯性指数</strong>。负系数的个数称为<strong>负惯性指数</strong>。若二次型$f$的正惯性指数为$p$，秩为$r$，则$f$的规范形便可确定为</p>
<script type="math/tex; mode=display">
f=y_{1}^{2}+\cdots+y_{p}^{2}-y_{p+1}^{2}-\cdots-y_{r}^{2}</script><hr>
<p>科学技术上用的较多的二次型是正惯性指数为n或负惯性指数为n的n元二次型，我们有下述定义</p>
<p>定义：设二次型$f(\mathbf{x})=\mathbf{x}^{\mathrm{T}} A \mathbf{x}$，如果对任何$\mathbf{x} \neq 0$，都有$f(\mathbf{x})&gt;0$，显然$f(\mathbf{0})=0$，则称$f$为<strong>正定二次型</strong>，并称对称矩阵$A$是<strong>正定的</strong>；如果对任何$\mathbf{x} \neq 0$，都有$f(\mathbf{x})&lt;0$，则称$f$为<strong>负定二次型</strong>，并称对称矩阵$A$是<strong>负定的</strong>；</p>
<p>定理：n元二次型$f=\mathbf{x}^{\mathrm{T}} \mathbf{A} \mathbf{x}$为正定的充分必要条件是：它的标准形的n个系数全为正，即它的规范形的n个系数全为1，亦即它的正惯性指数等于n。</p>
<p>推论：对称矩阵$A$为正定的充分必要条件是：$A$的特征值全为正</p>
<hr>
<p><strong>赫尔维茨定理</strong>：对称矩阵$A$为正定的充分必要条件是：$A$的各阶主子式都为正，即</p>
<script type="math/tex; mode=display">
a_{11}>0, \quad\left|\begin{array}{cc}
{a_{11}} & {a_{12}} \\
{a_{21}} & {a_{22}}
\end{array}\right|>0, \cdots,\left|\begin{array}{ccc}
{a_{11}} & {\cdots} & {a_{1 n}} \\
{\vdots} & {} & {\vdots} \\
{a_{n 1}} & {\cdots} & {a_{n n}}
\end{array}\right|</script><p>对称矩阵$A$为负定的充分必要条件是：奇数阶主子式为负，偶数阶主子式为正，即</p>
<script type="math/tex; mode=display">
(-1)^{r}\left|\begin{array}{ccc}
{a_{11}} & {\cdots} & {a_{1 r}} \\
{\vdots} & {} & {\vdots} \\
{a_{r 1}} & {\cdots} & {a_{r r}}
\end{array}\right|>0,(r=1,2, \cdots, n)</script><hr>
<p>实对称矩阵A正定的充要条件</p>
<p>（1）$\mathbf{A}$的所有特征值$\lambda_{i}$均为正<br>（2）$\mathbf{x}^{T} \mathbf{A} \mathbf{x}&gt;0$对所有非零向量$\mathbf{x}$成立<br>（3）$\mathbf{A}$的所有顺序主子式都是正的<br>（4）$\mathbf{A}$的所有主元（无行交换）都是正的<br>（5）存在列满秩矩阵$\mathbf{R}$，使得$\mathbf{A}=\mathbf{R}^{T} \mathbf{R}$<br>（6）$\mathbf{A}$的所有主子式都是正的<br>以上六条是等价的</p>
<h2 id="3-半正定矩阵及其判别条件"><a href="#3-半正定矩阵及其判别条件" class="headerlink" title="3.半正定矩阵及其判别条件"></a>3.半正定矩阵及其判别条件</h2><p>定义：若实对称矩阵$\mathbf{A}$的特征值均非负，那么称$\mathbf{A}$为<strong>半正定矩阵</strong>（positive semidefinite matrix）</p>
<p>半正定矩阵的判别条件：<br>（1）$\mathbf{A}$的所有特征值$\lambda_{i}$均非负<br>（2）$\mathbf{x}^{T} \mathbf{A} \mathbf{x}\geq 0$对所有非零向量$\mathbf{x}$成立<br>（3）存在矩阵$\mathbf{R}$，使得$\mathbf{A}=\mathbf{R}^{T} \mathbf{R}$。（ $\mathbf{R}$可为不可逆矩阵）<br>（4）$\mathbf{A}$的所有主子式均非负<br>以上四条是等价的</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>斐波那契数列的通项公式</title>
    <url>/2019/12/24/Fibonacci/</url>
    <content><![CDATA[<p>由差分方程，特征值，特征向量推导斐波那契数列的通项公式</p>
<span id="more"></span>
<h2 id="1-斐波那契数列（Fibonacci-sequence）"><a href="#1-斐波那契数列（Fibonacci-sequence）" class="headerlink" title="1.斐波那契数列（Fibonacci sequence）"></a>1.斐波那契数列（Fibonacci sequence）</h2><p><strong>斐波那契数列（Fibonacci sequence）</strong>，又称黄金分割数列、因数学家列昂纳多·斐波那契（Leonardoda Fibonacci）以兔子繁殖为例子而引入，故又称为“兔子数列”，指的是这样一个数列：1、1、2、3、5、8、13、21、34、……在数学上，斐波那契数列以如下被以递推的方法定义：$F(1)=1, \quad F(2)=1, F(n)=F(n-1)+F(n-2) \quad\left(n&gt;=3, \quad n \in N^{*}\right)$。在现代物理、准晶体结构、化学等领域，斐波纳契数列都有直接的应用，为此，美国数学会从1963年起出版了以《斐波纳契数列季刊》为名的一份数学杂志，用于专门刊载这方面的研究成果。</p>
<p>斐波那契数列是在数学中非常有趣的一个数列，它在自然界中也有很多应用。比如叶子按照螺旋的方式长在树上，苹果树上叶子绕着它的茎每2圈有5片叶子，榆树上每3圈有8片叶子，柳树上每5圈有13片叶子。</p>
<h2 id="2-矩阵可对角化在斐波那契数列通项公式推导中的作用"><a href="#2-矩阵可对角化在斐波那契数列通项公式推导中的作用" class="headerlink" title="2.矩阵可对角化在斐波那契数列通项公式推导中的作用"></a>2.矩阵可对角化在斐波那契数列通项公式推导中的作用</h2><p>Fibonacci数列：数列$F_{n}: 0,1,1,2,3,5,8,13, \cdots$满足规律：</p>
<script type="math/tex; mode=display">
F_{k+2}=F_{k+1}+F_{k}</script><p>这是一个差分方程。</p>
<p>怎么样从$F<em>{0}=0, F</em>{1}=1$出发，求出Fibonacci数列的通项公式呢？<br>令</p>
<script type="math/tex; mode=display">
\mathbf{u}_{k}=\left(\begin{array}{c}{F_{k+1}} \\ {F_{k}}\end{array}\right)</script><p>则</p>
<script type="math/tex; mode=display">\left\{\begin{array}{l}{F_{k+2}=F_{k+1}+F_{k}} \\ {F_{k+1}=F_{k+1}}\end{array}\right.</script><p>即</p>
<script type="math/tex; mode=display">\mathbf{u}_{k+1}=\left(\begin{array}{ll}{1} & {1} \\ {1} & {0}\end{array}\right)\mathbf{u}_{k}=A \mathbf{u}_{k}</script><p>于是$\mathbf{u}<em>{k}=A^{k} \mathbf{u}</em>{0}$，只需求$A^{k}$</p>
<script type="math/tex; mode=display">
\operatorname{det}(A-\lambda I)=\left|\begin{array}{cc}{1-\lambda} & {1} \\ {1} & {-\lambda}\end{array}\right|=\lambda^{2}-\lambda-1</script><script type="math/tex; mode=display">
\Longrightarrow \lambda_{1}=\frac{1+\sqrt{5}}{2}, \lambda_{2}=\frac{1-\sqrt{5}}{2}</script><p>由初始值$F<em>{0}=0, F</em>{1}=1$，给出</p>
<script type="math/tex; mode=display">
\mathbf{u}_{0}=\left(\begin{array}{l}{1} \\ {0}\end{array}\right)</script><script type="math/tex; mode=display">
\left(\begin{array}{c}{F_{k+1}} \\ {F_{k}}\end{array}\right)=\mathbf{u}_{k}=A^{k} \mathbf{u}_{0}=S \Lambda^{k} S^{-1} \mathbf{u}_{0}</script><script type="math/tex; mode=display">
=\left(\begin{array}{cc}{\lambda_{1}} & {\lambda_{2}} \\ {1} & {1}\end{array}\right)\left(\begin{array}{cc}{\lambda_{1}^{k}} & {0} \\ {0} & {\lambda_{2}^{k}}\end{array}\right)\left(\begin{array}{c}{1} \\ {-1}\end{array}\right) \frac{1}{\lambda_{1}-\lambda_{2}}</script><p>Fibonacci数$F_{k}$是这个乘积的第二个分量</p>
<script type="math/tex; mode=display">
F_{k}=\frac{\lambda_{1}^{k}}{\lambda_{1}-\lambda_{2}}-\frac{\lambda_{2}^{k}}{\lambda_{1}-\lambda_{2}}=\frac{1}{\sqrt{5}}\left[\left(\frac{1+\sqrt{5}}{2}\right)^{k}-\left(\frac{1-\sqrt{5}}{2}\right)^{k}\right]</script><p>这就是斐波那契数列的通项公式</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（12）：方阵的特征值与特征向量，方阵的对角化</title>
    <url>/2019/12/15/linear_algebra_12/</url>
    <content><![CDATA[<p>清华大学线性代数课程第19，20，21，22讲：方阵的特征值与特征向量，方阵的对角化</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<h2 id="1-方阵的特征值与特征向量"><a href="#1-方阵的特征值与特征向量" class="headerlink" title="1. 方阵的特征值与特征向量"></a>1. 方阵的特征值与特征向量</h2><p>定义：设$A$是n阶矩阵，如果数$\lambda$和n维<strong>非零</strong>列向量$x$使关系式</p>
<script type="math/tex; mode=display">
A \mathbf{x}=\lambda \mathbf{x}</script><p>成立，那么，这样的数$\lambda$称为矩阵$A$的<strong>特征值</strong>（eigenvalue）,非零向量$x$称为$A$的对应于特征值$\lambda$的<strong>特征向量</strong>（eigenvector），上式也可以写成</p>
<script type="math/tex; mode=display">
(A-\lambda E) \mathbf{x}=0</script><p>这是n个未知数n个方程的齐次线性方程组，它有非零解的充分必要条件是系数行列式</p>
<script type="math/tex; mode=display">
|A-\lambda E|=0</script><p>即</p>
<script type="math/tex; mode=display">
\left|\begin{array}{cccc}{a_{11}-\lambda} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}-\lambda} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}-\lambda}\end{array}\right|=0</script><p>上式是以$\lambda$为未知数的一元n次方程，称为矩阵$A$的<strong>特征方程</strong>（characteristic equation），其左端$|A-\lambda E|$是$\lambda$的n次多项式，记作$f(\lambda)$，称为矩阵$A$的<strong>特征多项式</strong>（characteristic polynomial）。显然，$A$的特征值就是特征方程的解。根据代数的基本定理，特征方程在复数范围内恒有解，其个数为方程的次数（重根按重数计算），因此，n阶矩阵$A$在复数范围内有n个特征值。<br>设n阶矩阵$A=\left(a<em>{i j}\right)$的特征值为$\lambda</em>{1}, \lambda<em>{2}, \cdots, \lambda</em>{n}$，不难证明<br>$\text { (i) } \lambda<em>{1}+\lambda</em>{2}+\cdots+\lambda<em>{n}=a</em>{11}+a<em>{22}+\cdots+a</em>{n n}$<br>$\text { (ii) } \lambda<em>{1} \lambda</em>{2} \cdots \lambda<em>{n}=|A|$<br>由$(\text { ii })$可知$A$是可逆矩阵的充分必要条件是它的n个特征值全不为零，若矩阵$A$不可逆，则$A$有零特征值。<br>设$\lambda=\lambda</em>{i}$为矩阵$A$的一个特征值，则由方程</p>
<script type="math/tex; mode=display">
\left(A-\lambda_{i} E\right) \mathbf{x}=0</script><p>可求得非零解$\mathbf{x}=p<em>{i}$，那么$\mathbf{p}</em>{i}$便是$A$的对应于特征值$\lambda<em>{i}$的特征向量。若$\lambda</em>{i}$为实数，则$\mathbf{p}<em>{i}$可以取实向量；若$\lambda</em>{i}$为复数，则$\mathbf{p}<em>{i}$为复向量。<br>显然，若$\mathbf{p}</em>{i}$是矩阵$A$的对应于特征值$\lambda<em>{i}$的特征向量，则$k p</em>{i}\quad(k \neq 0)$也是对应于$\lambda_{i}$的特征向量</p>
<p><strong>特征空间</strong>（eigenspace）是具有相同特征值的特征向量与一个同维数的零向量的集合</p>
<hr>
<p>特征值的性质：设$\lambda$是方阵$A$的特征值，则<br>（1）$\lambda^{2}$是$A^{2}$的特征值<br>（2）当$\mathbf{A}$可逆时，$\frac{1}{\lambda}$是$A^{-1}$的特征值<br>证明：因为$\lambda$是方阵$A$的特征值，故有$p \neq 0$使得$A p=\lambda p$，于是<br>（1）因为$A^{2} p=A(A p)=A(\lambda p)=\lambda(A p)=\lambda^{2} p$，所以$\lambda^{2}$是$A^{2}$的特征值。<br>（2）当$A$可逆时，由$A p=\lambda p$，有$p=\lambda A^{-1} p$，因$p \neq 0$，知$\lambda \neq 0$，故</p>
<script type="math/tex; mode=display">
A^{-1} p=\frac{1}{\lambda} p</script><p>所以$\frac{1}{\lambda}$是$A^{-1}$的特征值。</p>
<p>以此类推，不难证明：若$\lambda$是方阵$A$的特征值，则$\lambda^{k}$是$A^{k}$的特征值；$\varphi(\lambda)$是$\varphi(A)$的特征值（其中$\varphi(\lambda)=a<em>{0}+a</em>{1} \lambda+\cdots+a<em>{m} \lambda^{m}$是$\lambda$的多项式，$\varphi(A)=a</em>{0} E+a<em>{1} A+\cdots+a</em>{m} A^{m}$是矩阵$A$的多项式）。这是特征值的一个重要性质。</p>
<hr>
<p>投影矩阵$P=A\left(A^{T} A\right)^{-1} A^{T}$的特征值为0或1，从几何角度理解，特征向量为经过投影后方向不变的向量，只有A的列空间和A的左零空间里面的向量投影后方向不变，其中C(A)空间里面的向量投影后大小方向不变，特征值为1，$N(A^{T})$空间里面的向量经过投影后为零向量，特征值为0。</p>
<p>反射矩阵的特征值为1，-1。设$\mathbf{u} \in \mathbb{R}^{n},|\mathbf{u}|=1$，则$R=I_{n}-2 \mathbf{u} \mathbf{u}^{T}$为关于与$\mathbf{u}$正交的超平面的反射矩阵。从几何角度理解，$\forall \mathbf{v} \in \mathbb{R}^{n}$，若$\mathbf{v} \perp \mathbf{u}$，则$R \mathbf{v}=\mathbf{v}$。若$\mathbf{v} / / \mathbf{u}$，则$R \mathbf{v}=-\mathbf{v}$</p>
<p>由于，投影矩阵和反射矩阵都是对称矩阵，所以他们不同特征值对应的特征空间正交。</p>
<hr>
<p>定理：设$\lambda<em>{1}, \lambda</em>{2}, \cdots, \lambda<em>{m}$是方阵$A$的m个特征值，$p</em>{1}, p<em>{2}, \cdots, p</em>{m}$依次是与之对应的特征向量，如果$\lambda<em>{1}, \lambda</em>{2}, \cdots, \lambda<em>{m}$各不相等，则$p</em>{1}, p<em>{2}, \cdots, p</em>{m}$线性无关。</p>
<p>推论：设$\lambda<em>{1}$和$\lambda</em>{2}$是方阵$A$的两不同的特征值，$\xi<em>{1}, \xi</em>{2}, \cdots, \xi<em>{s}$和$\eta</em>{1}, \eta<em>{2}, \cdots, \eta</em>{t}$分别是对应于$\lambda<em>{1}$和$\lambda</em>{2}$的线性无关的特征向量，则$\xi<em>{1}, \xi</em>{2}, \cdots, \xi<em>{s}, \eta</em>{1}, \eta<em>{2}, \cdots, \eta</em>{t}$线性无关。</p>
<h2 id="2-相似矩阵"><a href="#2-相似矩阵" class="headerlink" title="2.相似矩阵"></a>2.相似矩阵</h2><p>定义：设$A, B$都是n阶矩阵，若有可逆矩阵$P$使</p>
<script type="math/tex; mode=display">
\mathbf{P}^{-1} \mathbf{A} \mathbf{P}=\mathbf{B}</script><p>则称$B$是$A$的<strong>相似矩阵</strong>，或说矩阵$A$和$B$相似，对$A$进行运算$\mathbf{P}^{-1} \mathbf{A} \mathbf{P}$称为对$A$进行<strong>相似变换</strong>，可逆矩阵$P$称为把$A$变成$B$的相似变换矩阵。</p>
<p>定理：若n阶矩阵$A$与$B$相似，则$A$与$B$的特征多项式相同，从而$A$与$B$的特征值亦相同<br>推论：若n阶矩阵$A$与对角矩阵</p>
<script type="math/tex; mode=display">
\mathbf{\Lambda}=\left(\begin{array}{cccc}{\lambda_{1}} \\ {} & {\lambda_{2}} \\ {} & {} & {\ddots} \\ {} & {} & {} & {\lambda_{n}}\end{array}\right)</script><p>相似，则$\lambda<em>{1}, \lambda</em>{2}, \cdots, \lambda_{n}$即是$A$的n个特征值。</p>
<p>因为若$A=P B P^{-1}$，则有$A^{k}=P B^{k} P^{-1}$，有A的多项式$\varphi(\mathbf{A})=\mathbf{P} \varphi(\mathbf{B}) \mathbf{P}^{-1}$。特别地，若有可逆矩阵$P$使$P^{-1} A P=\Lambda$为对角矩阵，即若$A$相似于对角矩阵$\mathbf{\Lambda}$，则</p>
<script type="math/tex; mode=display">
A^{k}=P \Lambda^{k} P^{-1}, \varphi(A)=P \varphi(\Lambda) P^{-1}</script><p>而对角矩阵的方幂是便于计算的。</p>
<hr>
<p>定理：n阶矩阵$A$与对角矩阵相似（即$A$能对角化）的充分必要条件是$A$有n个线性无关的特征向量<br>推论：如果n阶矩阵$A$的n个特征值互不相等，则$A$与对角矩阵相似。</p>
<hr>
<h3 id="特征值的代数重数和几何重数"><a href="#特征值的代数重数和几何重数" class="headerlink" title="特征值的代数重数和几何重数"></a>特征值的代数重数和几何重数</h3><p>定义：设$\operatorname{det}(A-\lambda I)=\left(\lambda<em>{1}-\lambda\right)^{n</em>{1}} \cdots\left(\lambda<em>{k}-\lambda\right)^{n</em>{k}}$，其中$\lambda<em>{i} \neq \lambda</em>{j}(i \neq j)$。称$n<em>{i}$为特征值$\lambda</em>{i}$的<strong>代数重数</strong>（algebraic multiplicity）,记作$A M\left(\lambda<em>{i}\right)=n</em>{i}$。称$\operatorname{dim} N\left(A-\lambda<em>{i} I\right)$为特征值$\lambda</em>{i}$的<strong>几何重数</strong>（geometric multiplicity），记作$G M\left(\lambda<em>{i}\right)=\operatorname{dim} N\left(A-\lambda</em>{i} I\right)$。</p>
<p>一般地，$G M(\lambda) \leq A M(\lambda)$，特征值的几何重数小于等于代数重数</p>
<p>定理：复方阵$A$可对角化$\Longleftrightarrow$对任意的特征值$\lambda<em>{i}, G M\left(\lambda</em>{i}\right)=A M\left(\lambda<em>{i}\right)$<br>因为：$\sum</em>{i=1}^{k} A M\left(\lambda<em>{i}\right)=n$。若$\forall i, G M\left(\lambda</em>{i}\right)=A M\left(\lambda<em>{i}\right)$，则$G M\left(\lambda</em>{1}\right)+\cdots+G M\left(\lambda_{k}\right)=n$，故$A$有n个线性无关的特征向量，从而$A$可以对角化。</p>
<p>可以看到，使$A$对角化的相似变换矩阵不是唯一的，一个特征向量乘以非零常数后仍是属于同一特征值的特征向量，所以若用任意非零常数乘以相似变换矩阵的各列，则得一个新的使$A$对角化的矩阵。而对于重特征值则有更大的自由度。同一特征值的不同特征向量的任意线性组合得到的不同线性无关特征向量都可以充当相似变换矩阵各列。</p>
<hr>
<h3 id="Jordan-标准形"><a href="#Jordan-标准形" class="headerlink" title="Jordan 标准形"></a>Jordan 标准形</h3><p>1个n阶Jordan块，形如</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cccc}
{\lambda_{0}} & {1} & {} & {} \\
{} & {\ddots} & {\ddots} & {} \\
{} & {} & {\ddots} & {1} \\
{} & {} & {} & {\lambda_{0}}
\end{array}\right)_{n \times n}</script><p>1个1阶Jordan块，形如</p>
<script type="math/tex; mode=display">
\left(\lambda_{0}\right)_{1 \times 1}</script><hr>
<p>n阶Jordan块$J<em>{\lambda</em>{0}, n}=\left(\begin{array}{ccccc}<br>{\lambda<em>{0}} &amp; {1} &amp; {} &amp; {} &amp; {} \<br>{} &amp; {\ddots} &amp; {\ddots} &amp; {} \<br>{} &amp; {} &amp; {\ddots} &amp; {1} \<br>{} &amp; {} &amp; {} &amp; {\lambda</em>{0}}<br>\end{array}\right)<em>{n \times n}$的性质<br>（1）只有一个n重特征值$\lambda</em>{0}$，只有一个线性无关的特征向量。代数重数为n，几何重数为1<br>（2）$\left(J<em>{\lambda</em>{0}, n}-\lambda<em>{0} I</em>{n}\right)^{n}=0$，$J<em>{\lambda</em>{0}, n}-\lambda<em>{0} I</em>{n}=N$为幂零矩阵。<br>（3）$J<em>{\lambda</em>{0}, n}$与$J<em>{\lambda</em>{0}, n}^{T}$相似，某些教材定义的Jordan块上面的1写到对角线下方，与1在对角线上方本质上是一致的，因为二者是相似的。</p>
<p>定理：设矩阵$A$有$s$个线性无关的特征向量，则存在可逆矩阵$P$，使得</p>
<script type="math/tex; mode=display">
P^{-1} A P=\left(\begin{array}{cccc}
{J_{1}} & {} & {} & {} \\
{} & {J_{2}} & {} & {} \\
{} & {} & {\ddots} & {} \\
{} & {} & {} & {J_{s}}
\end{array}\right)=: J</script><p>其中</p>
<script type="math/tex; mode=display">
J_{i}=\left(\begin{array}{cccc}
{\lambda_{i}} & {1} & {} & {} \\
{} & {\lambda_{i}} & {\ddots} & {} \\
{} & {} & {\ddots} & {1} \\
{} & {} & {} & {\lambda_{i}}
\end{array}\right) \quad(i=1, \cdots, s)</script><p>$J$称为矩阵$A$的Jordan标准形，若不计Jordan块的次序，则Jordan标准形唯一。<br>注：（1）Jordan标准形中Jordan块的个数等于矩阵$A$中线性无关的特征向量的个数<br>（2）若$s=n$，则$J$是对角阵，$A$可对角化</p>
<hr>
<h2 id="3-特征值在差分方程和微分方程中的应用"><a href="#3-特征值在差分方程和微分方程中的应用" class="headerlink" title="3. 特征值在差分方程和微分方程中的应用"></a>3. 特征值在差分方程和微分方程中的应用</h2><h3 id="特征值在差分方程中的应用"><a href="#特征值在差分方程中的应用" class="headerlink" title="特征值在差分方程中的应用"></a>特征值在差分方程中的应用</h3><p>设矩阵$A$可对角化，即存在可逆矩阵$S$，使得$S^{-1} A S=\Lambda$为对角阵，则$A=S \Lambda S^{-1}$，于是$A^{k}=S \Lambda^{k} S^{-1}$。<br>对于差分方程$\mathbf{u}<em>{k+1}=A \mathbf{u}</em>{k}$，解为</p>
<script type="math/tex; mode=display">
\mathbf{u}_{k}=A^{k} \mathbf{u}_{0}=S \Lambda^{k} S^{-1} \mathbf{u}_{0}</script><script type="math/tex; mode=display">
=c_{1} \lambda_{1}^{k} \mathbf{x}_{1}+\cdots+c_{n} \lambda_{n}^{k} \mathbf{x}_{n}</script><p>其中</p>
<script type="math/tex; mode=display">
\mathbf{u}_{0}=c_{1} \mathbf{x}_{1}+\cdots+c_{n} \mathbf{x}_{n}, A \mathbf{x}_{i}=\lambda_{i} \mathbf{x}_{i}(1 \leq i \leq n)</script><h3 id="特征值在微分方程中的应用"><a href="#特征值在微分方程中的应用" class="headerlink" title="特征值在微分方程中的应用"></a>特征值在微分方程中的应用</h3><p>问题：设关于t的向量值可导函数</p>
<script type="math/tex; mode=display">
\mathbf{u}=\mathbf{u}(t)=\left(\begin{array}{c}{u_{1}(t)} \\ {\vdots} \\ {u_{n}(t)}\end{array}\right)</script><p>满足</p>
<script type="math/tex; mode=display">
\frac{d \mathbf{u}}{d t}=A \mathbf{u}</script><p>其中$A=\left(a_{i j}\right)$为n阶常数矩阵，求解$\mathbf{u}=\mathbf{u}(t)$</p>
<p>当$A$不是对角矩阵时，这个问题是耦合的，每个分量的导数都和其他分量有关，问题不易解决。当$A$为对角矩阵时，这个问题是解耦的。若</p>
<script type="math/tex; mode=display">
A=\left(\begin{array}{ccc}{\lambda_{1}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {\lambda_{n}}\end{array}\right)</script><p>为对角矩阵，则</p>
<script type="math/tex; mode=display">
\frac{d \mathbf{u}}{d t}=\left(\begin{array}{ccc}{\lambda_{1}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {\lambda_{n}}\end{array}\right) \mathbf{u}</script><script type="math/tex; mode=display">
\Longleftrightarrow \frac{d u_{i}}{d t}=\lambda_{i} u_{i}(1 \leq i \leq n)</script><script type="math/tex; mode=display">
\Longleftrightarrow \mathbf{u}=\mathbf{u}(t)=\left(\begin{array}{c}{e^{\lambda_{1} t} c_{1}} \\ {\vdots} \\ {e^{\lambda_{n} t} c_{n}}\end{array}\right), c_{i}=\mathrm{const}</script><p>我们看到，当$A$是对角矩阵时，每个分量的导数只与自身有关，这样的方程组称为“解耦的”（uncoupled）</p>
<p>那么问题来了，如果对于一般的矩阵$A$，如何求解$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$。</p>
<p>（1）$A$可以对角化的情形<br>一般的，若$A=S \Lambda S^{-1}$可对角化</p>
<script type="math/tex; mode=display">
\frac{d \mathbf{u}}{d t}=A \mathbf{u}=S \Lambda S^{-1} \mathbf{u} \Longleftrightarrow \frac{d\left(S^{-1} \mathbf{u}\right)}{d t}=\Lambda\left(S^{-1} \mathbf{u}\right)</script><script type="math/tex; mode=display">
\Longrightarrow S^{-1} \mathbf{u}=\left(\begin{array}{c}{e^{\lambda_{1} t} c_{1}} \\ {\vdots} \\ {e^{\lambda_{n} t} c_{n}}\end{array}\right)</script><script type="math/tex; mode=display">
\Longrightarrow \mathbf{u}(t)=c_{1} e^{\lambda_{1} t} \mathbf{x}_{1}+\cdots+c_{n} e^{\lambda_{n} t} \mathbf{x}_{n}</script><script type="math/tex; mode=display">
\mathbf{u}(0)=c_{1} \mathbf{x}_{1}+\cdots+c_{n} \mathbf{x}_{n}</script><p>（1）$A$不可对角化的情形</p>
<p>首先定义矩阵的指数函数：<br>对于普通的指数函数，由泰勒展开有：</p>
<script type="math/tex; mode=display">
e^{x}=1+x+\frac{x^{2}}{2 !}+\cdots+\frac{x^{n}}{n !}+\cdots</script><p>设$A$为n阶矩阵，定义</p>
<script type="math/tex; mode=display">
e^{A t}=I+A t+\frac{(A t)^{2}}{2 !}+\cdots+\frac{(A t)^{n}}{n !}+\cdots</script><p>对它关于t求导</p>
<script type="math/tex; mode=display">
\Longrightarrow \frac{d}{d t}\left(e^{A t}\right)=A+A^{2} t+\frac{A^{3} t^{2}}{2 !}+\cdots</script><script type="math/tex; mode=display">
=A\left(I+A t+\frac{(A t)^{2}}{2 !}+\cdots\right)=A e^{A t}</script><p>⚠️$\mathbf{u}(t)=e^{A t} \mathbf{u}(0)$总是方程$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$的解</p>
<p>矩阵的指数函数的性质：<br>（1）若</p>
<script type="math/tex; mode=display">
\Lambda=\left(\begin{array}{ccc}{\lambda_{1}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {\lambda_{n}}\end{array}\right)</script><p>则</p>
<script type="math/tex; mode=display">
e^{\Lambda t}=\left(\begin{array}{ccc}{e^{\lambda_{1} t}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {e^{\lambda_{n} t}}\end{array}\right)</script><p>证明：按矩阵的指数函数定义展开即得。<br>（2）若$A B=B A$，即$A$ $B$可交换，则$e^{A+B}=e^{A} \cdot e^{B}$<br>（3）若存在可逆阵$P$，使得$A=P B P^{-1}$，则$e^{A t}=P e^{B t} P^{-1}$</p>
<p>利用矩阵的指数函数的性质，回到$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$的问题<br>若$A$可对角化，即存在可逆矩阵$S$，使$S^{-1} A S=\Lambda$为对角矩阵，即有$A=S \Lambda S^{-1}$。因为对于方程$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$总有解</p>
<script type="math/tex; mode=display">
\mathbf{u}(t)=e^{A t} \mathbf{u}(0)=S e^{\Lambda t} S^{-1} \mathbf{u}(0)</script><script type="math/tex; mode=display">
=\left(\mathrm{x}_{1}, \cdots, \mathrm{x}_{n}\right)\left(\begin{array}{ccc}{e^{\lambda_{1} t}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {e^{\lambda_{n} t}}\end{array}\right)\left(\begin{array}{c}{c_{1}} \\ {\vdots} \\ {c_{n}}\end{array}\right)</script><script type="math/tex; mode=display">
=c_{1} e^{\lambda_{1} t} \mathbf{x}_{1}+\cdots+c_{n} e^{\lambda_{n} t} \mathbf{x}_{n}</script><p>其中$S^{-1} \mathbf{u}(0)=\mathbf{c}$，即$\mathbf{u}(0)=S \mathbf{c}=c<em>{1} \mathbf{x}</em>{1}+\cdots+c<em>{n} \mathbf{x}</em>{n}$</p>
<p>若$A$不能对角化，如何求解呢？<br>对任何方阵$A$，总存在可逆矩阵$P$，有$P^{-1} A P=\Lambda+N$，<strong>即方阵A可以相似变换为一个对角矩阵$\Lambda$和一个幂零矩阵$N$的和</strong>，而且对角矩阵$\Lambda$和幂零矩阵$N$是可交换的。即$A=P (\Lambda+N) P^{-1}$<br>因为方程$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$总有解$\mathbf{u}(t)=e^{A t} \mathbf{u}(0)=P e^{(\Lambda+N)t} P^{-1} \mathbf{u}(0)=P e^{\Lambda t} e^{N t} P^{-1} \mathbf{u}(0)$。其中$e^{\Lambda t}$按照矩阵的指数函数的性质，有</p>
<script type="math/tex; mode=display">
e^{\Lambda t}=\left(\begin{array}{ccc}{e^{\lambda_{1} t}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {e^{\lambda_{n} t}}\end{array}\right)</script><p>而$N$是幂零矩阵，$e^{N t}$按矩阵的指数函数定义展开，计算结果必为有限个项，高次幂的项都为零。</p>
<h3 id="微分方程-frac-d-mathbf-u-d-t-A-mathbf-u-的稳定性"><a href="#微分方程-frac-d-mathbf-u-d-t-A-mathbf-u-的稳定性" class="headerlink" title="微分方程$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$的稳定性"></a>微分方程$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$的稳定性</h3><p>和差分方程一样，$t \rightarrow \infty$时，决定解$\mathbf{u}(t)$状态的是$A$的特征值。若$A$可对角化，则$\frac{d \mathbf{u}}{d t}=A \mathbf{u}$有通解：</p>
<script type="math/tex; mode=display">
\mathbf{u}(t)=c_{1} e^{\lambda_{1} t} \mathbf{x}_{1}+\cdots+c_{n} e^{\lambda_{n} t} \mathbf{x}_{n}</script><p>（1）若所有$R e \lambda_{i}&lt;0$，则$e^{A t} \rightarrow 0$，解是稳定的。</p>
<p>（2）若所有$Re \lambda_{i} \leq 0$，则$e^{A t}$有界，解是中性稳定的。</p>
<p>（3）若至少有一个特征值满足$Re \lambda&gt;0$，则$e^{A t}$无界，解是不稳定的。<br>（Re是一个数学符号，表示取一个复数的实部）</p>
<h2 id="4-实对称矩阵"><a href="#4-实对称矩阵" class="headerlink" title="4.实对称矩阵"></a>4.实对称矩阵</h2><p>若矩阵$A$满足$A^{T}=A$，则称$A$为是<strong>对称矩阵</strong>。实对称矩阵应用广泛，理论丰富、优美。</p>
<p>定理：实对称矩阵的特征值都是实数<br>证明：设实对称矩阵$A$有$A \mathbf{x}=\lambda \mathbf{x}, \mathbf{x} \neq \mathbf{0}$。则</p>
<script type="math/tex; mode=display">
\left.\begin{array}{l}{\overline{\mathbf{x}}^{T} A \mathbf{x}=\lambda \overline{\mathbf{x}}^{T} \mathbf{x}} \\ {\overline{\mathbf{x}}^{T} A \mathbf{x}=\overline{\mathbf{x}}^{T} \bar{A}^{T} \mathbf{x}=(\overline{A \mathbf{x}})^{T} \mathbf{x}=\bar{\lambda} \overline{\mathbf{x}}^{T} \mathbf{x},}\end{array}\right\} \Longrightarrow(\lambda-\bar{\lambda}) \overline{\mathbf{x}}^{T} \mathbf{x}=0</script><p>因为$\mathrm{x} \neq 0, \overline{\mathrm{x}}^{T} \mathrm{x}&gt;0$，故$\bar{\lambda}=\lambda$，即$\lambda$为实数。</p>
<hr>
<p>定理：实对称矩阵的属于不同特征值的特征向量相互正交<br>证明：设$\lambda$和$\mu$是实对称矩阵$A$的两互异特征值（由前面定理$\lambda, \mu$是实数），$\mathbf{X}, \mathbf{Y}$是相应的特征向量，即$A \mathbf{x}=\lambda \mathbf{x}, A \mathbf{y}=\mu \mathbf{y}$。于是有</p>
<script type="math/tex; mode=display">
\left.\begin{array}{l}{\mathbf{y}^{T} A \mathbf{x}=\lambda \mathbf{y}^{T} \mathbf{x}} \\ {\mathbf{y}^{T} A \mathbf{x}=\mathbf{y}^{T} A^{T} \mathbf{x}=(A \mathbf{y})^{T} \mathbf{x}=\mu \mathbf{y}^{T} \mathbf{x},}\end{array}\right\} \Longrightarrow(\lambda-\mu) \mathbf{y}^{T} \mathbf{x}=0</script><p>而$\lambda \neq \mu$，故$\mathbf{y}^{T} \mathbf{x}=0$</p>
<hr>
<p>定理：设$A$为n阶对称矩阵，则必有正交矩阵$P$，使得$P^{-1} A P=P^{\mathrm{T}} A P=\Lambda$，其中$\mathbf{\Lambda}$是以$A$的n个特征值为对角元的对角矩阵。</p>
<hr>
<p>由前面定理可知，对任何实对称矩阵$A$，有$Q^{T} A Q=\Lambda$，其中$Q=\left(\mathbf{q}<em>{1}, \cdots, \mathbf{q}</em>{n}\right)$为正交矩阵，$\Lambda=\operatorname{diag}\left(\lambda<em>{1}, \cdots, \lambda</em>{n}\right), A \mathbf{q}<em>{i}=\lambda</em>{i} \mathbf{q}_{i}$，于是</p>
<script type="math/tex; mode=display">
A=Q \Lambda Q^{T}=\left(\mathbf{q}_{1}, \cdots, \mathbf{q}_{n}\right)\left(\begin{array}{ccc}{\lambda_{1}} & {} & {} \\ {} & {\ddots} & {} \\ {} & {} & {\lambda_{n}}\end{array}\right)\left(\begin{array}{c}{\mathbf{q}_{1}^{T}} \\ {\vdots} \\ {\mathbf{q}_{n}^{T}}\end{array}\right)</script><p>即有<strong>特征分解</strong>（eigen decomposition）：又称为<strong>谱分解</strong>（Spectral decomposition）</p>
<script type="math/tex; mode=display">
A=\lambda_{1} \mathbf{q}_{1} \mathbf{q}_{1}^{T}+\cdots+\lambda_{n} \mathbf{q}_{n} \mathbf{q}_{n}^{T}</script><p>注记：$P<em>{j}:=\mathbf{q}</em>{j} \mathbf{q}<em>{j}^{T}$为到由特征向量$\mathbf{q}</em>{j}$张成的一维空间的投影矩阵。任意实对称矩阵可以表示为秩1投影矩阵的和。</p>
<hr>
<p>定理：实对称矩阵的正特征值数与正主元数相同（惯性定理）</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（11）：方阵的行列式及其性质</title>
    <url>/2019/12/08/linear_algebra_11/</url>
    <content><![CDATA[<p>清华大学线性代数课程第16，17，18讲：行列式及其性质</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<h2 id="1-行列式的定义"><a href="#1-行列式的定义" class="headerlink" title="1.行列式的定义"></a>1.行列式的定义</h2><p>定义：由n阶方阵<strong>A</strong>的元素所构成的行列式（各元素的位置不变），称为方阵<strong>A</strong>的行列式，记作$\operatorname{det} \mathbf{A}$或$|\mathbf{A}|$。<br>由$\mathbf{A}$确定$|\mathbf{A}|$的这个运算满足下述运算规律（设<strong>A</strong>、<strong>B</strong>为n阶方阵，$\lambda$为数）：<br>（1） $\left|A^{\mathrm{T}}\right|=|A|$ （行列式性质1）；<br>（2） $|\lambda \mathbf{A}|=\lambda^{n}|\mathbf{A}|$；<br>（3） $|A B|=|A||B|$<br>（4） $|A| \neq 0 \Longleftrightarrow A$可逆<br>（5） 若A可逆，则$\operatorname{det}\left(A^{-1}\right)=(\operatorname{det}(A))^{-1}$<br>（6） 若A是一个正交矩阵，则$\operatorname{det} A=\pm 1$<br>（7）$\left|I<em>{n}\right|=\operatorname{det}\left(I</em>{n}\right)=1$<br>注意：$|A+B| \neq|A|+|B|,|k A| \neq k|A|, k \in \mathbb{R}, n \geq 2$，行列式的线性性是针对于行列式某一行某一列的，而不是整个行列式的</p>
<h2 id="2-行列式的性质"><a href="#2-行列式的性质" class="headerlink" title="2.行列式的性质"></a>2.行列式的性质</h2><p>性质1:行列式与它的转置行列式相等，$\left|A^{\mathrm{T}}\right|=|A|$</p>
<p>性质2:对换行列式的两行（列），行列式变号<br>推论：如果行列式有两行（列）完全相同，则此行列式等于零<br>证明：设行列式</p>
<script type="math/tex; mode=display">
D_{1}=\left|\begin{array}{cccc}{b_{11}} & {b_{12}} & {\cdots} & {b_{1 \mathrm{n}}} \\ {b_{21}} & {b_{22}} & {\cdots} & {b_{2 n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {b_{n 1}} & {b_{n 2}} & {\cdots} & {b_{n n}}\end{array}\right|</script><p>对换任意两行（列），有D = -D，故 D=0。</p>
<p>性质3：行列式的某一行（列）中所有的元素都乘同一数k，等于用数k乘此行列式<br>推论：行列式中某一行（列）的所有元素的公因子可以提到行列式记号的外面</p>
<p>性质4: 行列式中如果有两行（列）元素成比例，则此行列式等于零<br>性质5: 若行列式的某一行（列）的元素都是两数之和，例如第i行的元素都是两数之和：</p>
<script type="math/tex; mode=display">
D=\left|\begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{i 1}+a_{i 1}^{\prime}} & {a_{i 2}+a_{i 2}^{\prime}} & {\cdots} & {a_{i n}+a_{i n}^{\prime}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}}\end{array}\right|</script><p>则D等于下列两个行列式之和：</p>
<script type="math/tex; mode=display">
D=\left|\begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{i 1}} & {a_{i 2}} & {\cdots} & {a_{i n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}}\end{array}\right|+\left|\begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{i 1}^{\prime}} & {a_{i 2}^{\prime}} & {\cdots} & {a_{i n}^{\prime}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}}\end{array}\right|</script><p>性质6: 把行列式的某一行（列）的各元素乘同一数然后加到另一行（列）对应的元素上面，行列式不变</p>
<h2 id="3-行列式的几何意义："><a href="#3-行列式的几何意义：" class="headerlink" title="3.行列式的几何意义："></a>3.行列式的几何意义：</h2><p>（1）行列式中的行或列向量所构成的超平行多面体的<strong>有向</strong>面积或体积。<br>（2）坐标系变换下的图形面积或体积的伸缩因子，即变换矩阵A的行列式 detA。</p>
<h2 id="4-行列式按行（列）展开"><a href="#4-行列式按行（列）展开" class="headerlink" title="4.行列式按行（列）展开"></a>4.行列式按行（列）展开</h2><p><strong>定义1</strong>：在n阶行列式中，把$(i, j)$元$a<em>{i j}$所在的第i行和第j列划去后，留下来的n-1阶行列式叫做$(i, j)$元$a</em>{i j}$的<strong>余子式</strong>（complement minor），记作$M_{i j}$；记</p>
<script type="math/tex; mode=display">
A_{i j}=(-1)^{i+j} M_{i j}</script><p>$A<em>{i j}$叫做$(i, j)$元$a</em>{i j}$的<strong>代数余子式</strong>（algebraic complement cofactor）</p>
<hr>
<p><strong>定义2</strong>：行列式$|\mathbf{A}|$的各个元素的代数余子式$A_{i j}$所构成的如下矩阵：</p>
<script type="math/tex; mode=display">
A^{*}=\left(\begin{array}{cccc}{A_{11}} & {A_{21}} & {\cdots} & {A_{n 1}} \\ {A_{12}} & {A_{22}} & {\cdots} & {A_{n 2}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {A_{1 n}} & {A_{2 n}} & {\cdots} & {A_{n n}}\end{array}\right)</script><p>称为矩阵A的<strong>伴随矩阵</strong></p>
<p>有</p>
<script type="math/tex; mode=display">
A A^{*}=A^{*} A=|A| E</script><p>证明：设$A=\left(a<em>{i j}\right)$，记$A A^{*}=\left(b</em>{i j}\right)$，则</p>
<script type="math/tex; mode=display">
b_{i j}=a_{i 1} A_{j 1}+a_{i 2} A_{j 2}+\cdots+a_{i n} A_{j n}=\left\{\begin{array}{cc}{|A|,} & {i=j} \\ {0,} & {i \neq j}\end{array}\right.</script><p>故</p>
<script type="math/tex; mode=display">
A A^{*}=\left(\begin{array}{cccc}{|A|} & {} & {} & {} \\ {} & {|A|} & {} & {} \\ {} & {} & {\ddots} & {} \\ {} & {} & {} & {|A|}\end{array}\right)=|A| E</script><p>类似有</p>
<script type="math/tex; mode=display">
A^{*} A=|A| E</script><p>伴随矩阵的结构讨论：<br>若A是一个n阶矩阵，$A^{*}$ 是$A$的伴随矩阵</p>
<p>（1）当$r(A) = n$，$A$满秩时，$A$可逆，$A^{-1}=\frac{1}{|A|} A^{*}$，故$A^{*}$可逆，$A^{*}$满秩，$r(A^{*}) = n$<br>（2）当$r(A) = n - 1$时，$A$不满秩，$|A|=0$，$A A^{*}=A^{*} A =|A| E=0$，故$A^{*}$的列属于A的零空间，而$\operatorname{dim} N(A)=1$，且存在$A^{*}_{i j} \neq 0$，故$r(A^{*}) = 1$，$A^{*}$为秩一矩阵，可看做一列向量与一行向量之积。<br>（3）$r(A) \leq n-2$，$A$的任意n-1阶子矩阵不可逆，所以$A^{*}$的所有元素都等于0，$A^{*}$为零矩阵</p>
<hr>
<p><strong>定理</strong>：若$|\mathbf{A}| \neq 0$，则矩阵A可逆，且</p>
<script type="math/tex; mode=display">
A^{-1}=\frac{1}{|A|} A^{*}</script><p>其中$A^{*}$为矩阵A的伴随矩阵 </p>
<h2 id="5-行列式的计算"><a href="#5-行列式的计算" class="headerlink" title="5.行列式的计算"></a>5.行列式的计算</h2><p>方法一：</p>
<p><strong>n阶行列式的定义</strong> 设有$n^{2}$个数，排成n行n列的数表</p>
<script type="math/tex; mode=display">
\begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\ {} & {\cdots} & {\cdots} & {\cdots} \\ {a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}}\end{array}</script><p>作出表中位于不同行不同列的n个数的乘积，并冠以符号$(-1)^{t}$，得到形如</p>
<script type="math/tex; mode=display">
(-1)^{t} a_{1 p_{1}} a_{2 p_{2}} \cdots a_{n p_{n}}</script><p>的项，其中$p<em>{1} p</em>{2} \cdots p_{n}$为自然数$1,2, \cdots, n$的一个排列，t为这个排列的逆序数。由于这样的排列共有n！个，因而形如上式的项共有n！项。所有这n！项的代数和</p>
<script type="math/tex; mode=display">
\sum(-1)^{t} a_{1 p_{1}} a_{2 p_{2}} \cdots a_{n p_{n}}</script><p>称为<strong>n阶行列式</strong>，记作</p>
<script type="math/tex; mode=display">
D=\left|\begin{array}{cccc}{a_{11}} & {a_{12}} & {\cdots} & {a_{1 n}} \\ {a_{21}} & {a_{22}} & {\cdots} & {a_{2 n}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{n 1}} & {a_{n 2}} & {\cdots} & {a_{n n}}\end{array}\right|</script><p>简记作$\operatorname{det}\left(a<em>{i j}\right)$，其中数$a</em>{i j}$为行列式D的(i，j)元。</p>
<p>⚠️n = 1时，一阶行列式$|a|=a$，注意不要和绝对值记号混淆</p>
<hr>
<p>方法二：<strong>行列式按行（列）展开法则</strong></p>
<p><strong>定理</strong> 行列式等于它的任一行（列）的各元素与其对应的代数余子式乘积之和，即</p>
<script type="math/tex; mode=display">
D=a_{i 1} A_{i 1}+a_{i 2} A_{i 2}+\cdots+a_{i n} A_{i n} \quad(i=1,2, \cdots, n)</script><p>或</p>
<script type="math/tex; mode=display">
D=a_{1 j} A_{1 j}+a_{2 j} A_{2 j}+\cdots+a_{n j} A_{n j} \quad(j=1,2, \cdots, n)</script><p><strong>推论</strong>: 行列式某一行（列）的元素与另一行（列）的对应元素的代数余子式的乘积之和等于零，即</p>
<script type="math/tex; mode=display">
\begin{array}{l}{a_{i 1} A_{j 1}+a_{i 2} A_{j 2}+\cdots+a_{i n} A_{j n}=0, \quad i \neq j} \\ {a_{1 i} A_{1 j}+a_{2 i} A_{2 j}+\cdots+a_{n i} A_{n j}=0, \quad i \neq j}\end{array}</script><p>证明：《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社 P19</p>
<hr>
<p>典型例题：证明<strong>范德蒙德（Vandermonde）行列式</strong></p>
<script type="math/tex; mode=display">
D_{n}=\left|\begin{array}{cccc}{1} & {1} & {\cdots} & {1} \\ {x_{1}} & {x_{2}} & {\cdots} & {x_{n}} \\ {x_{1}^{2}} & {x_{2}^{2}} & {\cdots} & {x_{n}^{2}} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {x_{1}^{n-1}} & {x_{2}^{n-1}} & {\cdots} & {x_{n}^{n-1}}\end{array}\right|=\prod_{n \geqslant i>j \geqslant 1}\left(x_{i}-x_{j}\right)</script><p>证明：《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社 P18～19</p>
<p>行列式的计算方法技巧：化上（下）三角形法和降阶法</p>
<h2 id="6-克拉默法则-Cramer’s-rule"><a href="#6-克拉默法则-Cramer’s-rule" class="headerlink" title="6.克拉默法则 Cramer’s rule"></a>6.克拉默法则 Cramer’s rule</h2><p>用途：解决方程个数与未知数个数相等并且系数行列式不等于零的线性方程组<br>含有n个未知数$x<em>{1}, x</em>{2}, \cdots, x_{n}$的n个线性方程的方程组</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}=b_{1}} \\ {a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}=b_{2}} \\ {\cdots \cdots \cdots \cdots} \\ {a_{n 1} x_{1}+a_{n 2} x_{2}+\cdots+a_{n n} x_{n}=b_{n}}\end{array}\right.</script><p>它的解可以用n阶行列式表示，即有</p>
<p><strong>克拉默法则</strong>：如果以上线性方程组的系数矩阵$A$的行列式不等于零，即</p>
<script type="math/tex; mode=display">
|A|=\left|\begin{array}{ccc}{a_{11}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {} & {\vdots} \\ {a_{n 1}} & {\cdots} & {a_{n n}}\end{array}\right| \neq 0</script><p>那么，方程组有唯一解</p>
<script type="math/tex; mode=display">
x_{1}=\frac{\left|A_{1}\right|}{|A|}, \quad x_{2}=\frac{\left|A_{2}\right|}{|A|}, \cdots, \quad x_{n}=\frac{\left|A_{n}\right|}{|A|}</script><p>其中$A_{j}(j=1,2, \cdots, n)$是把系数矩阵$A$中第j列的元素用方程组右端的常数项代替后所得到的n阶矩阵，即</p>
<script type="math/tex; mode=display">
A_{j}=\left(\begin{array}{ccccccc}{a_{11}} & {\cdots} & {a_{1, j-1}} & {b_{1}} & {a_{1, j+1}} & {\cdots} & {a_{1 n}} \\ {\vdots} & {} & {\vdots} & {\vdots} & {\vdots} & {} & {\vdots} \\ {a_{n 1}} & {\cdots} & {a_{n, j-1}} & {b_{n}} & {a_{n, j+1}} & {\cdots} & {a_{n n}}\end{array}\right)</script><p>证明：《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社 P45</p>
<h2 id="7-行列式表示向量积和混合积"><a href="#7-行列式表示向量积和混合积" class="headerlink" title="7.行列式表示向量积和混合积"></a>7.行列式表示向量积和混合积</h2><p>下面推导向量积的坐标表示式</p>
<p>设$a=a<em>{x} i+a</em>{y} j+a<em>{z} k, b=b</em>{x} i+b<em>{y} j+b</em>{z} k$，有</p>
<script type="math/tex; mode=display">
\begin{aligned} \mathbf{a} \times \mathbf{b}=&\left(a_{x} \mathbf{i}+a_{y} \mathbf{j}+a_{z} \mathbf{k}\right) \times\left(b_{x} \mathbf{i}+b_{y} \mathbf{j}+b_{z} \mathbf{k}\right) \\=& a_{x} \mathbf{i} \times\left(b_{x} \mathbf{i}+b_{y} \mathbf{j}+b_{z} \mathbf{k}\right)+\\ & a_{y} \mathbf{j} \times\left(b_{x} \mathbf{i}+b_{y} \mathbf{j}+b_{z} \mathbf{k}\right)+a_{z} \mathbf{k} \times\left(b_{x} \mathbf{i}+b_{y} \mathbf{j}+b_{z} \mathbf{k}\right) \\=& a_{x} b_{x}(\mathbf{i} \times \mathbf{i})+a_{x} b_{y}(\mathbf{i} \times \mathbf{j})+a_{x} b_{z}(\mathbf{i} \times \mathbf{k})+\\ & a_{y} b_{x}(\mathbf{j} \times \mathbf{i})+a_{y} b_{y}(\mathbf{j} \times \mathbf{j})+a_{y} b_{z}(\mathbf{j} \times \mathbf{k})+\\ & a_{z} b_{x}(\mathbf{k} \times \mathbf{i})+a_{z} b_{y}(\mathbf{k} \times \mathbf{j})+a_{z} b_{z}(\mathbf{k} \times \mathbf{k}) \end{aligned}</script><p>因为$i \times i=j \times j=k \times k=0, i \times j=k, j \times k=i, k \times i=j, j \times i=-k, k \times j=-i, i \times k=-j$，所以</p>
<script type="math/tex; mode=display">
a \times b=\left(a_{y} b_{z}-a_{z} b_{y}\right) i+\left(a_{z} b_{x}-a_{x} b_{z}\right) j+\left(a_{x} b_{y}-a_{y} b_{x}\right) k</script><p>为了帮助记忆，利用三阶行列式，上式写成</p>
<script type="math/tex; mode=display">
\mathbf{a} \times \mathbf{b}=\left|\begin{array}{ccc}{\mathbf{i}} & {\mathbf{j}} & {\mathbf{k}} \\ {a_{x}} & {a_{y}} & {a_{z}} \\ {b_{x}} & {b_{y}} & {b_{z}}\end{array}\right|</script><hr>
<p>下面来推出三向量的混合积的坐标表示式：<br>设$a=\left(a<em>{x}, a</em>{y}, a<em>{z}\right), b=\left(b</em>{x}, b<em>{y}, b</em>{z}\right), c=\left(c<em>{x}, c</em>{y}, c_{z}\right)$，因为</p>
<script type="math/tex; mode=display">
\mathbf{a} \times \mathbf{b}=\left|\begin{array}{ccc}{\mathbf{i}} & {\mathbf{j}} & {\mathbf{k}} \\ {a_{x}} & {a_{y}} & {a_{z}} \\ {b_{x}} & {b_{y}} & {b_{z}}\end{array}\right|</script><script type="math/tex; mode=display">
=\left|\begin{array}{ll}{a_{y}} & {a_{z}} \\ {b_{y}} & {b_{z}}\end{array}\right| \mathbf{i}-\left|\begin{array}{ll}{a_{x}} & {a_{z}} \\ {b_{x}} & {b_{z}}\end{array}\right| \mathbf{j}+\left|\begin{array}{ll}{a_{x}} & {a_{y}} \\ {b_{x}} & {b_{y}}\end{array}\right| \mathbf{k}</script><p>再按两向量的数量积的坐标表示式，便得$(a \times b) \cdot c$</p>
<script type="math/tex; mode=display">
=c_{x}\left|\begin{array}{ll}{a_{y}} & {a_{z}} \\ {b_{y}} & {b_{z}}\end{array}\right|-c_{y}\left|\begin{array}{ll}{a_{x}} & {a_{z}} \\ {b_{x}} & {b_{z}}\end{array}\right|+c_{z}\left|\begin{array}{ll}{a_{x}} & {a_{y}} \\ {b_{x}} & {b_{y}}\end{array}\right|</script><script type="math/tex; mode=display">
=\left|\begin{array}{lll}{a_{x}} & {a_{y}} & {a_{z}} \\ {b_{x}} & {b_{y}} & {b_{z}} \\ {c_{x}} & {c_{y}} & {c_{z}}\end{array}\right|</script><h2 id="8-行列式与QR分解的联系"><a href="#8-行列式与QR分解的联系" class="headerlink" title="8.行列式与QR分解的联系"></a>8.行列式与QR分解的联系</h2><p>定理：$V^{2}=\operatorname{det}\left(A^{T} A\right)$</p>
<p>V为A的列向量组成的超平行六面体的体积，其中A不一定是方阵，把矩阵A的列向量通过Gram-Schmidt正交化，</p>
<script type="math/tex; mode=display">
A=Q R, R=\left(\begin{array}{cccc}{\left\|e_{1}\right\|} & {*} & {\cdots} & {*} \\ {0} & {\left\|e_{2}\right\|} & {\cdots} & {*} \\ {\vdots} & {\vdots} & {} & {\vdots} \\ {0} & {0} & {\cdots} & {\left\|e_{n}\right\|}\end{array}\right)</script><script type="math/tex; mode=display">
A^{T} A=R^{T}Q^{T} QR=R^{T}R \Longrightarrow \operatorname{det}\left(A^{T} A\right)=\operatorname{det}\left(R^{T} R\right)=[\operatorname{det}(R)]^{2}=\left[\left\|\mathbf{e}_{1}\right\| ……\left\|\mathbf{e}_{n}\right\|\right]^{2}=V^{2}</script><p>个人理解：A如果是方阵，结论就很显然了，因为$\operatorname{det}\left(A^{T} A\right)=[\operatorname{det}(A)]^{2}$，而A的行列式的几何意义就是A的列向量组成的超平行六面体的体积，但是A不一定是方阵，如果A是一个m行n列的矩阵，我们通过证明发现，这个式子依旧是成立的，依旧有$V^{2}=\operatorname{det}\left(A^{T} A\right)$</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（10）：Gram-Schmidt正交化</title>
    <url>/2019/12/06/linear_algebra_10/</url>
    <content><![CDATA[<p>清华大学线性代数课程第十五讲：Gram-Schmidt正交化</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<h2 id="1-正交向量组与正交矩阵"><a href="#1-正交向量组与正交矩阵" class="headerlink" title="1.正交向量组与正交矩阵"></a>1.正交向量组与正交矩阵</h2><p>定理：设$\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{k} \in \mathbb{R}^{n}$是非零的k个向量，满足$\mathbf{v}<em>{i}^{T} \mathbf{v}</em>{j}=0, i \neq j$，则$\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{k}$线性无关。<br>证明：见《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社 P115</p>
<p>定理中$\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{k}$称为<strong>正交向量组（orthogonal vectors）</strong></p>
<p>定义： 如果n阶矩阵<strong>A</strong>满足<strong>$A^{\mathrm{T}} A=E$</strong>，即 <strong>$\left(A^{-1}=A^{\mathrm{T}}\right)$</strong>，那么称<strong>A</strong>为<strong>正交矩阵</strong>。上式用<strong>A</strong>的列向量表示，即是：</p>
<script type="math/tex; mode=display">
\left(\begin{array}{c}{a_{1}^{\mathrm{T}}} \\ {a_{2}^{\mathrm{T}}} \\ {\vdots} \\ {a_{n}^{\mathrm{T}}}\end{array}\right)\left(a_{1}, a_{2}, \cdots, a_{n}\right)=E</script><p>这也就是$n^{2}$个关系式：</p>
<script type="math/tex; mode=display">
a_{i}^{\top} a_{j}=\left\{\begin{array}{l}{1, i=j} \\ {0, i \neq j}\end{array} \quad(i, j=1,2, \cdots, n)\right.</script><p>这就说明：方阵<strong>A</strong>为正交矩阵的充分必要条件是<strong>A</strong>的列向量都是单位向量，且两两正交。<br>因为$A^{T} A=E$与$A A^{\top}=E$等价，所以上述结论对A的行向量亦成立。<br>由此可见，n阶正交矩阵A的n个列（行）向量构成向量空间$\mathbb{R}^{n}$的一个标准正交基。</p>
<p>定义： 若<strong>P</strong>为正交矩阵，则线性变换$y=Px$称为<strong>正交变换</strong>，设$y=Px$为正交变换，则有</p>
<script type="math/tex; mode=display">
\|y\|=\sqrt{y^{\mathrm{T}} y}=\sqrt{x^{\mathrm{T}} P^{\mathrm{T}} P x}=\sqrt{x^{\mathrm{T}} x}=\|x\|</script><p>因此<script type="math/tex">\|y\|=\|x\|</script>说明经正交变换线段长度保持不变。</p>
<h2 id="2-Gram-Schmidt正交化过程"><a href="#2-Gram-Schmidt正交化过程" class="headerlink" title="2.Gram-Schmidt正交化过程"></a>2.Gram-Schmidt正交化过程</h2><p>见《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社 P117-P118</p>
<h2 id="3-QR分解"><a href="#3-QR分解" class="headerlink" title="3.QR分解"></a>3.QR分解</h2><p>Gram-Schmidt正交化把线性无关向量组$a<em>{1}, \cdots, a</em>{r}$导出标准正交向量组$q<em>{1}, \cdots, q</em>{r}$，用矩阵来描述这个过程，其中$A=\left(\mathbf{a}<em>{1}, \mathbf{a}</em>{2}, \cdots,\mathbf{a}<em>{r}\right)$，$Q=\left(\mathbf{q}</em>{1}, \mathbf{q}<em>{2}, \cdots, \mathbf{q}</em>{r}\right)$，$A<em>{m \times n}=Q</em>{m \times n} R_{n \times n}$。</p>
<p>以$\mathbb{R}^{3}$为例：</p>
<script type="math/tex; mode=display">
\left[\begin{array}{lll}{a}_{1} & {a}_{2} & {a}_{3}\end{array}\right]=\left[\begin{array}{lll}{q_{1}} & {q_{2}} & {q_{3}}\end{array}\right]\left[\begin{array}{ccc}{q_{1}^{\mathrm{T}} {a}_{1}} & {q_{1}^{\mathrm{T}} {a}_{2}} & {q_{1}^{\mathrm{T}} {a}_{3}} \\ {} & {q_{2}^{\mathrm{T}} {a}_{2}} & {q_{2}^{\mathrm{T}} {a}_{3}} \\ {} & {} & {q_{3}^{\mathrm{T}} {a}_{3}}\end{array}\right] \quad \text { or } \quad A=Q R</script><p><strong>Q</strong>是正交矩阵，<strong>R</strong>是对角线上为正数的上三角阵。</p>
<p>性质：若<strong>A</strong>是可逆矩阵，则<strong>QR</strong>分解是唯一的<br>证明：设$A=Q<em>{1} R</em>{1}=Q<em>{2} R</em>{2}$为可逆矩阵<strong>A</strong>的两个<strong>QR</strong>分解，则$Q<em>{2}^{-1} Q</em>{1}=R<em>{2} R</em>{1}^{-1}$。$Q<em>{2}^{-1} Q</em>{1}$为正交矩阵，$R<em>{2} R</em>{1}^{-1}$为上三角阵，且对角元素为正。因为正交矩阵的逆矩阵仍为正交矩阵，且是原矩阵的转置，上三角矩阵的转置是下三角矩阵。故原矩阵只能为单位矩阵。故$Q<em>{2}^{-1} Q</em>{1}=R<em>{2} R</em>{1}^{-1}=I_{n}$。</p>
<p>个人理解$A=Q R$其实就是基变换公式，<strong>Q</strong>的列向量为施密特正交化后的标准正交基，其中<strong>R</strong>为过渡矩阵。</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（9）：正交投影与最小二乘法</title>
    <url>/2019/06/04/linear_algebra_9/</url>
    <content><![CDATA[<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课</p>
<span id="more"></span>
<h2 id="正交投影"><a href="#正交投影" class="headerlink" title="正交投影"></a>正交投影</h2><h3 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h3><p>首先复习下Ax=b在行空间中的唯一性，从几何直观上看，Ax=b在A的行空间中的唯一解，实际上是所有解在行空间里面的投影。</p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/屏幕快照%202019-05-23%20上午10.21.08.png" alt="R^n到R^m映射"></p>
<p>定理：若$A \mathbf{x}=\mathbf{b}$有解，则$A \mathbf{x}=\mathbf{b}$在$C\left(A^{T}\right)$中有唯一解</p>
<p>我们首先学习下投影的概念</p>
<h3 id="点在直线上的投影"><a href="#点在直线上的投影" class="headerlink" title="点在直线上的投影"></a>点在直线上的投影</h3><p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/projection.png" alt="点在直线上的投影"></p>
<p>如图所示，求$b$在$a (a \neq 0)$上的投影向量$p$</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}{p+e=b, e \perp \alpha} \\ {p=t \alpha(t \in R)}\end{array}\right.</script><script type="math/tex; mode=display">
\mathbf{e} \perp \mathbf{a} \Longrightarrow \mathbf{a}^{T}(\mathbf{b}-t \mathbf{a})=0 \Longrightarrow t=\frac{\mathbf{a}^{T} \mathbf{b}}{\mathbf{a}^{T} \mathbf{a}}(\mathbf{a} \neq \mathbf{0})</script><p>即$\mathbf{b}$在$\mathbf{a}$上的投影向量为$\left(\frac{\mathbf{a}^{T} \mathbf{b}}{\mathbf{a}^{T} \mathbf{a}}\right) \mathbf{a}=\mathbf{p}$</p>
<script type="math/tex; mode=display">\left(\mathbf{a}^{T} \mathbf{b}\right) \mathbf{a}=\mathbf{a}\left(\mathbf{a}^{T} \mathbf{b}\right)=\left(\mathbf{a} \mathbf{a}^{T}\right) \mathbf{b}</script><p>因此，$\mathbf{p}=\left(\frac{\mathbf{a} \mathbf{a}^{T}}{\mathbf{a}^{T} \mathbf{a}}\right) \mathbf{b}$</p>
<p>其中$S=\frac{\mathbf{a} \mathbf{a}^{T}}{\mathbf{a}^{T} \mathbf{a}}$称为投影矩阵，$S$满足$S^{2}=S, S^{T}=S$<br>$\forall \mathbf{b} \in \mathbb{R}^{2}$，$S \mathbf{b}$是$\mathbf{b}$在$\mathbf{a}$上的投影向量</p>
<h3 id="点在平面上的投影"><a href="#点在平面上的投影" class="headerlink" title="点在平面上的投影"></a>点在平面上的投影</h3><p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/projection2.png" alt="点在直线上的投影"></p>
<p>现在我们考虑点在平面上的投影。给定$\mathbf{v}=\left(v<em>{1}, v</em>{2}, v<em>{3}\right)^{T} \in \mathbb{R}^{3}$，平面$\pi : a x+b y+c z=0$。设$\mathbf{P}$是$\mathbf{v}$在$\pi$上的投影，求$\mathbf{P}$。<br>令$\alpha</em>{1}, \alpha<em>{2}$是平面$\pi$上的两无关向量，即$a x+b y+c z=0$的基础解系或$N((a, b, c))$的一组基。<br>令$A=\left(\alpha</em>{1}, \alpha<em>{2}\right)$，则平面$\pi=C(A)$，求投影$\mathbf{p} \Longleftrightarrow$求$\mathbf{v}$关于$\mathbb{R}^{3}=C(A)+N\left(A^{T}\right)$的分解$\mathbf{v}</em>{l}+\mathbf{v}<em>{l n}$，其中$\mathbf{v}</em>{l}=\mathbf{p}, \mathbf{v}_{l n}=\mathbf{e} \in N\left(A^{T}\right)$</p>
<script type="math/tex; mode=display">
\begin{array}{l}{\mathbf{p}=\mathbf{v}_{l} \in C(A) \Longleftrightarrow \exists \hat{\mathbf{x}}, A \hat{\mathbf{x}}=\mathbf{p}} \\ {\mathbf{e}=\mathbf{v}-\mathbf{p} \perp \pi \Longrightarrow A^{T}(A \hat{\mathbf{x}}-\mathbf{v})=\mathbf{0}}\end{array}</script><p>即$\hat{\mathbf{x}}$是$A^{T} A \mathbf{x}=A^{T} \mathbf{v}$的解。$A^{T} A$是可逆矩阵（A列满秩）$\Longrightarrow \hat{\mathbf{x}}=\left(A^{T} A\right)^{-1} A^{T} \mathbf{v}$，则$\mathbf{p}=A\left(A^{T} A\right)^{-1} A^{T} \mathbf{v}$。<br>此时，$A\left(A^{T} A\right)^{-1} A^{T}$为投影矩阵</p>
<p>一般地，一个矩阵$P$满足$P^{2}=P, P^{T}=P$，则称$P$为投影矩阵</p>
<p>定理：若$P$是一个投影矩阵，则$C(P)=N(I-P), N(P)=C(I-P)$<br>证明：从P(I-P)=(I-P)P=0思考</p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>回到解方程组$A<em>{m \times n} \mathbf{x}=\mathbf{b}$<br>$A \mathbf{x}=\mathbf{b}$有解$\Longleftrightarrow \mathbf{b} \in C(A)$<br>假设它无解，则$\mathbf{b} \notin C(A)$，此时问题转化为：<br>求$\hat{\mathbf{x}}$使得$\Vert A \hat{\mathbf{x}}-\mathbf{b}\Vert$最小，即$\min </em>{\mathbf{x} \in \mathbb{R}^{n}}\Vert A \hat{\mathbf{x}}-\mathbf{b}\Vert$的最小值点。<br>$\hat{\mathbf{x}}$为最小二乘解（the least square solution）<br>$A^{T} A \hat{\mathbf{x}}=A^{T} \mathbf{b}$称为法方程组（normal equations）</p>
<p>性质：<br>1.法方程组总有解（无论A是否列满秩），这是因为$C\left(A^{T}\right)=C\left(A^{T} A\right), A^{T} \mathbf{b} \in C\left(A^{T}\right)=C\left(A^{T} A\right)$<br>2.$A^{T} A \hat{\mathbf{x}}=A^{T} \mathbf{b}$的解可能有无穷个，但$A \hat{\mathbf{x}}$（投影p）唯一。<br>3.若A列满秩，则$A^{T} A$可逆，$\hat{\mathbf{x}}=\left(A^{T} A\right)^{-1} A^{T} \mathbf{b}$</p>
<h3 id="最小二乘法的应用：曲线拟合"><a href="#最小二乘法的应用：曲线拟合" class="headerlink" title="最小二乘法的应用：曲线拟合"></a>最小二乘法的应用：曲线拟合</h3><p>给定数据$\left{\left(x<em>{1}, y</em>{1}\right), \cdots,\left(x<em>{N}, y</em>{N}\right)\right}$<br>寻求直线$y=C+D x$，使得误差</p>
<script type="math/tex; mode=display">E(C, D)=\left[y_{1}-\left(C+D x_{1}\right)\right]^{2}+\cdots+\left[y_{N}-\left(C+D x_{N}\right)\right]^{2}</script><p>最小，即使向量：</p>
<script type="math/tex; mode=display">\left(\begin{array}{c}{y_{1}-\left(C+D x_{1}\right)} \\ {\vdots} \\ {y_{N}-\left(C+D x_{N}\right)}\end{array}\right)</script><p>的长度最小。<br>令</p>
<script type="math/tex; mode=display">A=\left(\begin{array}{cc}{1} & {x_{1}} \\ {\vdots} & {\vdots} \\ {1} & {x_{N}}\end{array}\right), \mathbf{b}=\left(\begin{array}{c}{y_{1}} \\ {\vdots} \\ {y_{N}}\end{array}\right), \hat{\mathbf{x}}=\left(\begin{array}{c}{\hat{C}} \\ {\hat{D}}\end{array}\right)</script><p>即求$\hat{\mathbf{x}}$使得$\Vert A \hat{\mathbf{x}}-\mathbf{b}\Vert$最小。解法方程组$A^{T} A \hat{\mathbf{x}}=A^{T} \mathbf{b}$，即</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cc}{N} & {\sum_{i=1}^{N} x_{i}} \\ {\sum_{i=1}^{N} x_{i}} & {\sum_{i=1}^{N} x_{i}^{2}}\end{array}\right)\left(\begin{array}{l}{\hat{C}} \\ {\hat{D}}\end{array}\right)=\left(\begin{array}{c}{\sum_{i=1}^{N} y_{i}} \\ {\sum_{i=1}^{N} x_{i} y_{i}}\end{array}\right)</script><p>令$\overline{x}=\frac{1}{N} \sum<em>{i=1}^{N} x</em>{i}, \overline{y}=\frac{1}{N} \sum<em>{i=1}^{N} y</em>{i}$，求得$\hat{C}=\overline{y}-\hat{D} \overline{x}, \hat{D}=\frac{x<em>{1} y</em>{1}+\cdots+x<em>{N} y</em>{N}-N \overline{x} \overline{y}}{x<em>{1}^{2}+\cdots+x</em>{N}^{2}-N \overline{x}^{2}}$<br>直线$y=\hat{C}+\hat{D} x$称为最小二乘直线。</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（8）：基和维数，四个基本子空间及其正交性</title>
    <url>/2019/05/07/linear_algebra_8/</url>
    <content><![CDATA[<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课</p>
<span id="more"></span>
<h2 id="基和维数"><a href="#基和维数" class="headerlink" title="基和维数"></a>基和维数</h2><p>$\left{\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{n}\right}$是$V$的一个基（basis）<br>（1）$\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{n}$线性无关<br>（2）$\forall \alpha \in V, \alpha$是$\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{n}$的线性组合<br>我们说$V$的维数（dimension）是$n =$基中向量个数</p>
<p>定理：设$V$是一个向量空间，$\left{\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{n}\right}$是一组基，$\left{\mathbf{w}<em>{1}, \cdots, \mathbf{w}</em>{m}\right}$是另一组基，则$m=n$</p>
<p>命题：$\mathbb{R}^{n}$中任意$n+1$个向量线性相关</p>
<p>性质：设$\mathbf{v}<em>{1}, \cdots, \mathbf{v}</em>{n}$是$\mathbb{R}^{n}$的一组基，$A$是一个$n \times n$可逆矩阵，则$A \mathbf{v}<em>{1}, \cdots, A \mathbf{v}</em>{n}$也是$\mathbb{R}^{n}$的一组基。</p>
<h3 id="关于秩的不等式"><a href="#关于秩的不等式" class="headerlink" title="关于秩的不等式"></a>关于秩的不等式</h3><p>(1) $r(A)=r\left(A^{T}\right)$<br>(2) $r(A B) \leq \min (r(A), r(B))$<br>(3) $r(A+B) \leq r(A)+r(B)$</p>
<h2 id="四个基本子空间"><a href="#四个基本子空间" class="headerlink" title="四个基本子空间"></a>四个基本子空间</h2><p>设A是一个$m \times n$阶矩阵，考虑：<br>列空间（column space）$C(A)=\left{\mathbf{y} \in \mathbb{R}^{m} | \mathbf{y}=A \mathbf{x}, \exists \mathbf{x} \in \mathbb{R}^{n}\right}$ $dim = r$<br>行空间（row space）$C\left(A^{T}\right)=\left{\mathbf{y} \in \mathbb{R}^{n} | \mathbf{y}=A^{T} \mathbf{x}, \exists \mathbf{x} \in \mathbb{R}^{m}\right}$ $dim = r$<br>零空间（null space）$N(A)=\left{\mathbf{x} \in \mathbb{R}^{n} | A \mathbf{x}=\mathbf{0}\right}$ $dim = n-r$<br>左零空间（left null space）$N\left(A^{T}\right)=\left{\mathbf{x} \in \mathbb{R}^{m} | A^{T} \mathbf{x}=\mathbf{0}\right} =\left{\mathrm{x} \in \mathbb{R}^{m} | \mathrm{x}^{T} A=0\right}$ $dim = m-r$</p>
<h3 id="维数公式"><a href="#维数公式" class="headerlink" title="维数公式"></a>维数公式</h3><p>设$V$是一个向量空间，$W<em>{1}, W</em>{2}$是两个子空间，则$W<em>{1} \cap W</em>{2}$和$W<em>{1}+W</em>{2}$是$V$的子空间，但$W<em>{1} \cup W</em>{2}$一般不是子空间</p>
<p>这些空间的维数有如下关系：<br>$\operatorname{dim} W<em>{1}+\operatorname{dim} W</em>{2}=\operatorname{dim}\left(W<em>{1} \cap W</em>{2}\right)+\operatorname{dim}\left(W<em>{1}+W</em>{2}\right)$</p>
<h2 id="四个子空间的正交性"><a href="#四个子空间的正交性" class="headerlink" title="四个子空间的正交性"></a>四个子空间的正交性</h2><p>定义：设$S$和$T$是$\mathbb{R}^{n}$的两个子空间（subspaces），我们说$S$垂直（perpendicular）于$T$，如果对于$\forall \mathbf{v} \in S, \mathbf{w} \in T, \mathbf{v}^{T} \mathbf{w}=0$，记作$S \perp T$或$T \perp S$。我们也可以说：$S$和$T$是正交（orthogonal）的。</p>
<p>定理：设$A$是$m \times n$阶矩阵，则$C(A)$和$N\left(A^{T}\right)$正交，$C\left(A^{T}\right)$和$N(A)$正交。</p>
<p>四个子空间还有如下的关系：<br>$C\left(A^{T}\right)+N(A)=\mathbb{R}^{n}, C\left(A^{T}\right) \perp N(A)$<br>$C(A)+N\left(A^{T}\right)=\mathbb{R}^{m}, C(A) \perp N\left(A^{T}\right)$</p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/正交补空间.png" alt="正交补空间Orthogonal complement space"></p>
<p>正交补定义：设$V \subset \mathbb{R}^{n}$是一个子空间，$V$在$\mathbb{R}^{n}$中的正交补定义为集合$\left{\mathbf{w} \in \mathbb{R}^{n} | \mathbf{v}^{T} \mathbf{w}=0, \forall \mathbf{v} \in V\right}$，记作$V^{\perp}$</p>
<h3 id="Ax-b在行空间中的唯一性"><a href="#Ax-b在行空间中的唯一性" class="headerlink" title="Ax=b在行空间中的唯一性"></a>Ax=b在行空间中的唯一性</h3><p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/屏幕快照%202019-05-23%20上午10.21.08.png" alt="R^n到R^m映射"></p>
<p>定理：若$A \mathbf{x}=\mathbf{b}$有解，则$A \mathbf{x}=\mathbf{b}$在$C\left(A^{T}\right)$中有唯一解</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（7）：向量空间</title>
    <url>/2019/05/02/linear_algebra_7/</url>
    <content><![CDATA[<p>复习线性代数：向量空间</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课</p>
<span id="more"></span>
<h2 id="向量空间和子空间"><a href="#向量空间和子空间" class="headerlink" title="向量空间和子空间"></a>向量空间和子空间</h2><p>定义：设$V \subset \mathbb{R}^{n}$是一些$n$维列向量的集合，且$V$关于向量加法和数乘封闭$\forall \alpha, \beta \in V, \forall c<em>{1}, c</em>{2} \in \mathbb{R} \Longrightarrow c<em>{1} \alpha+c</em>{2} \beta \in V$。则称$V$为一个向量空间（vector space）</p>
<p>性质：零向量属于向量空间$V$</p>
<p>更广义定义：<br>一个实向量空间（real vector space）是“向量”的集合，其关于加法和（实数的）数乘封闭（即线性组合封闭）且满足1.7节的八条性质</p>
<p>例如：<br>1.$M_{n}(\mathbb{R})={n阶实矩阵}$<br>2.$V=\left{y=f(x) | y^{\prime \prime \prime \prime}=0\right}$</p>
<p>子空间：设$V$是一个向量空间，$W \subset V$，若$W$关于$V$的加法、数乘封闭，则$W$是$V$的一个子空间（subspace）</p>
<h2 id="列空间和零空间"><a href="#列空间和零空间" class="headerlink" title="列空间和零空间"></a>列空间和零空间</h2><p>关于$A \mathrm{x}=\mathrm{b}$，相关联的有两类（子）空间<br>$A$的列向量的全部线性组合，称为$A$的列空间（column space），记作$C(A)$，$C(A)=\left{\mathbf{y} \in \mathbb{R}^{m} | \mathbf{y}=A \mathbf{x}, \mathbf{x} \in \mathbb{R}^{n}\right}$，是$\mathbb{R}^{m}$的子空间。</p>
<p>定理：$A \mathbf{x}=\mathbf{b}$有解$\Longleftrightarrow \mathbf{b} \in C(A)$<br>例如，$A \mathbf{x}=\mathbf{0}$总有解，因为$0 \in C(A)$</p>
<p>另一类子空间：零空间$N(A)=\left{\mathbf{x} \in \mathbb{R}^{n} | A \mathbf{x}=\mathbf{0}\right}$是$\mathbb{R}^{n}$的子空间。<br>注意：$A \mathrm{x}=\mathrm{b} \neq 0$的解集不是一个空间</p>
<h2 id="阶梯形（row-echelon-form）"><a href="#阶梯形（row-echelon-form）" class="headerlink" title="阶梯形（row-echelon form）"></a>阶梯形（row-echelon form）</h2><p>定理：设$A$是一个$m \times n$阶矩阵，则只经过行变换，$A$可化成一个行阶梯形矩阵$U$，最终化成行最简约阶梯形矩阵$U_{0}$（reduced row echelon form），即消去主元所在列的其余元素，且主元化为1。<br>主元所在列称为主列（pivot column），主元对应变量称为主变量（pivot variable）。其余列称为自由列（free column）。自由变量一一对应$n-r$个解向量称为基础解系</p>
<p>注意：行阶梯形矩阵$U$一般不唯一，行最简约阶梯形矩阵$U_{0}$是唯一的</p>
<h3 id="求解齐次线性方程组"><a href="#求解齐次线性方程组" class="headerlink" title="求解齐次线性方程组"></a>求解齐次线性方程组</h3><p>简化行阶梯阵$U_{0}$可以通过列变换化成如下形式：</p>
<script type="math/tex; mode=display">R=\left( \begin{array}{ll}{I} & {F} \\ {0} & {0}\end{array}\right)</script><p>总结：</p>
<script type="math/tex; mode=display">A \stackrel{E}{\longrightarrow} U_{0} \stackrel{P}{\longrightarrow} R=\left( \begin{array}{cc}{I_{r}} & {F} \\ {0} & {0}\end{array}\right)</script><p>即$R=E A P$</p>
<p>注意：$A x=0$与$R y=0$的解的关系：<br>若$x \in N(A)$，则$P^{-1} x \in N(R)$<br>$y \in N(R) \Rightarrow P y \in N(A)$</p>
<p>求$R_{\mathrm{X}}=0$的解，考虑：</p>
<script type="math/tex; mode=display">
N=\left( \begin{array}{c}{-F} \\ {I_{n-r}}\end{array}\right)</script><p>称为零空间矩阵，null space matrix，$R N=0$展示$N$的每一列均是$R \mathrm{x}=0$的解，这$n-r$个列向量的全体线性组合为$R \mathrm{x}=0$的解集$N(R)$，即$C(N)=N(R)$</p>
<h3 id="求解非齐次线性方程组"><a href="#求解非齐次线性方程组" class="headerlink" title="求解非齐次线性方程组"></a>求解非齐次线性方程组</h3><p>求解一般线性方程组$A \mathbf{x}=\mathbf{b}$<br>设$\mathbf{x}^{*}$是$A \mathbf{x}=\mathbf{b}$的一特解，则 $\mathbf{x}^{*}+N(A)$是方程组的全部解</p>
<h3 id="解的一般性的讨论"><a href="#解的一般性的讨论" class="headerlink" title="解的一般性的讨论"></a>解的一般性的讨论</h3><p>设$A$是一个$m \times n$阶矩阵，$r$为$A$的秩，若$r=n$，则称$A$是一个列满秩矩阵（matrix of full column rank）。若$r=m$，则称$A$是一个行满秩矩阵（matrix of full row rank）</p>
<p>Case1：$r=n=m$</p>
<p>$A$可逆，$A \mathbf{x}=\mathbf{b}$有唯一解$\mathrm{x}=A^{-1} \mathrm{b}$</p>
<script type="math/tex; mode=display">R=I_{n}</script><p>Case2：$r=n&lt;m$列满秩</p>
<p>$A \mathbf{x}=\mathbf{0}$只有零解，$A \mathbf{x}=\mathbf{b}$无解或有唯一解（特解）</p>
<script type="math/tex; mode=display">
R=\left( \begin{array}{ll}{I_{n}} \\ {0}\end{array}\right)</script><p>Case3：$r=m&lt;n$行满秩</p>
<p>$A \mathbf{x}=\mathbf{b}$有无穷多个解</p>
<script type="math/tex; mode=display">
R=\left(I_{r} | F\right)</script><p>Case4：$r&lt;m,r&lt;n$</p>
<p>$A \mathbf{x}=\mathbf{b}$若有解，则有无穷多个解</p>
<script type="math/tex; mode=display">
R= \left( \begin{array}{cc}{I_{r}} & {F} \\ {0} & {0}\end{array}\right)</script>]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（6）：LU分解</title>
    <url>/2019/04/26/linear_algebra_6/</url>
    <content><![CDATA[<p>复习线性代数：LU分解</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课</p>
<span id="more"></span>
<h1 id="LU分解"><a href="#LU分解" class="headerlink" title="LU分解"></a>LU分解</h1><p>回忆消元法的过程：利用初等行变换，方阵$A \longrightarrow$上三角矩阵$U$，使用矩阵语言：$E A=U$，$E$是初等矩阵乘积。<br>$A=E^{-1} U$，将矩阵$A$分解成一个下三角矩阵（lower triangular matrix）和一个上三角矩阵的乘积（upper triangular matrix），$U$为上三角矩阵，对角元为$A$的主元。$L$为下三角矩阵，对角元为$1$，乘数$l_{ij}$位于对角元下方。有时，可以把$U$写成$DU$的形式$A=LDU$，$D$是对角矩阵。</p>
<h1 id="Gauss消元法的计算量"><a href="#Gauss消元法的计算量" class="headerlink" title="Gauss消元法的计算量"></a>Gauss消元法的计算量</h1><p>含乘除法次数：$\frac{n^{3}}{3}+n^{2}-\frac{n}{3} \approx \frac{n^{3}}{3}$<br>加减法次数：$\frac{n^{3}}{3}+\frac{n^{2}}{2}-\frac{5 n}{6} \approx \frac{n^{3}}{3}$</p>
<h1 id="LU分解的存在性和唯一性"><a href="#LU分解的存在性和唯一性" class="headerlink" title="LU分解的存在性和唯一性"></a>LU分解的存在性和唯一性</h1><p>定理：设可逆矩阵$A$的顺序主子阵$A_{k}(k=1, \cdots, n)$均为可逆阵，则$A$有$LU$分解。</p>
<p>定理：设$n$阶可逆阵$A$有$A=LU$，其中$L$为下三角矩阵，$U$为上三角矩阵，且$l<em>{i i}=1, u</em>{i i} \neq 0(1 \leq i \leq n)$，则分解唯一。<br>证明：设可逆阵$A$有两个$LU$分解：$A=L<em>{1} U</em>{1}=L<em>{2} U</em>{2}$，则</p>
<script type="math/tex; mode=display">L_{1}^{-1} L_{2}=U_{1} U_{2}^{-1}</script><p>为对角阵。因$L<em>{1}, L</em>{2}$的对角元为$1$，故$L<em>{1}^{-1} L</em>{2}$对角元全为$1$。故$L<em>{1}^{-1} L</em>{2}=U<em>{1} U</em>{2}^{-1}=I$，即$L<em>{1}=L</em>{2}, U<em>{1}=U</em>{2}$。<br>同理，设可逆矩阵$A=L D U$，则分解唯一。</p>
<h1 id="对称矩阵的-L-D-L-T-分解"><a href="#对称矩阵的-L-D-L-T-分解" class="headerlink" title="对称矩阵的$L D L^{T}$分解"></a>对称矩阵的$L D L^{T}$分解</h1><p>设可逆对称矩阵$A$不需换行，只通过消元能化成上三角矩阵$U$，即有$A=L D U$，则$A=A^{T}=U^{T} D L^{T}$。由$A=L D U$分解唯一性知$U=L^{T}$。故$A=L D L^{T}$。</p>
<h1 id="置换矩阵permutation-matrix"><a href="#置换矩阵permutation-matrix" class="headerlink" title="置换矩阵permutation matrix"></a>置换矩阵permutation matrix</h1><p>定义：单位矩阵行重排后得到的矩阵称为置换矩阵<br>共有$n!$个$n$阶置换阵<br>置换阵的逆还是置换阵，置换阵的乘积仍是置换阵<br>置换阵$P$满足$P^{-1}=P^{T}$</p>
<h1 id="P-A-L-U-分解"><a href="#P-A-L-U-分解" class="headerlink" title="$P A=L U$分解"></a>$P A=L U$分解</h1><p>定理：设$A$是一个$n$阶可逆阵，则存在置换阵$P$，使得$P A=L U$。</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（5）：矩阵的逆</title>
    <url>/2019/04/14/linear_algebra_5/</url>
    <content><![CDATA[<p>复习线性代数<strong>逆矩阵</strong>部分</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课</p>
<span id="more"></span>
<h2 id="可逆矩阵的定义"><a href="#可逆矩阵的定义" class="headerlink" title="可逆矩阵的定义"></a>可逆矩阵的定义</h2><p>定义：对<strong>方阵</strong>$A$，若存在矩阵$B$，满足$A B=B A=I$，则称$A$是可逆的（invertible），称$B$是$A$的逆矩阵（inverse matrix），记作$A^{-1}$。<br>不可逆矩阵也称为奇异矩阵（singular matrix），而可逆矩阵也称为非奇异矩阵（nonsingular matrix）。</p>
<h2 id="矩阵可逆的性质"><a href="#矩阵可逆的性质" class="headerlink" title="矩阵可逆的性质"></a>矩阵可逆的性质</h2><p>（1）若方阵$A$满足$A B=I, C A=I$，则$B=C$。特别的，方阵的逆唯一<br>证明：$C=C I=C(A B)=(C A) B=I B=B$<br>（2）若$A$可逆，则$A \mathbf{x}=\mathbf{b}$有唯一解$\mathbf{x}=A^{-1} \mathbf{b}$<br>（3）$A \mathbf{x}=0$有非零解$\Longleftrightarrow A$不可逆，$A$可逆$\Longleftrightarrow A \mathbf{x}=0$只有零解<br>定理：<br>（1）若$A$是可逆矩阵，则$A^{-1}$也可逆，且$\left(A^{-1}\right)^{-1}=A$<br>（2）若n阶方阵$A$和$B$都可逆，则$A B$可逆，且$(A B)^{-1}=B^{-1} A^{-1}$<br>（3）若$A$可逆，则$A^{T}$也可逆，且$\left(A^{T}\right)^{-1}=\left(A^{-1}\right)^{T}$<br>证明：$A^{T}\left(A^{-1}\right)^{T}=\left(A^{-1} A\right)^{T}=I^{T}=I,\left(A^{-1}\right)^{T} A^{T}=\left(A A^{-1}\right)^{T}=I^{T}=I$</p>
<p>注意⚠️：$(A+B)^{-1}=A^{-1}+B^{-1}$是错的</p>
<h2 id="Gauss-Jordan消元法求矩阵-A-的逆"><a href="#Gauss-Jordan消元法求矩阵-A-的逆" class="headerlink" title="Gauss-Jordan消元法求矩阵$A$的逆"></a>Gauss-Jordan消元法求矩阵$A$的逆</h2><p>设$A$可逆，则通过初等行变换$\left(A \vert I<em>{n}\right) \longrightarrow \left(I</em>{n} \vert A^{-1}\right)$</p>
<p>由Gauss-Jordan消元法求逆矩阵的过程知：<br>设矩阵$A$可逆，则$A$可经过一系列初等行变换化成单位矩阵$I$。因此有初等矩阵$E<em>{1}, E</em>{2}, \cdots, E_{k}$使得</p>
<script type="math/tex; mode=display">E_{k} \cdots E_{2} E_{1} A=I</script><p>故</p>
<script type="math/tex; mode=display">A^{-1}=E_{k} \cdots E_{2} E_{1}</script><script type="math/tex; mode=display">A=\left(E_{k} \cdots E_{2} E_{1}\right)^{-1}=E_{1}^{-1} E_{2}^{-1} \cdots E_{k}^{-1}</script><p>$A$可逆$\longleftrightarrow$ $A$可表示成一系列初等矩阵的乘积</p>
<script type="math/tex; mode=display">
\underbrace{E_{k} \cdots E_{1}}_{A^{-1}}(A \vert I)=\left(I \vert A^{-1}\right)</script><h2 id="矩阵可逆与主元个数"><a href="#矩阵可逆与主元个数" class="headerlink" title="矩阵可逆与主元个数"></a>矩阵可逆与主元个数</h2><p>定理：$n$阶矩阵$A$可逆$\longleftrightarrow$ $A$有$n$个主元</p>
<h2 id="下三角矩阵的逆"><a href="#下三角矩阵的逆" class="headerlink" title="下三角矩阵的逆"></a>下三角矩阵的逆</h2><p>主对角线下（上）方元素全为零的方阵称为上（下）三角矩阵<br>定理：两个$n$阶下（上）三角矩阵$A$和$B$的乘积仍为下（上）三角矩阵，且$AB$的主对角元等于$A$与$B$的相应主对角元的乘积。</p>
<p>定理：<br>下三角矩阵可逆$\longleftrightarrow$主对角元素都非零<br>可逆下三角矩阵的逆也是下三角阵<br>若原矩阵对角元素都是1，则逆的对角元也都是1</p>
<h2 id="分块矩阵的消元和逆"><a href="#分块矩阵的消元和逆" class="headerlink" title="分块矩阵的消元和逆"></a>分块矩阵的消元和逆</h2><p>分块矩阵的初等行变换：<br>1，把一个块行减去另一个块行左乘以$P$</p>
<script type="math/tex; mode=display">
\left( \begin{array}{cc}{I} & {0} \\ {-P} & {I}\end{array}\right) \left( \begin{array}{cc}{A} & {B} \\ {C} & {D}\end{array}\right)=\left( \begin{array}{cc}{A} & {B} \\ {C-P A} & {D-P B}\end{array}\right)</script><p>2，两个块行互换位置</p>
<script type="math/tex; mode=display">
\left( \begin{array}{ll}{0} & {I} \\ {I} & {0}\end{array}\right) \left( \begin{array}{ll}{A} & {B} \\ {C} & {D}\end{array}\right)=\left( \begin{array}{ll}{C} & {D} \\ {A} & {B}\end{array}\right)</script><p>3，用一个可逆矩阵左乘某一块行</p>
<script type="math/tex; mode=display">
\left( \begin{array}{cc}{P} & {0} \\ {0} & {I}\end{array}\right) \left( \begin{array}{cc}{A} & {B} \\ {C} & {D}\end{array}\right)=\left( \begin{array}{cc}{P A} & {P B} \\ {C} & {D}\end{array}\right)</script><p>类似有分块矩阵的初等列变换，则需要用矩阵作右乘</p>
<p>注意：分块矩阵$A,B$可乘$\longleftrightarrow$ $A$的列的划分与$B$的行的划分一致</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（4）：矩阵乘法的性质，方幂和转置</title>
    <url>/2019/04/13/linear_algebra_4/</url>
    <content><![CDATA[<p>复习线性代数第四章：矩阵乘法的性质，方幂和转置</p>
<p>参考资料：<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang 麻省理工视频公开课程</p>
<span id="more"></span>
<p>关于矩阵的基础运算：<strong>加法，数乘，矩阵乘法，分块矩阵（block matrix）</strong>等内容，由于内容比较基础，就略过不予赘述。</p>
<h2 id="矩阵乘法的性质"><a href="#矩阵乘法的性质" class="headerlink" title="矩阵乘法的性质"></a>矩阵乘法的性质</h2><p>1，<strong>矩阵乘法不满足乘法交换律</strong>，设$A$为$m \times n$矩阵，$B$为$n \times p$矩阵。$A$和$B$可做乘法，但$B$与$A$未必可做乘法。即使$A$与$B$，$B$与$A$都可做乘法，也有可能$A B \neq B A$，若碰巧$A B = B A$，称A和B<strong>可交换</strong>。<br>2，<strong>消去律对矩阵乘法不成立</strong>，即若$A B = A C$，不能断定$B=C$<br>3，若$AB=0$，不能断定$A=0$或$B=0$</p>
<p>分块矩阵的乘法：<br>分块矩阵$A,B$可乘$\longleftrightarrow$ $A$的列的划分与$B$的行的划分一致</p>
<h2 id="矩阵的方幂"><a href="#矩阵的方幂" class="headerlink" title="矩阵的方幂"></a>矩阵的方幂</h2><p>设$A$是$n \times n$矩阵，$p$是正整数，则$A^{p}=\underbrace{A \cdots A}<em>{p 个}$ 称为矩阵$A$的$p$次幂。规定$A^{0}=I</em>{n}$。</p>
<script type="math/tex; mode=display">A^{p} \cdot A^{q}=A^{p+q},\left(A^{p}\right)^{q}=A^{p q}</script><p>注意：由于矩阵乘法不满足交换律。所以一般地</p>
<script type="math/tex; mode=display">(A B)^{p} \neq A^{p} B^{p}，(A+B)^{2} \neq A^{2}+2 A B+B^{2}</script><h3 id="分块矩阵的计算机意义"><a href="#分块矩阵的计算机意义" class="headerlink" title="分块矩阵的计算机意义"></a>分块矩阵的计算机意义</h3><p>当矩阵太大时，不适于存储在高速计算机内存中，分块矩阵允许计算机一次处理几块子矩阵，当把矩阵分块后再进行矩阵计算会更有效。</p>
<h2 id="矩阵的转置"><a href="#矩阵的转置" class="headerlink" title="矩阵的转置"></a>矩阵的转置</h2><p>将$m \times n$矩阵$A=\left(a<em>{i j}\right)</em>{m \times n}$的行与列互换，得到的矩阵$\left(a<em>{j i}\right)</em>{n \times m}$称为$A$的转置（transpose），记为$A^{T}$。</p>
<p>性质：<br>（1）$\left(A^{T}\right)^{T}=A$<br>（2）$(A+B)^{T}=A^{T}+B^{T}$<br>（3）对任意数$k$,$(k A)^{T}=k A^{T}$<br>（4）$(A B)^{T}=B^{T} A^{T}$<br>$(A B)^{T}=B^{T} A^{T}$的证明：<br>设$A$为$m \times n$矩阵，$B$为$n \times p$矩阵</p>
<script type="math/tex; mode=display">
\left((A B)^{T}\right)_{i j}=(A B)_{j i}=\sum_{k=1}^{n} a_{j k} b_{k i}</script><script type="math/tex; mode=display">
\left(B^{T} A^{T}\right)_{i j}=\sum_{k=1}^{n}\left(B^{T}\right)_{i k}\left(A^{T}\right)_{k j}=\sum_{k=1}^{n} b_{k i} a_{j k}</script><p>故$(A B)^{T}=B^{T} A^{T}$</p>
<script type="math/tex; mode=display">
\longrightarrow\left(A_{1} A_{2} \cdots A_{k}\right)^{T}=A_{k}^{T} \cdots A_{2}^{T} A_{1}^{T}</script><h3 id="矩阵转置的应用：内积"><a href="#矩阵转置的应用：内积" class="headerlink" title="矩阵转置的应用：内积"></a>矩阵转置的应用：内积</h3><p>设$\mathbf{x}, \mathbf{y}$为两$n$维列向量，则$\mathbf{x} \cdot \mathbf{y}=\mathbf{x}^{T} \mathbf{y}=\mathbf{y}^{T} \mathbf{x}$</p>
<h3 id="对称矩阵（symmetric-matrix）"><a href="#对称矩阵（symmetric-matrix）" class="headerlink" title="对称矩阵（symmetric matrix）"></a>对称矩阵（symmetric matrix）</h3><p>若$A^{T}=A$，则称$A$是一个对称矩阵（symmetric matrix）<br>若$A^{T}=-A$，则称$A$是一个反对称矩阵（anti-symmetric matrix）</p>
<p>设$R$为$m \times n$矩阵，则$R R^{T}$为$m \times m$对称矩阵，$R^{T} R$为$n \times n$对称矩阵，且其对角元均非负。而且这两个矩阵尽管阶数不同，但是它们两对角线上元素的和是相等的，即迹（trace）相等</p>
<script type="math/tex; mode=display">\left(R R^{T}\right)_{i i}=\sum_{k=1}^{n} R_{i k}\left(R^{T}\right)_{k i}=\sum_{k=1}^{n} r_{i k}^{2} \geqslant 0</script><script type="math/tex; mode=display">\left(R^{T} R\right)_{j j}=\sum_{l=1}^{m} \left(R^{T}\right)_{j l}R_{l j}=\sum_{l=1}^{m} r_{l j}^{2} \geqslant 0</script><script type="math/tex; mode=display">\operatorname{tr}\left(R R^{T}\right)=\sum_{i=1}^{m}\left(R R^{T}\right)_{i i}=\sum_{i=1}^{m} \sum_{k=1}^{n} r_{i k}^{2}</script><script type="math/tex; mode=display">
\sum_{j=1}^{n}\left(R^{T} R\right)_{j j}=\sum_{j=1}^{n} \sum_{l=1}^{m} r_{l j}^{2}=t r\left(R^{T} R\right)</script><script type="math/tex; mode=display">\operatorname{tr}\left(R R^{T}\right)=\operatorname{tr}\left(R^{T} R\right)</script>]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（3）：高斯消元法</title>
    <url>/2019/04/11/linear_algebra_3/</url>
    <content><![CDATA[<p>复习线性代数第三章：高斯消元法</p>
<p>参考资料：<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang 麻省理工视频公开课程</p>
<span id="more"></span>
<h2 id="1-高斯（Gauss）消元法"><a href="#1-高斯（Gauss）消元法" class="headerlink" title="1.高斯（Gauss）消元法"></a>1.高斯（Gauss）消元法</h2><p>在自然科学 社会科学及工程技术的许多领域，我们会遇到具有若干个未知量，若干个方程的大型线性方程组，这就要求我们思考求解线性方程组的系统解法，高斯消元法以德国著名数学家高斯（1777-1855）命名。高斯被认为是历史上最重要的数学家之一，还享有“数学王子”美誉。</p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/Carl_Friedrich_Gauss_1840_by_Jensen.jpg" alt="gauss"><br>德国十马克上的高斯头像，纸币正面还有正态分布曲线，哥廷根市标志性建筑，包括大学大讲堂，天文馆，教堂，市政厅。背面是航海用的六分仪。<br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/mark-gauss.jpg" alt="10mark"></p>
<h3 id="Gauss消元法的步骤："><a href="#Gauss消元法的步骤：" class="headerlink" title="Gauss消元法的步骤："></a>Gauss消元法的步骤：</h3><p>（1）若方程组的第一个主元位置为0，则交换方程以得到第一个主元<br>（2）用第一个方程的倍数消去第一个主元下方所有系数<br>（3）确定第二个主元，继续以上消元过程<br>（4）最后得到含一个未知量的方程，回代得到方程组的解</p>
<p>n个方程有n个主元 $\longleftrightarrow$ 方程组有唯一解<br>消元中止（即出现$0 = c \ne 0$ 或 $0 = 0$） $\longrightarrow$ 方程组无解或有无穷多解</p>
<h3 id="Gauss消元法例子："><a href="#Gauss消元法例子：" class="headerlink" title="Gauss消元法例子："></a>Gauss消元法例子：</h3><p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/Gauss消元.png" alt=""><br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/gauss消元矩阵表示.png" alt=""></p>
<h2 id="增广矩阵（augmented-matrix）"><a href="#增广矩阵（augmented-matrix）" class="headerlink" title="增广矩阵（augmented matrix）"></a>增广矩阵（augmented matrix）</h2><p>对于非齐次线性方程组</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}{a_{11} x_{1}+a_{12} x_{2}+\cdots+a_{1 n} x_{n}=b_{1}} \\ {a_{21} x_{1}+a_{22} x_{2}+\cdots+a_{2 n} x_{n}=b_{2}} \\ {\dots \ldots \ldots \ldots} \\ {a_{m 1} x_{1}+a_{m 2} x_{2}+\cdots+a_{m n} x_{n}=b_{m}}\end{array}\right.</script><p>有如下几个有用的矩阵：</p>
<script type="math/tex; mode=display">
A=\left(a_{i j}\right), x=\left( \begin{array}{c}{x_{1}} \\ {x_{2}} \\ {\vdots} \\ {x_{n}}\end{array}\right), \mathbf{b}=\left( \begin{array}{c}{b_{1}} \\ {b_{2}} \\ {\vdots} \\ {b_{m}}\end{array}\right),B=(A | \mathbf{b})</script><p>其中$A$称为系数矩阵，$x$称为未知数矩阵，$\mathbf{b}$称为常数项矩阵，$B$称为增广矩阵</p>
<h2 id="初等行变换和初等矩阵（elementary-matrix）"><a href="#初等行变换和初等矩阵（elementary-matrix）" class="headerlink" title="初等行变换和初等矩阵（elementary matrix）"></a>初等行变换和初等矩阵（elementary matrix）</h2><p>定义：以下三种变换称为矩阵的<strong>初等行变换</strong><br>（1）对换两行（对换i，j两行，记作$r<em>{i} \leftrightarrow r</em>{j}$）<br>（2）以数$k \ne 0$ 乘某一行中的所有元（第i行乘k，记作$r<em>{i} \times k$）<br>（3）把某一行所有元的k倍加到另一行所对应的元上去（第j行的k倍加到第i行上，记作$r</em>{i}+k r_{j}$）<br>把定义中的“行”换成“列”，即得矩阵的<strong>初等列变换</strong>的定义（所用的记号把“r”换成“c”）<br>矩阵的初等行变换和初等列变换统称<strong>初等变换</strong></p>
<p>定义：由单位矩阵E经过一次初等变换得到的矩阵称为<strong>初等矩阵（elementary matrix）</strong></p>
<p>初等矩阵性质：设A是一个 $m \times n$ 矩阵，对A施行一次初等行变换，相当于在A的左边乘相应的m阶初等矩阵；对A施行一次初等列变换，相当于在A的右边乘相应的n阶初等矩阵。</p>
<p>简而言之：左乘换行，右乘换列</p>
<p>高斯消元法就是利用初等矩阵把方程组的增广矩阵的系数矩阵部分变换成上三角矩阵，然后求解，结果可能有唯一解，无穷多解和无解三种可能。</p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>线性代数（2）：矩阵与线性方程组</title>
    <url>/2019/04/08/linear_algebra_2/</url>
    <content><![CDATA[<p>复习线性代数： 矩阵与线性方程组</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang 麻省理工视频公开课程</p>
<span id="more"></span>
<h2 id="2-1-矩阵与向量的乘积"><a href="#2-1-矩阵与向量的乘积" class="headerlink" title="2.1 矩阵与向量的乘积"></a>2.1 矩阵与向量的乘积</h2><p>矩阵与向量乘积以<strong>旋转变换</strong>为例：<br>矩阵$<br>\left(<br>\begin{matrix}<br>\cos \varphi&amp;-\sin \varphi\<br>\sin \varphi&amp;\cos \varphi\<br>\end{matrix}<br>\right)<br>$对应的线性变换</p>
<script type="math/tex; mode=display">
\left(
\begin{matrix}
\cos \varphi&-\sin \varphi\\
\sin \varphi&\cos \varphi\\
\end{matrix}
\right)

\left(
\begin{matrix}
x\\
y\\
\end{matrix}
\right)
= 
\left(
\begin{matrix}
x_{1}\\
y_{1}\\
\end{matrix}
\right)</script><script type="math/tex; mode=display">\left\{\begin{array}{l}{x_{1}=x \cos \varphi-y \sin \varphi} \\ {y_{1}=x \sin \varphi+y \cos \varphi}\end{array}\right.</script><p>把$xOy$平面上的向量$\overrightarrow{O P}=(x,y)$变换为向量$\overrightarrow{O P<em>{1}}=(x</em>{1},y_{1})$。<br>设$\overrightarrow{OP}$的长度为$r$，辐角为$\theta$，即设$x=r \cos \theta, y=r \sin \theta$，那么</p>
<script type="math/tex; mode=display">
\begin{aligned} x_{1} &=r(\cos \varphi \cos \theta-\sin \varphi \sin \theta)=r \cos (\theta+\varphi) \\ y_{1} &=r(\sin \varphi \cos \theta+\cos \varphi \sin \theta)=r \sin (\theta+\varphi) \end{aligned}</script><p>表明$\overrightarrow{O P_{1}}$的长度为$r$，而辐角为$\theta+\varphi$。因此，这是把向量$\overrightarrow{OP}$（依逆时针方向）旋转$\varphi$角（即把点P以原点为中心逆时针旋转$\varphi$角）的<strong>旋转变换</strong>。</p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/旋转变换.png" alt="旋转变换"></p>
<p>进一步还可推知，用</p>
<script type="math/tex; mode=display">
A^{n}=\left( \begin{array}{cc}{\cos \varphi} & {-\sin \varphi} \\ {\sin \varphi} & {\cos \varphi}\end{array}\right)^{n}</script><p>，相当于把向量$\overrightarrow{O P}$按逆时针方向旋转n个$\varphi$角，即旋转$n \varphi$角，而旋转$n\varphi$角的变换所对应的矩阵为</p>
<script type="math/tex; mode=display">
\left( \begin{array}{cc}{\cos n \varphi} & {-\sin n \varphi} \\ {\sin n \varphi} & {\cos n \varphi}\end{array}\right)</script><p>，亦即成立</p>
<script type="math/tex; mode=display">\left( \begin{array}{cc}{\cos \varphi} & {-\sin \varphi} \\ {\sin \varphi} & {\cos \varphi}\end{array}\right)^{n}=\left( \begin{array}{cc}{\cos n \varphi} & {-\sin n \varphi} \\ {\sin n \varphi} & {\cos n \varphi}\end{array}\right)</script><h2 id="2-2-可逆矩阵"><a href="#2-2-可逆矩阵" class="headerlink" title="2.2 可逆矩阵"></a>2.2 可逆矩阵</h2><p>若对于方阵$A$： $A \mathbf{x}=\mathbf{b}$对任意向量$\vec b$有唯一解，则矩阵$A$是可逆的（invertible）。可逆矩阵的列向量线性无关（linearly independent），且$A \mathbf{x}=\mathbf{0}$只有零解。不可逆矩阵称为奇异（singular）矩阵，列向量是线性相关的（linearly depentent），$A \mathbf{x}=\mathbf{0}$有无穷多解。</p>
<h2 id="2-3-线性方程组的行图和列图"><a href="#2-3-线性方程组的行图和列图" class="headerlink" title="2.3 线性方程组的行图和列图"></a>2.3 线性方程组的行图和列图</h2><h3 id="方程组的行图（row-picture）"><a href="#方程组的行图（row-picture）" class="headerlink" title="方程组的行图（row picture）"></a>方程组的行图（row picture）</h3><p>例子1：给定线性方程组</p>
<script type="math/tex; mode=display">
\left\{\begin{array}{l}{x-2 y=0} \\ {2 x-y=3}\end{array}\right.</script><p>它可以写成矩阵的形式</p>
<script type="math/tex; mode=display">
\left( \begin{array}{ll}{1} & {-2} \\ {2} & {-1}\end{array}\right) \left( \begin{array}{l}{x} \\ {y}\end{array}\right)=\left( \begin{array}{l}{0} \\ {3}\end{array}\right)</script><p>从行（row）的角度看，每行代表一条直线，方程组的解为两直线的交点</p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/行图row_picture.png" alt=""></p>
<h3 id="方程组的列图（column-picture）"><a href="#方程组的列图（column-picture）" class="headerlink" title="方程组的列图（column picture）"></a>方程组的列图（column picture）</h3><p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/列图column_picture.png" alt=""></p>
<h3 id="例2"><a href="#例2" class="headerlink" title="例2:"></a><strong>例2:</strong></h3><p><strong>行图</strong><br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/行图.png" alt=""><br><strong>列图</strong><br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/列图.png" alt=""></p>
<h3 id="从行图，列图去理解方程组有唯一解，无解和无穷多解"><a href="#从行图，列图去理解方程组有唯一解，无解和无穷多解" class="headerlink" title="从行图，列图去理解方程组有唯一解，无解和无穷多解"></a>从行图，列图去理解方程组有<strong>唯一解，无解和无穷多解</strong></h3><p>唯一解的行图列图<br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/唯一解的行图列图.png" alt="唯一解的行图列图"></p>
<p>无解的行图列图<br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/无解的行图列图.png" alt="无解的行图列图"></p>
<p>无穷多解的行图列图<br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/无穷多解的行图列图.png" alt="无穷多解的行图列图"></p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World格式参考文件</title>
    <url>/2019/04/08/hello-world/</url>
    <content><![CDATA[<p>这个主要作为markdown格式的参照</p>
<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h1 id="这是一个一级标题"><a href="#这是一个一级标题" class="headerlink" title="这是一个一级标题"></a>这是一个一级标题</h1><h2 id="这是一个二级标题"><a href="#这是一个二级标题" class="headerlink" title="这是一个二级标题"></a>这是一个二级标题</h2><h3 id="这是一个三级标题"><a href="#这是一个三级标题" class="headerlink" title="这是一个三级标题"></a>这是一个三级标题</h3><p><em>斜体</em><br><strong>加粗</strong></p>
<span id="more"></span>
<ul>
<li>列表1 12345678</li>
<li>列表2 12345678<br>  a 子列表1<br>  b 子列表2</li>
<li>列表3</li>
</ul>
<p><code>单行的代码&lt;/html&gt;</code></p>
<blockquote>
<p>这是一个引用</p>
</blockquote>
<hr>
<hr>
<hr>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>线性代数（1）：向量及其运算</title>
    <url>/2019/04/03/linear_algebra_1/</url>
    <content><![CDATA[<p>复习线性代数第一章：向量及向量空间的定义</p>
<p>参考资料:<br>清华大学数学科学系-线性代数-马辉<br>《工程数学 <strong>线性代数</strong> 第六版》 同济大学数学系 高等教育出版社<br>Linear Algebra by Gilbert Strang MIT麻省理工线性代数公开视频课，非常<strong>推荐</strong>！</p>
<span id="more"></span>
<h2 id="1-向量空间的定义"><a href="#1-向量空间的定义" class="headerlink" title="1.向量空间的定义"></a>1.向量空间的定义</h2><p>在由称为“向量”的元素构成的非空集合 V 中，若定义了加法和数乘运算，且对任意向量 $\vec a,\vec b,\vec c$ 及数$k,l \in F$  （F为数域）满足以下<strong>8条性质</strong>：</p>
<p><strong>加法结合律和加法交换律</strong></p>
<p>1.$\vec a+(\vec b+\vec c)=(\vec a+\vec b)+\vec c$<br>2.$\vec a+\vec b=\vec b+\vec a$</p>
<p><strong>存在 零向量 和 负向量</strong></p>
<p>3.存在<strong>零向量</strong> $\vec 0$，有$\vec a+\vec 0=\vec a$<br>4.对任意向量$\vec a$，存在唯一相反向量 $-\vec a$，使得$\vec a+(-\vec a)=\vec 0$</p>
<p><strong>数乘规律：满足乘法结合律和分配律</strong></p>
<p>5.$1·\vec a=\vec a$<br>6.$(kl)\vec a=k(l\vec a)$<br>7.$k(\vec a+\vec b)=k\vec a+k\vec b$<br>8.$(k+l)\vec a=k\vec a+l\vec a$</p>
<h2 id="2-向量的线性组合"><a href="#2-向量的线性组合" class="headerlink" title="2.向量的线性组合"></a>2.向量的线性组合</h2><p>定义：设$\vec v<em>{1},…,\vec v</em>{m}$ 为m个n维向量，$c<em>{1},…,c</em>{m}\in R$，则称$c<em>{1}\vec v</em>{1}+…+c<em>{m}\vec v</em>{m}$ 为向量$\vec v<em>{1},…,\vec v</em>{m}$ 的一个线性组合。（包含向量的加法和数乘）</p>
<h2 id="3-向量的点积，长度"><a href="#3-向量的点积，长度" class="headerlink" title="3.向量的点积，长度"></a>3.向量的点积，长度</h2><p>定义：设$\vec v=(v<em>{1},…,v</em>{n}),\vec w=(w<em>{1},…,w</em>{n})$ 是两个n维向量，定义<strong>点积（dot product）</strong>$\vec v·\vec w$为</p>
<script type="math/tex; mode=display">\vec v·\vec w=v_{1}w_{1}+...+v_{n}w_{n}</script><p>点积又称为<strong>内积(inner product)</strong>或<strong>数量积(scalar product)</strong><br>注意：<strong>两个向量的点积是一个数</strong></p>
<p>定义：向量$\vec v$的 <strong>长度(length)</strong> 或 <strong>模(norm)</strong> 定义为</p>
<script type="math/tex; mode=display">||\vec v||=\sqrt{\vec v·\vec v}</script><p>若$\vec v=(v<em>{1},…v</em>{n})$，则$||\vec v||=\sqrt{v<em>{1}^{2}+…+v</em>{n}^{2}}$</p>
<p>定义：若$||\vec v||=1$，则$\vec v$称为<strong>单位向量(unit vector)</strong><br>向量<strong>单位化</strong>：任给一个非零向量$\vec v$，则$\frac{\vec v}{||\vec v||}$是沿$\vec v$方向的单位向量</p>
<p>向量点积的性质：<br>1.$\vec v·\vec w=\vec w·\vec v$（对称性）<br>2.$\vec u·(c\vec v+d\vec w)=c\vec u·\vec v+d\vec u·\vec w$（线性性）<br>3.$\vec v·\vec v=||\vec v||^{2}\ge 0$，且当且仅当$\vec v=\vec 0$时等号成立（正定性）</p>
<h2 id="4-向量的夹角"><a href="#4-向量的夹角" class="headerlink" title="4.向量的夹角"></a>4.向量的夹角</h2><p>定义：若$\vec v·\vec w=0$，则称向量$\vec v$和$\vec w$垂直（perpendicular）或称正交（orthogonal）。记作$\vec v\perp \vec w$或$\vec w\perp \vec v$<br>规定：零向量与任意向量垂直</p>
<p>命题：两非零向量$\vec v,\vec w$的夹角$\theta$满足$cos\theta=\frac{\vec v·\vec w}{||\vec v||||\vec w||}$</p>
<h2 id="5-两个重要不等式"><a href="#5-两个重要不等式" class="headerlink" title="5.两个重要不等式"></a>5.两个重要不等式</h2><p>（1）Cauchy-Schwarz不等式：$|\vec v·\vec w|\le ||\vec v||||\vec w||$，等号成立当且仅当一个向量是另一个向量的倍数</p>
<p>（2）三角不等式（Triangle inequality）$||\vec v+\vec w||\le ||\vec v||+||\vec w||$，等号成立当且仅当$\vec v,\vec w$之一为另一向量的非负倍数。</p>
<p><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/Cauchy.jpg" alt="Cauchy"><br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/Schwarz.jpg" alt="Schwarz"></p>
]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用Hexo博客框架搭建个人博客</title>
    <url>/2019/04/02/Hexo/</url>
    <content><![CDATA[<p>这个主要讲解hexo博客框架搭建的基础知识</p>
<p>hexo官方说明文档网址：<a href="https://hexo.io/zh-cn/docs/">https://hexo.io/zh-cn/docs/</a></p>
<span id="more"></span>
<h2 id="Hexo框架介绍"><a href="#Hexo框架介绍" class="headerlink" title="Hexo框架介绍"></a>Hexo框架介绍</h2><p><a href="https://hexo.io/zh-cn/">Hexo</a> 是一个快速、简洁且高效的博客框架(blog framework)。Hexo使用<a href="https://daringfireball.net/projects/markdown/">Markdown</a>渲染引擎解析文章，在几秒内，即可利用相应主题生成静态网页。</p>
<h3 id="Hexo的优点"><a href="#Hexo的优点" class="headerlink" title="Hexo的优点"></a>Hexo的优点</h3><p>1、快速：基于Node.js 所带来的超快生成速度，让上百个页面在几秒内瞬间完成渲染。<br>2、支持Markdown：Hexo 支持 GitHub Flavored Markdown 的所有功能。<br>3、一键部署：只需一条指令即可部署到 GitHub Pages, Heroku 或其他网站。<br>4、有丰富的插件支持：Hexo 拥有强大的插件系统，安装插件可以让 Hexo 支持 Jade, CoffeeScript。</p>
<h3 id="Hexo的安装"><a href="#Hexo的安装" class="headerlink" title="Hexo的安装"></a>Hexo的安装</h3><p>安装过程不予赘述，mac系统下请参照<a href="https://hexo.io/zh-cn/docs/">Hexo网站说明</a>打开Mac终端(Terminal)自行安装</p>
<h2 id="建站-Setup"><a href="#建站-Setup" class="headerlink" title="建站 Setup"></a>建站 Setup</h2><p> 安装好Hexo后，打开Mac 终端，进入你所希望保存博客文件的路径。(我个人把博客文件保存在桌面 /Users/wang/Desktop)<br> 执行以下命令，Hexo会在你当前工作路径下新建搭建博客的所需要的文件。</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo init &lt;folder&gt; <span class="comment"># &lt;folder&gt;是你的博客文件名，可以任意起，我就命名为blog，此命令会在当前工作目录下创建一个&lt;folder&gt;文件夹，里面放好了搭建博客所需的各种文件</span></span><br><span class="line">$ <span class="built_in">cd</span> &lt;folder&gt; <span class="comment"># 进入blog文件</span></span><br><span class="line">$ npm install <span class="comment"># 把所有相关的模块还原回来</span></span><br></pre></td></tr></table></figure>
<p>代码运行完成后，会在当前目录下创建一个文件夹，里面含如下文件：<br><img src="https://raw.githubusercontent.com/Splendid-sun/Hexo-photo/master/linear-algebra/1.png" alt="picture"></p>
<h2 id="各文件介绍"><a href="#各文件介绍" class="headerlink" title="各文件介绍"></a>各文件介绍</h2><h3 id="config-yml"><a href="#config-yml" class="headerlink" title="_config.yml"></a>_config.yml</h3><p>网站的配置信息，可以再次配置大部分参数</p>
<h3 id="package-json"><a href="#package-json" class="headerlink" title="package.json"></a>package.json</h3><p>应用程序信息</p>
<h3 id="scaffolds"><a href="#scaffolds" class="headerlink" title="scaffolds"></a>scaffolds</h3><p><a href="https://hexo.io/zh-cn/docs/writing">模版</a> 文件夹。当您新建文章时，Hexo会根据scaffold来建立文件。Hexo的模板是指在新建的markdown文件中默认填充的内容。例如，如果您修改scaffold/post.md中的Front-matter内容，那么每次新建一篇文章时都会包含这个修改。</p>
<h3 id="source"><a href="#source" class="headerlink" title="source"></a>source</h3><p>资源文件夹是存放用户资源的地方。</p>
<h3 id="themes"><a href="#themes" class="headerlink" title="themes"></a>themes</h3><p><a href="https://hexo.io/zh-cn/docs/themes">主题</a> 文件夹。Hexo会根据主题来生成静态网页。</p>
]]></content>
      <tags>
        <tag>Hexo博客框架</tag>
      </tags>
  </entry>
</search>
